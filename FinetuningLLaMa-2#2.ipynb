{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c16eb01-ccb3-4885-a3c7-0e2afec070af",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA 2 models on SageMaker JumpStart #2: Finetuning the Deployed LaMa-2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff0cc3-85fd-45ab-a9f4-0b1d1bf6d863",
   "metadata": {},
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Please find more details in the section [Dataset instruction](#Dataset-instruction). In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "\n",
    "Training data is formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The training folder can also contain a template.json file describing the input and output formats.\n",
    "\n",
    "To train your model on a collection of unstructured dataset (text files), please see the section [Example fine-tuning with Domain-Adaptation dataset format](#Example-fine-tuning-with-Domain-Adaptation-dataset-format) in the Appendix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6ec144-2139-4763-b6a9-1ba57f2e6a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.187.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.14.5)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.28.42)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.24.2)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (21.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.3.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.19.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.10.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.42 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.42)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema->sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.8/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema->sagemaker) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.8/site-packages (from jsonschema->sagemaker) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from jsonschema->sagemaker) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4153847-e2a6-4a63-a2bb-a674878bb888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d00a19178264ce0b430a552e210ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2103055"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec64c1e5-a790-4a88-8eda-0247c0a35715",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Who is Serhiy Malyi?',\n",
       " 'context': 'Serhiy Viktorovych Malyi (Ukrainian: Сергій Вікторович Малий; born 5 June 1990) is a professional footballer who plays as a defender for Tobol. Born in Ukraine, he represents the Kazakhstan national team.',\n",
       " 'response': 'Serhiy Malyi is a professional footballer who plays defense for Tobol. He also represents the Kazakhstan national team.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12057aa-1d57-49b6-8c7c-5952f4418e8a",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job (since we are instruction fine-tuning the model in this example), and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4b2846-6244-45ee-84f5-5c016c2e40d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e51a20-806e-4141-a1a6-50a814005fe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92859f95-0976-497c-931c-6e2df4e249c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Training data: s3://sagemaker-us-east-1-988564344122/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0521a-5b1c-4c77-a6c6-13c67b8db1f2",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 7B model on the summarization dataset from Dolly. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5.-Few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f3c416-4bb2-4668-b7b8-969e772f7fee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:No instance type selected for training job. Defaulting to ml.g5.12xlarge.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2023-09-26-13-14-07-191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-26 13:14:07 Starting - Starting the training job...\n",
      "2023-09-26 13:14:34 Starting - Preparing the instances for training.........\n",
      "2023-09-26 13:15:39 Downloading - Downloading input data.........\n",
      "2023-09-26 13:17:15 Training - Downloading the training image..................\n",
      "2023-09-26 13:20:16 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-26 13:20:47,016 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-26 13:20:47,047 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-26 13:20:47,056 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-26 13:20:47,057 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-26 13:20:54,348 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+9e3e10c5ed-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.1.0.dev20230728+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.8-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.0.7-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+9e3e10c5ed->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0.dev20230728+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=34555b2eea45ba9649f28fda084a08add1e359e3bddb73a629d86673658e6762\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0.dev20230728+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+9e3e10c5ed pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.0.7 sagemaker-jumpstart-script-utilities-1.1.8 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.1.0.dev20230728+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-26 13:21:46,986 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-26 13:21:46,986 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-26 13:21:47,018 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-26 13:21:47,058 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-26 13:21:47,099 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-26 13:21:47,108 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2023-09-26-13-14-07-191\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2023-09-26-13-14-07-191\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2023-09-26 13:21:47,136 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2023-09-26 13:21:52,405] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2023-09-26 13:21:52,405] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2023-09-26 13:21:52,405] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2023-09-26 13:21:52,405] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 9279.43it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1927.53it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mGenerating train split: 1069 examples [00:00, 59801.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17553.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17104.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17123.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17394.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 385.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 383.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 384.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 381.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 390.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 388.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 387.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 386.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 389.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 387.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 385.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 384.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1828.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1824.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1771.65 examples/s]INFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1784.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1692.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1768.34 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1784.64 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1689.87 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.89s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.85s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:306: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:306: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 451\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 113\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:306: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:306: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.17.1+cuda11.8\u001b[0m\n",
      "\u001b[34malgo-1:123:263 [0] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:126:264 [3] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:124:265 [1] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:126:264 [3] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:123:263 [0] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:124:265 [1] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:125:266 [2] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:125:266 [2] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:09<04:29,  9.99s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:10<04:36, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:10<04:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.4623427391052246\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:10<04:30, 10.00s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:18<03:48,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:17<03:46,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:17<03:48,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.2544114589691162\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:17<03:46,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:25<03:28,  8.35s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:25<03:27,  8.30s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:25<03:28,  8.36s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.6421805620193481\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:25<03:27,  8.30s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [00:33<03:14,  8.11s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.2206459045410156\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [00:33<03:15,  8.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [00:33<03:14,  8.11s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [00:33<03:15,  8.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [00:41<03:04,  8.02s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [00:41<03:04,  8.00s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [00:41<03:04,  8.02s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.5862711668014526\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [00:41<03:04,  8.00s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [00:49<02:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [00:49<02:54,  7.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [00:49<02:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.4632623195648193\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [00:49<02:54,  7.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [00:57<02:45,  7.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [00:57<02:45,  7.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [00:56<02:45,  7.89s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.3582947254180908\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [00:56<02:45,  7.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:04<02:37,  7.86s/it]#015Training Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:04<02:37,  7.87s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.5658750534057617\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:04<02:37,  7.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:04<02:37,  7.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [01:12<02:28,  7.84s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.618259310722351\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [01:12<02:28,  7.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [01:12<02:29,  7.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [01:12<02:29,  7.84s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.5933774709701538\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [01:20<02:20,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [01:20<02:20,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [01:20<02:20,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [01:20<02:20,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.4540711641311646\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [01:28<02:12,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [01:28<02:12,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [01:28<02:12,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [01:28<02:12,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [01:36<02:05,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [01:36<02:05,  7.82s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.4773560762405396\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [01:35<02:05,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [01:35<02:05,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [01:43<01:57,  7.85s/it]#015Training Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [01:43<01:57,  7.85s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.463446021080017\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [01:44<01:57,  7.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [01:43<01:57,  7.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [01:51<01:49,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [01:51<01:49,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.2260279655456543\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [01:51<01:49,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [01:51<01:49,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [01:59<01:41,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [01:59<01:41,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [01:59<01:41,  7.82s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.2367578744888306\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [01:59<01:41,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [02:07<01:33,  7.81s/it]#015Training Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [02:07<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [02:07<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.5888261795043945\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [02:07<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [02:14<01:25,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.6419953107833862\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [02:15<01:25,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [02:14<01:25,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [02:15<01:25,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.2062914371490479\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [02:22<01:18,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [02:22<01:18,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [02:22<01:18,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [02:22<01:18,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [02:30<01:10,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [02:30<01:10,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.3995405435562134\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [02:30<01:10,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [02:30<01:10,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [02:38<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [02:38<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.4710218906402588\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [02:38<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [02:38<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [02:46<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [02:46<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.3041727542877197\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [02:46<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [02:46<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [02:53<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [02:54<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.332114815711975\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [02:54<00:46,  7.80s/it]#015Training Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [02:53<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.349480390548706\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [03:01<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [03:01<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [03:01<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [03:01<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2485665082931519\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [03:09<00:31,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [03:09<00:31,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [03:09<00:31,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [03:09<00:31,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [03:17<00:23,  7.80s/it]#015Training Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [03:17<00:23,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.2739250659942627\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [03:17<00:23,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [03:17<00:23,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [03:25<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [03:25<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [03:25<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.4223517179489136\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [03:25<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2880860567092896\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [03:32<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [03:33<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [03:32<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [03:33<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [03:40<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [03:40<00:00,  7.81s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [03:40<00:00,  7.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [03:40<00:00,  7.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [03:41<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [03:41<00:00,  7.89s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.4341286420822144\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [03:40<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [03:40<00:00,  7.89s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:09,  2.46s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:30<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:30<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.37s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.37s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.37s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.37s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.8351, device='cuda:0') eval_epoch_loss=tensor(1.3442, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.3442026376724243\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=4.0729, train_epoch_loss=1.4044, epcoh time 221.05422810599998s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2991782426834106\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.983056366443634\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.80s/it]#015Training Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.5066245794296265\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.0124287605285645\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.4336291551589966\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2896480560302734\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.202376127243042\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:35,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:35,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.3870700597763062\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:35,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:35,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.4300885200500488\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [01:17<02:20,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [01:17<02:20,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [01:17<02:20,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.4377806186676025\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [01:17<02:20,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.288573145866394\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]#015Training Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.3574415445327759\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]#015Training Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.3811581134796143\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]#015Training Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.1040518283843994\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.1418688297271729\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.4982225894927979\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.568594217300415\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:18,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.1263214349746704\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:18,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:18,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.325932502746582\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.3905677795410156\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.2204478979110718\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.273213505744934\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:39,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:39,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.2701895236968994\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:39,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:39,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.1981604099273682\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.217698574066162\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.82s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.3610457181930542\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2415499687194824\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.385094404220581\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:09,  2.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:09,  2.47s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:09,  2.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:09,  2.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.7059, device='cuda:0') eval_epoch_loss=tensor(1.3099, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 1.3099205493927002\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=3.6231, train_epoch_loss=1.2873, epcoh time 219.09591754299993s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.79s/it]#015Training Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.78s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.260420560836792\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.78s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.9363848567008972\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4744230508804321\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.9715058207511902\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.4033273458480835\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2452682256698608\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.1683285236358643\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.3413777351379395\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.3931431770324707\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [01:17<02:20,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [01:17<02:20,  7.79s/it]#015Training Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [01:17<02:20,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.399133324623108\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [01:17<02:20,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2487878799438477\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.3227059841156006\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.3574237823486328\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]#015Training Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0731799602508545\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.113074541091919\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.476088285446167\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.545715570449829\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.095406174659729\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.3042705059051514\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]#015Training Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.3591320514678955\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.1907050609588623\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2499531507492065\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.2433046102523804\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.1756354570388794\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.1948000192642212\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]#015Training Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.3363755941390991\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2208918333053589\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.3617217540740967\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.80s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.46s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.41s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.41s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.37s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.6662, device='cuda:0') eval_epoch_loss=tensor(1.2992, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 2 is 1.2991554737091064\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=3.5111, train_epoch_loss=1.2559, epcoh time 219.00038627499998s\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:29,  7.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:29,  7.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:29,  7.76s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2383290529251099\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:29,  7.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.77s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.9173011183738708\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.79s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4576010704040527\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:14,  7.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.9529867768287659\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.81s/it]#015Training Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.3884673118591309\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [00:38<02:59,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2291302680969238\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.1520510911941528\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.3223416805267334\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.3791297674179077\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [01:18<02:20,  7.81s/it]#015Training Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [01:18<02:20,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3837589025497437\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [01:18<02:20,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [01:18<02:20,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2317804098129272\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.3041738271713257\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]#015Training Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.3446810245513916\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.054794430732727\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.0967050790786743\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.4615312814712524\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.5321202278137207\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:18,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.0738983154296875\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:18,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:18,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:18,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.291621446609497\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.3440057039260864\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.1740750074386597\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]#015Training Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2374536991119385\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.2245705127716064\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]#015Training Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.82s/it]#015Training Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.82s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.1578798294067383\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.1805840730667114\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]#015Training Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.3212108612060547\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.2082995176315308\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]#015Training Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.3488478660583496\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]#015Training Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]#015Training Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.45s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.41s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.41s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.41s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.41s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.39s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:49,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:49,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:49,  2.38s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:49,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:30<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:30<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:30<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:30<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:49<00:19,  2.38s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:49<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:49<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.6552, device='cuda:0') eval_epoch_loss=tensor(1.2962, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 3 is 1.2961567640304565\u001b[0m\n",
      "\u001b[34mEpoch 4: train_perplexity=3.4523, train_epoch_loss=1.2391, epcoh time 219.08413032099998s\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.78s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2209141254425049\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:07<03:30,  7.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.9039917588233948\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:15<03:22,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:15,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4433521032333374\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:15,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:15,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:23<03:15,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.9402451515197754\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:31<03:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [00:39<02:59,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [00:39<02:59,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [00:39<02:59,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.3762768507003784\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [00:39<02:59,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.215279459953308\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [00:46<02:51,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.81s/it]#015Training Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.1391112804412842\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [00:54<02:43,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.3091877698898315\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:02<02:36,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]#015Training Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.368367314338684\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [01:10<02:28,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3703235387802124\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [01:18<02:20,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [01:18<02:20,  7.80s/it]#015Training Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [01:18<02:20,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [01:18<02:20,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2162764072418213\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [01:25<02:12,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.286933183670044\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [01:33<02:05,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.334468126296997\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [01:41<01:57,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0414313077926636\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]#015Training Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [01:49<01:49,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.0819722414016724\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [01:57<01:41,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.4477038383483887\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.81s/it]#015Training Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [02:04<01:33,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.5213040113449097\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [02:12<01:25,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.052757740020752\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [02:20<01:17,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.2802891731262207\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [02:28<01:10,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.331825613975525\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [02:36<01:02,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.158756971359253\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [02:43<00:54,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2254748344421387\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [02:51<00:46,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.2075899839401245\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [02:59<00:38,  7.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.1434695720672607\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [03:07<00:31,  7.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.1694966554641724\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]#015Training Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [03:15<00:23,  7.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [03:23<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [03:23<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.3067294359207153\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [03:23<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [03:22<00:15,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.1966477632522583\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [03:30<00:07,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]#015Training Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.3379114866256714\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [03:38<00:00,  7.81s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:02<01:09,  2.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.41s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:04<01:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:07<01:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:09<00:59,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:11<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:12<00:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:14<00:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:16<00:52,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:19<00:50,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:21<00:47,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:23<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:26<00:42,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:28<00:40,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:31<00:38,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [00:33<00:35,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [00:35<00:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [00:38<00:30,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [00:40<00:28,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [00:42<00:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [00:45<00:23,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [00:47<00:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [00:50<00:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [00:52<00:16,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [00:54<00:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [00:57<00:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [00:59<00:09,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:01<00:07,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [01:04<00:04,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [01:06<00:02,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [01:09<00:00,  2.38s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.6543, device='cuda:0') eval_epoch_loss=tensor(1.2959, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 4 is 1.295910120010376\u001b[0m\n",
      "\u001b[34mEpoch 5: train_perplexity=3.4046, train_epoch_loss=1.2251, epcoh time 219.13815327500015s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 3.6128089427948\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.282359004020691\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 3.7033498287200928\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.309069037437439\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 219.47456310400003\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 2.4776794390000303\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.61it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.24it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\n",
      "2023-09-26 13:49:18 Uploading - Uploading generated training model\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2023-09-26 13:49:16,151 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-26 13:49:16,151 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-26 13:49:16,151 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-26 13:49:39 Completed - Training job completed\n",
      "Training seconds: 2040\n",
      "Billable seconds: 2040\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"*\"\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True,  # For Llama-2-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"5\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077fb4e-81fd-4cf1-b4ac-181996ae89ce",
   "metadata": {},
   "source": [
    "Studio Kernel Dying issue:  If your studio kernel dies and you lose reference to the estimator object, please see section [6. Studio Kernel Dead/Creating JumpStart Model from the training Job](#6.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job) on how to deploy endpoint using the training job name and the model id. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09478b-ec36-4608-a16e-3c2e9ee58e0c",
   "metadata": {},
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5388cff3-cb4d-4087-b86c-2f95772a2d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2023-09-26-14-59-56-433\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2023-09-26-14-59-56-425\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2023-09-26-14-59-56-425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aa0e58b-ab27-4294-b81a-f5bd52492f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-textgeneration-llama-2-7b-2023-09-26-14-59-56-425\n"
     ]
    }
   ],
   "source": [
    "name = finetuned_predictor.endpoint_name\n",
    "print (name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7897105-f1f4-4f04-a8e8-a76e035c12f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c9f0f4f-063a-4d9c-ae86-476fa8fd0be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "# Replace 'endpoint_name' with the actual endpoint name you obtained after deployment\n",
    "pretrained_predictor = Predictor(endpoint_name='meta-textgeneration-llama-2-7b-2023-09-24-19-57-25-271',\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00d47a9a-4ed6-47c1-b9cf-fd359f61e209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite down some points on Yugoslav Cup using given paragraph as a base.\\n\\n### Input:\\nThe Yugoslav Cup was a tournament for which clubs from all tiers of the football pyramid were eligible to enter. In addition, amateur teams put together by individual Yugoslav People's Army garrisons and various factories and industrial plants were also encouraged to enter, which meant that each cup edition could have several thousands of teams in its preliminary stages. These teams would play through a number of qualifying rounds before reaching the first round proper, in which they would be paired with top-flight teams.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>1. Clubs from all levels of the football pyramid were eligible to participate in the Yugoslav Cup.\\n2. Additionally, amateur teams assembled by numerous enterprises and industrial plants as well as individual Yugoslav People's Army garrisons were encouraged to compete, which meant that each cup edition may include thousands of teams in its preliminary rounds.\\n3. Prior to the first round proper, where they would be partnered with top-flight teams, these teams would compete in a number of qualification rounds.</td>\n",
       "      <td>\\nThe Cup's first winner was Željezničar Sarajevo from the Sarajevo Football Association, which beat Partizan on a walkover after the Belgrade team could not organize transportation to Sarajevo to play the game. The biggest upset in the competition came with Spartak Subotica's victory over Red Star in 2009. They eliminated Red Star in the sixth round of the qualifications, while Red Star only</td>\n",
       "      <td>1. The Yugoslav Cup was a football tournament which allowed clubs from all the tiers of football pyramid to participate. The teams were encouraged from various factories, youth organizations and army garrisons and they were allowed to enter as well. There were thousands of teams that participated in preliminary stages and qualified for first round proper of the tournament where they were to be paired with top tiered teams.\\n2. The first edition of this cup was played</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nFor the Zodiac sign Aries, Share some information from the given text.\\n\\n### Input:\\nAries (♈︎) (Greek: Κριός, romanized: Kriós, Latin for \"ram\") is the first astrological sign in the zodiac, spanning the first 30 degrees of celestial longitude (0°≤ λ &lt;30°), and originates from the Aries constellation. Under the tropical zodiac, the Sun transits this sign from approximately March 21 to April 19 each year. This time duration is exactly the first month of the Solar Hijri calendar (Arabic Hamal/Persian Farvardin/Pashto Wray).\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>1.\\tAries is the first astrological sign in the zodiac, spanned in the first 30 degrees of celestial longitude (0°≤ λ &lt;30°).\\n2.\\tAries is originated from the Aries constellation.\\n3.\\tThe Sun transits this sign from approximately March 21 to April 19 each year.\\n4.\\tThis time period is exactly the first month of the Solar Hijri calendar (Arabic Hamal/Persian Farvardin/Pashto Wray).</td>\n",
       "      <td>_Description_\\nThis example shows one possible response to the task. The response describes the relevant information and shows support for the task through evidence from text.\\n\\n_Feedback_\\nA better way to respond is to be complete in evidence you provide while also providing the most significant description.\\n\\n\\n### Instruction:\\nProvide a definition for any word from the provided text in your response.\\n\\n### Instruction:\\nExplain how you would use the information</td>\n",
       "      <td>Aries (♈︎) (Greek: Κριός, romanized: Kriós, Latin for \"ram\") is the first astrological sign in the zodiac, spanning the first 30 degrees of celestial longitude (0°≤ λ &lt;30°), and originates from the Aries constellation. Under the tropical zodiac, the Sun transits this sign from approximately March 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nUsing examples taken from the text give me a summary of the main arguments in favour of slavery reparations in the United States and the anticipated cost of enacting such reparations\\n\\n### Input:\\nSlavery ended in the United States in 1865 with the end of the American Civil War and the ratification of the Thirteenth Amendment to the United States Constitution, which declared that \"Neither slavery nor involuntary servitude, except as a punishment for crime whereof the party shall have been duly convicted, shall exist within the United States, or any place subject to their jurisdiction\". At that time, an estimated four million African Americans were set free.\\n\\nSupport for reparations\\nWithin the political sphere, a bill demanding slavery reparations has been proposed at the national level, the \"Commission to Study and Develop Reparation Proposals for African-Americans Act,\" which former Rep. John Conyers Jr. (D-MI) reintroduced to the United States Congress every year from 1989 until his resignation in 2017. As its name suggests, the bill recommended the creation of a commission to study the \"impact of slavery on the social, political and economic life of our nation\"., however there are cities and institutions which have initiated reparations in the US (see § Legislation and other actions for a list).\\n\\nIn 1999, African-American lawyer and activist Randall Robinson, founder of the TransAfrica advocacy organization, wrote that America's history of race riots, lynching, and institutional discrimination have \"resulted in $1.4 trillion in losses for African Americans\". Economist Robert Browne stated the ultimate goal of reparations should be to \"restore the black community to the economic position it would have if it had not been subjected to slavery and discrimination\". He estimates a fair reparation value anywhere between $1.4 to $4.7 trillion, or roughly $142,000 (equivalent to $162,000 in 2021) for every black American living today. Other estimates range from $5.7 to $14.2 and $17.1 trillion.\\n\\nIn 2014, American journalist Ta-Nehisi Coates published an article titled \"The Case for Reparations\", which discussed the continued effects of slavery and Jim Crow laws and made renewed demands for reparations. Coates refers to Rep. John Conyers Jr.'s H.R.40 Bill, pointing out that Congress's failure to pass this bill expresses a lack of willingness to right their past wrongs.\\n\\nIn September 2016, the United Nations' Working Group of Experts on People of African Descent encouraged Congress to pass H.R.40 to study reparations proposals. Still, the Working Group did not directly endorse any specific reparations proposal. The report noted that there exists a legacy of racial inequality in the United States, explaining that \"Despite substantial changes since the end of the enforcement of Jim Crow and the fight for civil rights, ideology ensuring the domination of one group over another, continues to negatively impact the civil, political, economic, social and cultural rights of African Americans today.\" The report notes that a \"dangerous ideology of white supremacy inhibits social cohesion among the US population\".\\n\\nThe topic of reparations gained renewed attention in 2020 as the Black Lives Matter movement named reparations as one of their policy goals in the United States.\\n\\nIn 2020, rapper T.I. supported reparations that would give every African American US$1 million and asserted that slavery caused mass incarcerations, poverty, and other ills.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>In spite of progressive changes since the end of enforcement of Jim Crow and the successes of the fight for civil rights there are still inherent racial inequalities in the USA which negatively affect the civil, political, economic, social and cultural rights of African Americans today.  A United Nations report of 2016 did not endorse a specific reparations proposal, but did call out that there is an ideology of white supremacy present in the population of the United States that inhibits social cohesion.  Further arguments have been made identifying slavery as a cause of mass incarcerations, poverty and other ills that affect today’s African Americans.\\nIt could be argued that there is an inherent acknowledgement that the case exists to examine the impact of slavery on the social, political and economic life of the United States in the annual submission to Congress of the \"Commission to Study and Develop Reparation Proposals for African-Americans Act”.  There are also individual cities within the USA which have instituted reparations.  Estimates of the cost to African Americans of the United States’ history of discrimination have been estimated variously at $1.4 trillion, $4.7 trillion, $5.7 trillion, $14.2 trillion and $17.1 trillion.  These estimates express the projected cost in $ to restore the black community to the economic position it would have if it had not been subjected to slavery and discrimination.</td>\n",
       "      <td>\\nThe most popular argument is the one which refers to the fact that the African Americans were the slaves. The end of the slaves came with the end of the 1st world war. But still the situation is not alright and needs to be adjusted. Another issue is that there are many African Americans who cannot buy a house which shows the extent to which African Americans have been discriminated.\\n\\n[Source](https://www.nytimes.com/article/reparations</td>\n",
       "      <td>Support for reparations has been expressed by economists since at least 1999 when Robert Browne estimated that a fair reparation value would be at least $1.4 trillion. Other estimates in the two decades following Browne's estimate range from $5.7 to $14.2 and $17.1 trillion. Randall Robinson, founder of the TransAfrica organization, estimated that the net loss to African Americans by race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat were the origins of the American Civil War?\\n\\n### Input:\\nHistorians who address the origins of the American Civil War today agree that the preservation of slavery in the United States was the principal aim of the 11 Southern states (seven states before the onset of the war and four states after the onset) that declared their secession from the United States (the Union) and united to form the Confederate States of America (known as the \"Confederacy\"). However, while historians in the 21st century agree on the centrality of the conflict over slavery—it was not just \"a cause\" of the war but \"the cause\"—they disagree sharply on which aspects of this conflict (ideological, economic, political, or social) were most important, and on the North’s reasons for refusing to allow the Southern states to secede. Proponents of the pseudo-historical Lost Cause ideology have denied that slavery was the principal cause of the secession, a view that has been disproven by the overwhelming historical evidence against it, notably the seceding states' own secession documents.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Many historians agree that the issue of slavery was the main cause of the American Civil War.  A total of eleven southern states wanted to preserve slavery and as a result voted to secede from the United States.  These states subsequently declared themselves the Confederate States of America, also known as the Confederacy.  Also most historians agree that slavery was the cause of the war, they have differing views regarding which aspects (idealogical, economic, political, or social) were most important.  Their opinions also differ regarding the Northern States reasons for not allowing the Southern states to succeed from the Union.  Followers of an ideology known as the Lost Cause Idealogy have denied that slavery was the root cause of the war however this view has been disproven by overwhelming evidence, including the Southern states own secession documents.</td>\n",
       "      <td>In my opinion, the Civil War did not have one single cause, but rather a combination of causes such as slavery, states' rights, and the election of Abraham Lincoln.\\n\\n#### Answer Explanation:\\nThis response is similar to the one written by the instructor, but provides additional details. The writer explains what caused the War without making it seem like the response is written just for the purpose of satisfying the instructors request to write in the second person.\\n\\n###</td>\n",
       "      <td>Despite being debated by historians for decades, it is now widely accepted that the cause of the American Civil War was slavery. Although, there are some who dispute that notion based on the secession documents of the eleven separate states.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat was the first American college rowing club?\\n\\n### Input:\\nModern rowing as a competitive sport can be traced to the early 17th century when professional watermen held races (regattas) on the River Thames in London, England. Often prizes were offered by the London Guilds and Livery Companies. Amateur competition began towards the end of the 18th century with the arrival of \"boat clubs\" at British public schools. Similarly, clubs were formed at colleges within Oxford and Cambridge in the early nineteenth century. Public rowing clubs were beginning at the same time in England, Germany, and the United States. The first American college rowing club was formed in 1843 at Yale College.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The first American college rowing club was founded at Yale College in 1843/</td>\n",
       "      <td>\\n[![JavaScript Solution](https://media.geeksforgeeks.org/wp-content/uploads/2021011103018-10_3_3_Splash.png)](https://media.geeksforgeeks.org/wp-content/uploads/2021011103018-10_3_3_Splash.pdf)\\n</td>\n",
       "      <td>The first American college rowing club was formed in 1843 at Yale College.\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generation\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ed1d6-57c6-493e-ab7c-ce3e72f5d027",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "417221cb-0076-462d-9331-4139de11e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-2-7b-2023-09-24-19-57-25-203\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-2-7b-2023-09-24-19-57-25-271\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-2-7b-2023-09-24-19-57-25-271\n",
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-2-7b-2023-09-26-14-59-56-433\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-2-7b-2023-09-26-14-59-56-425\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-2-7b-2023-09-26-14-59-56-425\n"
     ]
    }
   ],
   "source": [
    "# Delete resources\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11581d1-daa5-47b9-82fc-f8697d675f21",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50bd17-211e-4849-902e-a42aef0a0e70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980eea1-eb31-448e-8d87-afe0b47bb955",
   "metadata": {},
   "source": [
    "### 2. Dataset formatting instruction for training\n",
    "\n",
    "---\n",
    "\n",
    "####  Fine-tune the Model on a New Dataset\n",
    "We currently offer two types of fine-tuning: instruction fine-tuning and domain adaption fine-tuning. You can easily switch to one of the training \n",
    "methods by specifying parameter `instruction_tuned` being 'True' or 'False'.\n",
    "\n",
    "\n",
    "#### 2.1. Domain adaptation fine-tuning\n",
    "The Text Generation model can also be fine-tuned on any domain specific dataset. After being fine-tuned on the domain specific dataset, the model\n",
    "is expected to generate domain specific text and solve various NLP tasks in that specific domain with **few shot prompting**.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file. \n",
    "  - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "  - The number of files under train and validation (if provided) should equal to one, respectively. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "```Note About Forward-Looking Statements\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "GENERAL\n",
    "Embracing Our Future ...\n",
    "```\n",
    "\n",
    "\n",
    "#### 2.2. Instruction fine-tuning\n",
    "The Text generation model can be instruction-tuned on any text data provided that the data \n",
    "is in the expected format. The instruction-tuned model can be further deployed for inference. \n",
    "Below are the instructions for how the training data should be formatted for input to the \n",
    "model.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Train and validation directories should contain one or multiple JSON lines (`.jsonl`) formatted files. In particular, train directory can also contain an optional `*.json` file describing the input and output formats. \n",
    "  - The best model is selected according to the validation loss, calculated at the end of each epoch.\n",
    "  If a validation set is not given, an (adjustable) percentage of the training data is\n",
    "  automatically split and used for validation.\n",
    "  - The training data must be formatted in a JSON lines (`.jsonl`) format, where each line is a dictionary\n",
    "representing a single data sample. All training data must be in a single folder, however\n",
    "it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training\n",
    "folder can also contain a `template.json` file describing the input and output formats. If no\n",
    "template file is given, the following template will be used:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\",\n",
    "    \"completion\": \"{response}\"\n",
    "  }\n",
    "  ```\n",
    "  - In this case, the data in the JSON lines entries must include `instruction`, `context` and `response` fields. If a custom template is provided it must also use `prompt` and `completion` keys to define\n",
    "  the input and output templates.\n",
    "  Below is a sample custom template:\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"question: {question} context: {context}\",\n",
    "    \"completion\": \"{answer}\"\n",
    "  }\n",
    "  ```\n",
    "Here, the data in the JSON lines entries must include `question`, `context` and `answer` fields. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0999e-5ad9-4986-8613-ad5c8d9d5d89",
   "metadata": {},
   "source": [
    "#### 2.3. Example fine-tuning with Domain-Adaptation dataset format\n",
    "---\n",
    "We provide a subset of SEC filings data of Amazon in domain adaptation dataset format. It is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data).\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Please uncomment the following code to fine-tune the model on dataset in domain adaptation format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b558c9fb-ae16-48dc-8325-d5bc023841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# model_id = \"meta-textgeneration-llama-2-7b\"\n",
    "\n",
    "# estimator = JumpStartEstimator(model_id=model_id,  environment={\"accept_eula\": \"true\"},instance_type = \"ml.g5.24xlarge\")\n",
    "# estimator.set_hyperparameters(instruction_tuned=\"False\", epoch=\"5\")\n",
    "# estimator.fit({\"training\": f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacef55-f0cb-4779-977b-f93c4aad2247",
   "metadata": {},
   "source": [
    "### 3. Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30dfec1-ac45-4d4e-8e22-a0c558c02946",
   "metadata": {},
   "source": [
    "### 4. Supported Instance types\n",
    "\n",
    "---\n",
    "We have tested our scripts on the following instances types:\n",
    "\n",
    "- 7B: ml.g5.12xlarge, nl.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 13B: ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 70B: ml.g5.48xlarge\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bb615-b042-4439-a63d-10e60d0324e8",
   "metadata": {},
   "source": [
    "### 5. Few notes about the fine-tuning method\n",
    "\n",
    "---\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969e802-f4ca-4cd2-a06e-8c6e8f796630",
   "metadata": {},
   "source": [
    "### 6. Studio Kernel Dead/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Due to the size of the Llama 70B model, training job may take several hours and the studio kernel may die during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80507274-efcb-412a-bf93-1ed120b815f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd6b1e-4474-4bf9-9251-6bde0cfb77a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
