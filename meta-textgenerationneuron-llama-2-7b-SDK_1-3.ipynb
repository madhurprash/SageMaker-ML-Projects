{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfa4226-79a1-4520-b131-c798e42e34f7",
   "metadata": {},
   "source": [
    "# Evaluate, Fine-tune and deploy LLaMA V2 models on [AWS Trainium] based instances in SageMaker JumpStart - Evaluate responses with Uptrain evaluation Metrics\n",
    "\n",
    "____\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy pre-trained Llama 2 model as well as fine-tune it for your dataset in domain adaptation or instruction tuning format on [AWS Trainium] and then evaluate the responses using uptrain.ai evaluators (Open Source Evaluations)\n",
    "\n",
    "## Evaluation\t\n",
    "\n",
    "1. Factual Accuracy --> Checks if the response is grounded by the context provided\n",
    "\n",
    "2. Guideline Adherence --> Checks if the response or the LLM adhers to the given guideline or not\n",
    "\n",
    "3. Response Completeness --> Grades how if the response completes the given question\n",
    "\n",
    "4. Response Completeness wrt Context --> Grades how complete the response was for the question specified with respect to the information present in the context\n",
    "\n",
    "5. Context Relevance --> Evaluates if the context has all the information to answer the given question\n",
    "\n",
    "6. Response Relevance --> Grades how relevant the generated response is or if it has any additional irrelevant information for the question asked.\n",
    "\n",
    "7. Tone Critique --> Assesses if the tone of machine-generated responses matches with the desired persona.\n",
    "\n",
    "8. Language Critique --> Scores machine generated responses in a conversation. The response is evaluated on multiple aspects - fluence, politeness, grammar, and coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9b99d-639b-40f3-91e3-1fe00ee032a4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Model License information\n",
    "---\n",
    "To perform inference on these models, you need to pass 'accept_eula=True' as part of model.deploy() call. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets 'accept_eula=False', so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Similarly, to perform fine-tuning on these models, you need pass environment variable '{\"accept_eula\": \"true\"}' to `JumpStartEstimator` class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c4fcd-d6c5-4381-8425-1d224c0ac197",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Set up\n",
    "\n",
    "---\n",
    "We begin by installing and upgrading necessary packages. Restart the kernel after executing the cell below for the first time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.199.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.33.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.24.4)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.19.1)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.95.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.0)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker) (0.14.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.9 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.33.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2023.7.22)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker) (0.58.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {},
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart through 2 options. Option 1 allows you to quickly deploy the endpoint with default setting in two lines of code. Option 2 allows you to have more customized configurations. \n",
    "\n",
    "To deploy a model on [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) or [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf2/) based instances, we will firstly need call PyTorch Neuron ([torch-neuronx](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html?highlight=graph#neuron-persistent-cache)) to compile the model into a Neuron specific graph. Then during runtime the graph is executed on the **NeuronCores** of the [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) or [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf2/) based instances. Compiling the graph involves running optimizations that can make use of the NeuronCores efficiently.\n",
    "\n",
    "In SageMaker JumpStart, we pre-compile the neuron graphs for a varieity of configurations such that you do not spend time waiting for compiling the graph during endpoint deployment, as long as the deployment parameters (**environmental variables**) matches one of configurations listed as below. Otherwise, the compilation will be triggered during endpoint deployment, which will take a slightly longer time to deploy a model.\n",
    "\n",
    "|||LLaMA V2 7B and 7B Chat|||\n",
    "|---|---|---|---|---|\n",
    "|Instance type|Context length|Batch size| Tensor parallel degree| Data type |\n",
    "|ml.inf2.xlarge|1024|1|2|fp16|\n",
    "|ml.inf2.8xlarge|2048|1|2|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|4|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|8|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|12|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|4|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|8|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|12|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|24|fp16|\n",
    "\n",
    "\n",
    "|||LLaMA V2 13B and 13B Chat|||\n",
    "|---|---|---|---|---|\n",
    "|Instance type|Context length|Batch size| Tensor parallel degree| Data type |\n",
    "|ml.inf2.8xlarge|1024|1|2|fp16|\n",
    "|ml.inf2.24xlarge|2048|4|4|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|8|fp16|\n",
    "|ml.inf2.24xlarge|4096|4|12|fp16|\n",
    "|ml.inf2.48xlarge|2048|4|4|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|8|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|12|fp16|\n",
    "|ml.inf2.48xlarge|4096|4|24|fp16|\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c89db8",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgenerationneuron-llama-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89769446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version = \"1.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e648c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXAMPLE_ENV = {\n",
    "    \"meta-textgenerationneuron-llama-2-13b-f\": {\n",
    "        \"context_length\": \"1024\",\n",
    "        \"batch_size\": \"1\",\n",
    "        \"tensor_parallel_degree\": \"2\",\n",
    "        \"instance_type\": \"ml.inf2.8xlarge\",\n",
    "    },\n",
    "    \"meta-textgenerationneuron-llama-2-13b\": {\n",
    "        \"context_length\": \"1024\",\n",
    "        \"batch_size\": \"1\",\n",
    "        \"tensor_parallel_degree\": \"2\",\n",
    "        \"instance_type\": \"ml.inf2.8xlarge\",\n",
    "    },\n",
    "    \"meta-textgenerationneuron-llama-2-7b-f\": {\n",
    "        \"context_length\": \"2048\",\n",
    "        \"batch_size\": \"1\",\n",
    "        \"tensor_parallel_degree\": \"2\",\n",
    "        \"instance_type\": \"ml.inf2.8xlarge\",\n",
    "    },\n",
    "    \"meta-textgenerationneuron-llama-2-7b\": {\n",
    "        \"context_length\": \"2048\",\n",
    "        \"batch_size\": \"1\",\n",
    "        \"tensor_parallel_degree\": \"2\",\n",
    "        \"instance_type\": \"ml.inf2.8xlarge\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "option = \"option1\"\n",
    "\n",
    "if option == \"option1\":\n",
    "    model = JumpStartModel(model_id=model_id)\n",
    "    \n",
    "else:\n",
    "    model = JumpStartModel(\n",
    "        model_id=model_id,\n",
    "        env={\n",
    "            \"OPTION_DTYPE\": \"fp16\", ## correspond to the column `Data type`\n",
    "            \"OPTION_N_POSITIONS\": EXAMPLE_ENV[model_id][\"context_length\"], ## correspond to the column `Contexnt length`\n",
    "            \"OPTION_TENSOR_PARALLEL_DEGREE\": EXAMPLE_ENV[model_id][\"tensor_parallel_degree\"], ## correspond to the column `Tensor parallel degree`\n",
    "            \"OPTION_MAX_ROLLING_BATCH_SIZE\": EXAMPLE_ENV[model_id][\"batch_size\"], ## correspond to the column `Batch size`\n",
    "        },\n",
    "        instance_type=EXAMPLE_ENV[model_id][\"instance_type\"] ## correspond to the column `Instance type`\n",
    "        \n",
    "    )\n",
    "    \n",
    "pretrained_predictor = model.deploy(accept_eula=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response['generated_text']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to be happy. I believe that happiness is a choice. I believe that happiness is a state of mind. I believe that happiness is a state of being. I believe that happiness is a state of being. I believe that happiness is a state of being. I believe that happiness is a state of being. I believe\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(payload)\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train directory containing either a JSON lines (`.jsonl`) or text (`.txt`) formatted file. \n",
    "  - For JSON lines (JSONL) file, each line is a dictionary, repsentating a dictionary. The key in dictionary (each line) has to be 'text'.\n",
    "  - The number of files under train directory should equal to one. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "For demonstration of using text file as input, please see [Appendix 2](#2.-Use-text-file-as-input-to-fine-tune-LLaMA-2)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dc60eeb048420384541e31b84eb0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2250246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "task = \"information_extraction\"\n",
    "# To train for summarization/closed question and answering, you can replace the assertion in next line to example[\"category\"] == \"sumarization\"/\"closed_qa\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == task)\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Extract the factors that influence the cost of building a house',\n",
       " 'context': 'The cost of building a house varies by country widely. According to data from the National Association of Realtors, the median cost of buying an existing single-family house in the United States is $274,600, whereas the average cost to build is $296,652. Several different factors can impact the cost of building a house, including the size of the dwelling, the location, and availability of resources, the slope of the land, the quality of the fixtures and fittings, and the difficulty in finding construction and building materials talent',\n",
       " 'response': 'The factors that influence the cost of building a house are, the size of the dwelling, the location, and availability of resources, the slope of the land, the quality of the fixtures and fittings, and the difficulty in finding construction and building materials talent'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we use a prompt template for preprocessing the data in an instruction / input format for the training job, and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = (\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}### Response:\\n{response}\\n\\n<s>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a19a34-1afd-4483-ac9f-81574d03d9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"text\": prompt.format(instruction=sample[\"instruction\"], context=sample[\"context\"], response=sample[\"response\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b5af455-3426-4d2e-9377-a57daa2cdd79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cdf11b205b427f9b04d9f573ba4f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4166d29a4a481aa3d0d70c6e695c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_processed = train_and_test_dataset.map(apply_prompt_template, remove_columns=list(train_and_test_dataset[\"train\"].features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1804128d-c3c8-410b-9ab6-b1e0a423c20b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c72a933dc864e498bab55492db3026c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c6da03ead2422b95f8057256252b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "271518"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed[\"train\"].to_json(f\"dolly/processed-train-{task}.jsonl\")\n",
    "dataset_processed[\"test\"].to_json(f\"dolly/processed-test-{task}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Training data: s3://sagemaker-us-west-2-390840497958/dolly_dataset_trn1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = f\"dolly/processed-train-{task}.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset_trn1\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 model on the summarization dataset from Dolly on [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) instance. You have two options: `ml.trn1.32xlarge` (default) and `ml.trn1n.32xlarge`. Finetuning scripts are based on scripts provided by [Neuronx-Nemo-Megatron](https://github.com/aws-neuron/neuronx-nemo-megatron). For a list of supported hyper-parameters and their default values, please see [supported hyperparameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc9a6b0-b4df-4420-b55d-1906e215b79e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'meta-textgenerationneuron-llama-2-7b' with wildcard version identifier '1.*'. You can pin to version '1.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_input_length': '2048', 'preprocessing_num_workers': 'None', 'learning_rate': '6e-06', 'min_learning_rate': '1e-06', 'max_steps': '20', 'global_train_batch_size': '256', 'per_device_train_batch_size': '1', 'layer_norm_epilson': '1e-05', 'weight_decay': '0.1', 'lr_scheduler_type': 'CosineAnnealing', 'warmup_steps': '10', 'constant_steps': '0', 'adam_beta1': '0.9', 'adam_beta2': '0.95', 'mixed_precision': 'True', 'tensor_parallel_degree': '8', 'pipeline_parallel_degree': '1', 'append_eod': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc46b1-bf7d-4ba3-9bff-4015189ea43e",
   "metadata": {},
   "source": [
    "Overwrite some of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee59a63-656d-49cb-b72d-08ebd8afb1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_input_length': '2048', 'preprocessing_num_workers': 'None', 'learning_rate': '0.0001', 'min_learning_rate': '1e-06', 'max_steps': '25', 'global_train_batch_size': '256', 'per_device_train_batch_size': '1', 'layer_norm_epilson': '1e-05', 'weight_decay': '0.1', 'lr_scheduler_type': 'CosineAnnealing', 'warmup_steps': '10', 'constant_steps': '0', 'adam_beta1': '0.9', 'adam_beta2': '0.95', 'mixed_precision': 'True', 'tensor_parallel_degree': '8', 'pipeline_parallel_degree': '1', 'append_eod': 'False'}\n"
     ]
    }
   ],
   "source": [
    "#my_hyperparameters[\"max_input_length\"] = \"4096\" # you can increase it up to 4096 for sequence length.\n",
    "my_hyperparameters[\"max_steps\"] = \"25\"\n",
    "my_hyperparameters[\"learning_rate\"] = \"0.0001\"\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5a3e2-5b64-4fca-aec7-045ccfa4c24b",
   "metadata": {},
   "source": [
    "Validate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6685ff5b-5f63-4822-85c0-1a65cca0e713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "# estimator = JumpStartEstimator(\n",
    "#     model_id=model_id,\n",
    "#     model_version=model_version,\n",
    "#     hyperparameters=my_hyperparameters,\n",
    "#     environment={\"accept_eula\": \"true\"}, # please change `accept_eula` to be `true` to accept EULA.\n",
    "#     #instance_type=\"ml.trn1n.32xlarge\", if not specified, default `ml.trn1.32xlarge` will be used.\n",
    "# )\n",
    "\n",
    "# estimator.fit({\"train\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa8453-aca3-4b47-a5aa-be6df72ab33e",
   "metadata": {},
   "source": [
    "### If you have a training job that is already successful, then do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3603356c-a64b-462e-b7bc-1f8cba2dd4e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'meta-textgenerationneuron-llama-2-7b' with wildcard version identifier '*'. You can pin to version '1.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-12-09 18:01:32 Starting - Preparing the instances for training\n",
      "2023-12-09 18:01:32 Downloading - Downloading input data\n",
      "2023-12-09 18:01:32 Training - Training image download completed. Training in progress.\n",
      "2023-12-09 18:01:32 Uploading - Uploading generated training model\n",
      "2023-12-09 18:01:32 Completed - Training job completed\n",
      "2023-12-09 18:01:32 Starting - Preparing the instances for training\n",
      "2023-12-09 18:01:32 Downloading - Downloading input data\n",
      "2023-12-09 18:01:32 Training - Training image download completed. Training in progress.\n",
      "2023-12-09 18:01:32 Uploading - Uploading generated training model\n",
      "2023-12-09 18:01:32 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:13,926 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:13,927 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:14,376 sagemaker-training-toolkit INFO     Found 32 neurons on this instance\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:14,385 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:14,387 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:15,927 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 sagemaker-jumpstart-tabular-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:18,022 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:18,022 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:18,023 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:18,432 sagemaker-training-toolkit INFO     Found 32 neurons on this instance\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:18,443 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:18,895 sagemaker-training-toolkit INFO     Found 32 neurons on this instance\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:18,906 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:19,385 sagemaker-training-toolkit INFO     Found 32 neurons on this instance\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:19,395 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.trn1.32xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.95\",\n",
      "        \"append_eod\": \"False\",\n",
      "        \"constant_steps\": \"0\",\n",
      "        \"global_train_batch_size\": \"256\",\n",
      "        \"layer_norm_epilson\": \"1e-05\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lr_scheduler_type\": \"CosineAnnealing\",\n",
      "        \"max_input_length\": \"2048\",\n",
      "        \"max_steps\": \"25\",\n",
      "        \"min_learning_rate\": \"1e-06\",\n",
      "        \"mixed_precision\": \"True\",\n",
      "        \"per_device_train_batch_size\": \"1\",\n",
      "        \"pipeline_parallel_degree\": \"1\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"tensor_parallel_degree\": \"8\",\n",
      "        \"warmup_steps\": \"10\",\n",
      "        \"weight_decay\": \"0.1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.trn1.32xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"meta-textgenerationneuron-llama-2-7b-2023-12-09-17-28-58-457\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 128,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 32,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.trn1.32xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.trn1.32xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.95\",\"append_eod\":\"False\",\"constant_steps\":\"0\",\"global_train_batch_size\":\"256\",\"layer_norm_epilson\":\"1e-05\",\"learning_rate\":\"0.0001\",\"lr_scheduler_type\":\"CosineAnnealing\",\"max_input_length\":\"2048\",\"max_steps\":\"25\",\"min_learning_rate\":\"1e-06\",\"mixed_precision\":\"True\",\"per_device_train_batch_size\":\"1\",\"pipeline_parallel_degree\":\"1\",\"preprocessing_num_workers\":\"None\",\"tensor_parallel_degree\":\"8\",\"warmup_steps\":\"10\",\"weight_decay\":\"0.1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.trn1.32xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.32xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.trn1.32xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.32xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=128\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=32\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.trn1.32xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.95\",\"append_eod\":\"False\",\"constant_steps\":\"0\",\"global_train_batch_size\":\"256\",\"layer_norm_epilson\":\"1e-05\",\"learning_rate\":\"0.0001\",\"lr_scheduler_type\":\"CosineAnnealing\",\"max_input_length\":\"2048\",\"max_steps\":\"25\",\"min_learning_rate\":\"1e-06\",\"mixed_precision\":\"True\",\"per_device_train_batch_size\":\"1\",\"pipeline_parallel_degree\":\"1\",\"preprocessing_num_workers\":\"None\",\"tensor_parallel_degree\":\"8\",\"warmup_steps\":\"10\",\"weight_decay\":\"0.1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.32xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"meta-textgenerationneuron-llama-2-7b-2023-12-09-17-28-58-457\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":128,\"num_gpus\":0,\"num_neurons\":32,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.trn1.32xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.32xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.95\",\"--append_eod\",\"False\",\"--constant_steps\",\"0\",\"--global_train_batch_size\",\"256\",\"--layer_norm_epilson\",\"1e-05\",\"--learning_rate\",\"0.0001\",\"--lr_scheduler_type\",\"CosineAnnealing\",\"--max_input_length\",\"2048\",\"--max_steps\",\"25\",\"--min_learning_rate\",\"1e-06\",\"--mixed_precision\",\"True\",\"--per_device_train_batch_size\",\"1\",\"--pipeline_parallel_degree\",\"1\",\"--preprocessing_num_workers\",\"None\",\"--tensor_parallel_degree\",\"8\",\"--warmup_steps\",\"10\",\"--weight_decay\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.95\u001b[0m\n",
      "\u001b[34mSM_HP_APPEND_EOD=False\u001b[0m\n",
      "\u001b[34mSM_HP_CONSTANT_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_GLOBAL_TRAIN_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_LAYER_NORM_EPILSON=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=CosineAnnealing\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=2048\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=25\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_LEARNING_RATE=1e-06\u001b[0m\n",
      "\u001b[34mSM_HP_MIXED_PRECISION=True\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_PARALLEL_DEGREE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_TENSOR_PARALLEL_DEGREE=8\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python310.zip:/usr/local/lib/python3.10:/usr/local/lib/python3.10/lib-dynload:/usr/local/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.95 --append_eod False --constant_steps 0 --global_train_batch_size 256 --layer_norm_epilson 1e-05 --learning_rate 0.0001 --lr_scheduler_type CosineAnnealing --max_input_length 2048 --max_steps 25 --min_learning_rate 1e-06 --mixed_precision True --per_device_train_batch_size 1 --pipeline_parallel_degree 1 --preprocessing_num_workers None --tensor_parallel_degree 8 --warmup_steps 10 --weight_decay 0.1\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:19,395 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:36:19,395 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers_neuronx/transformers_neuronx-0.8.268-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/site-packages (from transformers-neuronx==0.8.268) (0.23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch-neuronx in /usr/local/lib/python3.10/site-packages (from transformers-neuronx==0.8.268) (1.13.1.1.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (from transformers-neuronx==0.8.268) (4.33.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (1.21.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/site-packages (from accelerate->transformers-neuronx==0.8.268) (0.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch-xla==1.13.1+torchneuronb in /usr/local/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx==0.8.268) (1.13.1+torchneuronb)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: libneuronxla==0.5.476 in /usr/local/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx==0.8.268) (0.5.476)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<5 in /usr/local/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx==0.8.268) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aws-neuronx-runtime-discovery~=2.0 in /usr/local/lib/python3.10/site-packages (from libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: neuronx-cc~=2.0 in /usr/local/lib/python3.10/site-packages (from libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.10.0.35+3817a0c8c)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3~=1.26 in /usr/local/lib/python3.10/site-packages (from libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (1.28.57)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore~=1.29 in /usr/local/lib/python3.10/site-packages (from libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (1.31.57)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.10/site-packages (from torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (68.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate->transformers-neuronx==0.8.268) (0.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (3.12.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (2023.8.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers->transformers-neuronx==0.8.268) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from huggingface-hub->accelerate->transformers-neuronx==0.8.268) (2023.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx==0.8.268) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx==0.8.268) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx==0.8.268) (1.26.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx==0.8.268) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/site-packages (from boto3~=1.26->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from boto3~=1.26->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/site-packages (from botocore~=1.29->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauth2client in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (4.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.35.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.34.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: neuronx-hwm==2.10.0.5 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.10.0.5+7b1976adf)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx<=2.6.3 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<=1.7.3 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (1.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-daemon>=2.2.4 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-unixsocket>=0.1.5 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: islpy<=2023.1,>2021.1 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2023.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pgzip>=0.3.0 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ec2-metadata<=2.10.0 in /usr/local/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils in /usr/local/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc~=2.0->libneuronxla==0.5.476->torch-neuronx->transformers-neuronx==0.8.268) (0.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (1.60.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (4.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneuronb->torch-neuronx->transformers-neuronx==0.8.268) (3.1.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: transformers-neuronx\u001b[0m\n",
      "\u001b[34mSuccessfully installed transformers-neuronx-0.8.268\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/Cython/Cython-3.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: Cython\u001b[0m\n",
      "\u001b[34mSuccessfully installed Cython-3.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/soupsieve/soupsieve-2.5-py3-none-any.whl (from -r extra_requirement.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sentence-transformers/sentence-transformers-2.2.2.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyarrow/pyarrow-13.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/omegaconf/omegaconf-2.2.3-py3-none-any.whl (from -r extra_requirement.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/portalocker/portalocker-2.8.2-py3-none-any.whl (from -r extra_requirement.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/einops/einops-0.7.0-py3-none-any.whl (from -r extra_requirement.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sacrebleu/sacrebleu-2.3.1-py3-none-any.whl (from -r extra_requirement.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ftfy/ftfy-6.1.1-py3-none-any.whl (from -r extra_requirement.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pydantic/pydantic-2.4.2-py3-none-any.whl (from -r extra_requirement.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ijson/ijson-3.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/OpenCC/OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl (from -r extra_requirement.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/GPUtil/GPUtil-1.4.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/faiss_cpu/faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/smmap/smmap-5.0.1-py3-none-any.whl (from -r extra_requirement.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/aiohttp/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nltk/nltk-3.8.1-py3-none-any.whl (from -r extra_requirement.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/rapidfuzz/rapidfuzz-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/frozenlist/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathtools/pathtools-0.1.2.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ipadic/ipadic-1.0.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightning_utilities/lightning_utilities-0.9.0-py3-none-any.whl (from -r extra_requirement.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/async_timeout/async_timeout-4.0.3-py3-none-any.whl (from -r extra_requirement.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/braceexpand/braceexpand-0.1.7-py2.py3-none-any.whl (from -r extra_requirement.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/yarl/yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lxml/lxml-4.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (from -r extra_requirement.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/antlr4-python3-runtime/antlr4-python3-runtime-4.9.3.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mecab_python3/mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/hydra_core/hydra_core-1.2.0-py3-none-any.whl (from -r extra_requirement.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflect/inflect-7.0.0-py3-none-any.whl (from -r extra_requirement.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/setproctitle/setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.5-py3-none-any.whl (from -r extra_requirement.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tensorboardX/tensorboardX-2.6.2.2-py2.py3-none-any.whl (from -r extra_requirement.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sacremoses/sacremoses-0.0.53.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/PyYAML/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/GitPython/GitPython-3.1.37-py3-none-any.whl (from -r extra_requirement.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sentry_sdk/sentry_sdk-1.32.0-py2.py3-none-any.whl (from -r extra_requirement.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dill/dill-0.3.7-py3-none-any.whl (from -r extra_requirement.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multidict/multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/appdirs/appdirs-1.4.4-py2.py3-none-any.whl (from -r extra_requirement.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docker_pycreds/docker_pycreds-0.4.0-py2.py3-none-any.whl (from -r extra_requirement.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pydantic_core/pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/xxhash/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r extra_requirement.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/gdown/gdown-4.7.1-py3-none-any.whl (from -r extra_requirement.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/jieba/jieba-0.42.1.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybind11/pybind11-2.11.1-py3-none-any.whl (from -r extra_requirement.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r extra_requirement.txt (line 46))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torchmetrics/torchmetrics-0.10.3-py3-none-any.whl (from -r extra_requirement.txt (line 47))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch_lightning/pytorch_lightning-1.8.6-py3-none-any.whl (from -r extra_requirement.txt (line 48))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/beautifulsoup4/beautifulsoup4-4.12.2-py3-none-any.whl (from -r extra_requirement.txt (line 49))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/youtokentome/youtokentome-1.0.6.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tabulate/tabulate-0.9.0-py3-none-any.whl (from -r extra_requirement.txt (line 51))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/aiosignal/aiosignal-1.3.1-py3-none-any.whl (from -r extra_requirement.txt (line 52))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/PySocks/PySocks-1.7.1-py3-none-any.whl (from -r extra_requirement.txt (line 53))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/annotated_types/annotated_types-0.6.0-py3-none-any.whl (from -r extra_requirement.txt (line 54))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pangu/pangu-4.0.6.1-py3-none-any.whl (from -r extra_requirement.txt (line 55))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/gitdb/gitdb-4.0.10-py3-none-any.whl (from -r extra_requirement.txt (line 56))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wandb/wandb-0.15.12-py3-none-any.whl (from -r extra_requirement.txt (line 57))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fsspec/fsspec-2023.6.0-py3-none-any.whl (from -r extra_requirement.txt (line 58))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/webdataset/webdataset-0.1.62-py3-none-any.whl (from -r extra_requirement.txt (line 59))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wcwidth/wcwidth-0.2.8-py2.py3-none-any.whl (from -r extra_requirement.txt (line 60))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (1.21.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (1.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (0.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex in /usr/local/lib/python3.10/site-packages (from sacrebleu==2.3.1->-r extra_requirement.txt (line 7)) (2023.8.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /usr/local/lib/python3.10/site-packages (from sacrebleu==2.3.1->-r extra_requirement.txt (line 7)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/site-packages (from pydantic==2.4.2->-r extra_requirement.txt (line 9)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.6->-r extra_requirement.txt (line 15)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.6->-r extra_requirement.txt (line 15)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk==3.8.1->-r extra_requirement.txt (line 16)) (8.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk==3.8.1->-r extra_requirement.txt (line 16)) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/site-packages (from lightning-utilities==0.9.0->-r extra_requirement.txt (line 21)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/site-packages (from yarl==1.9.2->-r extra_requirement.txt (line 24)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.14.5->-r extra_requirement.txt (line 31)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.14.5->-r extra_requirement.txt (line 31)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/site-packages (from datasets==2.14.5->-r extra_requirement.txt (line 31)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/site-packages (from tensorboardX==2.6.2.2->-r extra_requirement.txt (line 32)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from sacremoses==0.0.53->-r extra_requirement.txt (line 33)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from sentry-sdk==1.32.0->-r extra_requirement.txt (line 36)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.26.11 in /usr/local/lib/python3.10/site-packages (from sentry-sdk==1.32.0->-r extra_requirement.txt (line 36)) (1.26.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown==4.7.1->-r extra_requirement.txt (line 43)) (3.12.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.31.0->-r extra_requirement.txt (line 46)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.31.0->-r extra_requirement.txt (line 46)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/site-packages (from wandb==0.15.12->-r extra_requirement.txt (line 57)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb==0.15.12->-r extra_requirement.txt (line 57)) (68.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (0.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.14.5->-r extra_requirement.txt (line 31)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.14.5->-r extra_requirement.txt (line 31)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.14.5->-r extra_requirement.txt (line 31)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2->-r extra_requirement.txt (line 2)) (10.0.1)\u001b[0m\n",
      "\u001b[34mPyYAML is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mdill is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sentence-transformers, antlr4-python3-runtime, GPUtil, pathtools, ipadic, sacremoses, jieba, youtokentome\u001b[0m\n",
      "\u001b[34mBuilding wheel for sentence-transformers (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sentence-transformers (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=f1633b194768da30444fe6181ffab5ba87f969dad23528bd6d6f87475c01b842\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e6/4d/bd/2759d6378cdae8c5bf306a568a08ca046e22f4a783b920c77a\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=8f22d7b544605508a462dd520034203c761a1ebcada90f5708012f6ee8326841\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/fe/0a/e5/208460ef4014a049ccce20703f142336caa9e4ecf1391c7ccf\u001b[0m\n",
      "\u001b[34mBuilding wheel for GPUtil (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for GPUtil (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=b7a19ba844574005a5f4b04b2e63811aef6bfe552975e0ec1962f36add9d44b1\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7e/0c/c1/5539f46eefa0b6fd0d7bc28e14c8bc396f84f2d68b2864dfbd\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=cdf0092a3c7b4cfbab3e87fbfdc32c6db19cf989e4264462b326909ff6be6270\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/07/68/48/289b55053de539a27e3d2588a5c5dc0d269353b7b749a24474\u001b[0m\n",
      "\u001b[34mBuilding wheel for ipadic (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ipadic (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556703 sha256=1e65ef85ef676ea73d7a0cbac8f430e3cdfd57e67ca9f2cc554e919553e7fb9d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/07/4e/dc/9c0933c272e2186098f1e25735168913d25a1142f68b5edd1a\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=09341f5babf4e8103b95bb4f6108f882fcca7f097a508932656a891cc7f385a3\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/12/5f/2e/8f4084e4d365d4e0fdececab575df828dacca9804451a79a11\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=a76c525c3ac185b6440d7b270d4af8ec5e1022d046f6f0d577e9f322f7c963fe\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/fe/c1/04/36aebeeee83a5f6ac34f8f3f47fdabba32c1a84437756065be\u001b[0m\n",
      "\u001b[34mBuilding wheel for youtokentome (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for youtokentome (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=1984766 sha256=1c945da600310e8e7cfe4b54829e530af460cb54f99b4c6f19e7474b0755e7b7\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5d/03/1c/07fbb33096f7d93a0266c39d6dfe0ad92339e3ec69f02969fa\u001b[0m\n",
      "\u001b[34mSuccessfully built sentence-transformers antlr4-python3-runtime GPUtil pathtools ipadic sacremoses jieba youtokentome\u001b[0m\n",
      "\u001b[34mInstalling collected packages: wcwidth, pathtools, pangu, OpenCC, mecab-python3, jieba, ipadic, ijson, GPUtil, faiss-cpu, braceexpand, appdirs, antlr4-python3-runtime, youtokentome, xxhash, webdataset, tensorboardX, tabulate, soupsieve, smmap, setproctitle, sentry-sdk, sacremoses, rapidfuzz, PySocks, pydantic-core, pybind11, pyarrow, portalocker, nltk, multidict, lxml, lightning-utilities, ftfy, fsspec, frozenlist, einops, docker-pycreds, async-timeout, annotated-types, yarl, sacrebleu, pydantic, omegaconf, gitdb, beautifulsoup4, aiosignal, transformers, inflect, hydra-core, GitPython, gdown, aiohttp, wandb, torchmetrics, sentence-transformers, pytorch-lightning, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2023.9.2\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2023.9.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2023.9.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.33.3\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.33.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.33.3\u001b[0m\n",
      "\u001b[34mSuccessfully installed GPUtil-1.4.0 GitPython-3.1.37 OpenCC-1.1.6 PySocks-1.7.1 aiohttp-3.8.6 aiosignal-1.3.1 annotated-types-0.6.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 async-timeout-4.0.3 beautifulsoup4-4.12.2 braceexpand-0.1.7 datasets-2.14.5 docker-pycreds-0.4.0 einops-0.7.0 faiss-cpu-1.7.4 frozenlist-1.4.0 fsspec-2023.6.0 ftfy-6.1.1 gdown-4.7.1 gitdb-4.0.10 hydra-core-1.2.0 ijson-3.2.3 inflect-7.0.0 ipadic-1.0.0 jieba-0.42.1 lightning-utilities-0.9.0 lxml-4.9.3 mecab-python3-1.0.8 multidict-6.0.4 nltk-3.8.1 omegaconf-2.2.3 pangu-4.0.6.1 pathtools-0.1.2 portalocker-2.8.2 pyarrow-13.0.0 pybind11-2.11.1 pydantic-2.4.2 pydantic-core-2.10.1 pytorch-lightning-1.8.6 rapidfuzz-3.4.0 sacrebleu-2.3.1 sacremoses-0.0.53 sentence-transformers-2.2.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 soupsieve-2.5 tabulate-0.9.0 tensorboardX-2.6.2.2 torchmetrics-0.10.3 transformers-4.31.0 wandb-0.15.12 wcwidth-0.2.8 webdataset-0.1.62 xxhash-3.4.1 yarl-1.9.2 youtokentome-1.0.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/onnx/onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from onnx==1.14.1) (1.21.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/site-packages (from onnx==1.14.1) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/site-packages (from onnx==1.14.1) (4.8.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: onnx\u001b[0m\n",
      "\u001b[34mSuccessfully installed onnx-1.14.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ruamel.yaml.clib/ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: ruamel.yaml.clib\u001b[0m\n",
      "\u001b[34mSuccessfully installed ruamel.yaml.clib-0.2.8\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ruamel.yaml/ruamel.yaml-0.17.35-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/site-packages (from ruamel.yaml==0.17.35) (0.2.8)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: ruamel.yaml\u001b[0m\n",
      "\u001b[34mSuccessfully installed ruamel.yaml-0.17.35\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/setuptools/setuptools-59.5.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: setuptools\u001b[0m\n",
      "\u001b[34mAttempting uninstall: setuptools\u001b[0m\n",
      "\u001b[34mFound existing installation: setuptools 68.2.2\u001b[0m\n",
      "\u001b[34mUninstalling setuptools-68.2.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled setuptools-68.2.2\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpython-daemon 3.0.1 requires setuptools>=62.4.0, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed setuptools-59.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/text_unidecode/text_unidecode-1.3-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: text-unidecode\u001b[0m\n",
      "\u001b[34mSuccessfully installed text-unidecode-1.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wget/wget-3.2.zip\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: wget\u001b[0m\n",
      "\u001b[34mBuilding wheel for wget (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for wget (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=e632319b0eb02cee171a26a30b4abdb1f8a5a310a6500e2b2d077c150ce3c548\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/51/ff/3e/aa0659a53eabadfc50e4ee9ccb3297c417e59743edba43d42f\u001b[0m\n",
      "\u001b[34mSuccessfully built wget\u001b[0m\n",
      "\u001b[34mInstalling collected packages: wget\u001b[0m\n",
      "\u001b[34mSuccessfully installed wget-3.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wrapt/wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: wrapt\u001b[0m\n",
      "\u001b[34mSuccessfully installed wrapt-1.15.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./build/apex-0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./build/nemo_toolkit-1.14.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>20.6 in /usr/local/lib/python3.10/site-packages (from apex==0.1) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (0.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (0.56.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.21.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (0.17.35)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools==59.5.0 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (59.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (2.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: text-unidecode in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wget in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wrapt in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch-lightning==1.8.6 in /usr/local/lib/python3.10/site-packages (from nemo-toolkit==1.14.0) (1.8.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (2.6.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/site-packages (from onnx>=1.7.0->nemo-toolkit==1.14.0) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch->nemo-toolkit==1.14.0) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch->nemo-toolkit==1.14.0) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch->nemo-toolkit==1.14.0) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch->nemo-toolkit==1.14.0) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->nemo-toolkit==1.14.0) (0.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub->nemo-toolkit==1.14.0) (3.12.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub->nemo-toolkit==1.14.0) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/site-packages (from numba->nemo-toolkit==1.14.0) (0.39.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil->nemo-toolkit==1.14.0) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/site-packages (from ruamel.yaml->nemo-toolkit==1.14.0) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->nemo-toolkit==1.14.0) (1.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn->nemo-toolkit==1.14.0) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->nemo-toolkit==1.14.0) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (1.59.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (1.35.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (0.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (3.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/site-packages (from tensorboard->nemo-toolkit==1.14.0) (2.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (3.8.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<2,>=1.6.3->tensorboard->nemo-toolkit==1.14.0) (4.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<2,>=1.6.3->tensorboard->nemo-toolkit==1.14.0) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<2,>=1.6.3->tensorboard->nemo-toolkit==1.14.0) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->nemo-toolkit==1.14.0) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->nemo-toolkit==1.14.0) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->nemo-toolkit==1.14.0) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->nemo-toolkit==1.14.0) (1.26.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->nemo-toolkit==1.14.0) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=0.11.15->tensorboard->nemo-toolkit==1.14.0) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->nemo-toolkit==1.14.0) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->nemo-toolkit==1.14.0) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->nemo-toolkit==1.14.0) (3.2.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: apex, nemo-toolkit\u001b[0m\n",
      "\u001b[34mSuccessfully installed apex-0.1 nemo-toolkit-1.14.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/numpy/numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy\u001b[0m\n",
      "\u001b[34mAttempting uninstall: numpy\u001b[0m\n",
      "\u001b[34mFound existing installation: numpy 1.21.6\u001b[0m\n",
      "\u001b[34mUninstalling numpy-1.21.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled numpy-1.21.6\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mneuronx-cc 2.10.0.35+3817a0c8c requires numpy<=1.21.6,>=1.20, but you have numpy 1.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed numpy-1.22.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightning_fabric/lightning_fabric-2.0.9-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (1.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/site-packages (from lightning-fabric==2.0.9) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (3.8.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->lightning-fabric==2.0.9) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->lightning-fabric==2.0.9) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->lightning-fabric==2.0.9) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->lightning-fabric==2.0.9) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->lightning-fabric==2.0.9) (59.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /usr/local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->lightning-fabric==2.0.9) (0.41.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (1.26.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->lightning-fabric==2.0.9) (2023.7.22)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: lightning-fabric\u001b[0m\n",
      "\u001b[34mSuccessfully installed lightning-fabric-2.0.9\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m/opt/ml/code/nemo/nemo/collections/nlp/data/language_modeling/megatron /opt/ml/code\u001b[0m\n",
      "\u001b[34mg++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/local/include/python3.10 -I/usr/local/lib/python3.10/site-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so\u001b[0m\n",
      "\u001b[34m/opt/ml/code\u001b[0m\n",
      "\u001b[34m{'architectures': ['LlamaForCausalLM'], 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 11008, 'max_position_embeddings': 2048, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'num_key_value_heads': 32, 'pad_token_id': 0, 'rms_norm_eps': 1e-05, 'tie_word_embeddings': False, 'torch_dtype': 'float16', 'transformers_version': '4.28.1', 'use_cache': True, 'vocab_size': 32000}\u001b[0m\n",
      "\u001b[34mLoading /opt/ml/additonals3data/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mLoading /opt/ml/additonals3data/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34m323\u001b[0m\n",
      "\u001b[34mLoaded Llama model\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 0 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1\u001b[0m\n",
      "\u001b[34mtorch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 1 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1\u001b[0m\n",
      "\u001b[34mtorch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 2 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 3 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 4 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 5 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1\u001b[0m\n",
      "\u001b[34mtorch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 6 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34m=== PP 0, TP 7 ===\u001b[0m\n",
      "\u001b[34mmodel.language_model.embedding.word_embeddings.weight 1 torch.Size([4000, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64])\u001b[0m\n",
      "\u001b[34mtorch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.input_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.post_attention_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.post_attention_layernorm.weight 0\u001b[0m\n",
      "\u001b[34mtorch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq 0 torch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.post_attention_layernorm.weight 0 torch.Size([4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.dense.weight 1 torch.Size([4096, 512]) torch.Size([4096, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq 0\u001b[0m\n",
      "\u001b[34mtorch.Size([64]) torch.Size([64])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight 1 torch.Size([4096, 1376]) torch.Size([4096, 11008])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight 1 torch.Size([1376, 4096]) torch.Size([11008, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.input_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.final_layernorm.weight 0 torch.Size([4096]) torch.Size([4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.output_layer.weight 1 torch.Size([4000, 4096]) torch.Size([32000, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.0.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.1.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.2.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.3.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.4.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.5.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.6.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.7.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.8.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.9.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.10.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.11.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.12.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.13.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.14.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.15.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.16.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.17.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.18.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.19.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.20.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.21.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.22.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.23.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.24.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.25.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.26.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.27.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.28.self_attention.query_key_value.weight 1 torch.Size([1536, 4096])\u001b[0m\n",
      "\u001b[34mtorch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.29.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.30.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mmodel.language_model.encoder.layers.31.self_attention.query_key_value.weight 1 torch.Size([1536, 4096]) torch.Size([12288, 4096])\u001b[0m\n",
      "\u001b[34mDone saving Megatron checkpoint\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:38:12 optimizers:67] Could not import distributed_fused_adam optimizer from Apex\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:38:13 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:38:13 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mVocab size: 32000\u001b[0m\n",
      "\u001b[34mOutput prefix: tmp/tokenized_data\u001b[0m\n",
      "\u001b[34mTime to startup: 0.012140989303588867\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:13 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mProcessing file /opt/ml/input/data/train/processed-train-information_extraction.jsonl 1/1\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:14 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mProcessed 100 documents (87.83124890585357 docs/s, 0.16090842004836442 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 200 documents\u001b[0m\n",
      "\u001b[34m(174.58433604174735 docs/s, 0.309820198243336 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 300 documents\u001b[0m\n",
      "\u001b[34m(260.0848862815683 docs/s, 0.4560378437214371 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 400 documents (344.7739886304345 docs/s, 0.5877321060247025 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 500 documents (428.39796900560043 docs/s, 0.7222703405854062 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 600 documents (511.0058922644384 docs/s, 0.8607368234241574 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 700 documents (592.8194053013492 docs/s, 1.0025138179220479 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 800 documents (673.7015239906453 docs/s, 1.1435005525429827 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 900 documents (753.5629950850077 docs/s, 1.2874241396168418 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 1000 documents (832.8689740270546 docs/s, 1.4165887770203602 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 1100 documents (910.7713299369253 docs/s, 1.5655666768790528 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 1200 documents (987.8557970104258 docs/s, 1.7022005715362392 MB/s).\u001b[0m\n",
      "\u001b[34mProcessed 1300 documents (1064.6798976909786 docs/s, 1.8299092016221514 MB/s).\u001b[0m\n",
      "\u001b[34m2023-12-09 17:38:15.000975:  739  INFO ||NEURON_PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\u001b[0m\n",
      "\u001b[34m./fine_tunin\u001b[0m\n",
      "\u001b[34mg_trn1.sh: line 6:\u001b[0m\n",
      "\u001b[34msudo: command not found\u001b[0m\n",
      "\u001b[34m--nproc_per_n\u001b[0m\n",
      "\u001b[34mode 32 --nnodes\u001b[0m\n",
      "\u001b[34m1 --node_rank 0 --master_addr localhost\u001b[0m\n",
      "\u001b[34m--master_port 41000\u001b[0m\n",
      "\u001b[34mSEQ_LEN=2048, HS=4096, FFN_HS=110\u001b[0m\n",
      "\u001b[34m08 TP=8 PP=1 N_\u001b[0m\n",
      "\u001b[34mLAYERS=32 N_AH=32 GBS=256 UBS=1 MIN_L\u001b[0m\n",
      "\u001b[34mR=1e-06\u001b[0m\n",
      "\u001b[34mINIT_METHOD_STD=, HIDDEN_DRO\u001b[0m\n",
      "\u001b[34mPOUT=, LAYERNORM_EPSILON=1e-05, OPTIM_\u001b[0m\n",
      "\u001b[34mNAME=, OPTIM_LR=0.0001\u001b[0m\n",
      "\u001b[34mOPTIM_WEIGHT_DECAY=0.1, OPTIM_\u001b[0m\n",
      "\u001b[34mSCHED_NAME=CosineAnnealing\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m**************************\u001b[0m\n",
      "\u001b[34m***************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variabl\u001b[0m\n",
      "\u001b[34me for each process to be 1 in default, to\u001b[0m\n",
      "\u001b[34mavoid your system being overloaded, please further tune th\u001b[0m\n",
      "\u001b[34me variable for optimal performance in your a\u001b[0m\n",
      "\u001b[34mpplication as needed. \u001b[0m\n",
      "\u001b[34m******************\u001b[0m\n",
      "\u001b[34m***********************\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:38:19 optimizers:67] Could not import d\u001b[0m\n",
      "\u001b[34mistributed_fused_adam optimizer from Apex\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:38:\u001b[0m\n",
      "\u001b[34m20 experimental:27] Module\u001b[0m\n",
      "\u001b[34m<class 'nemo.collections.nlp.data.language_modeling.megatron\u001b[0m\n",
      "\u001b[34m.megatron_batch_samplers.MegatronPretrainin\u001b[0m\n",
      "\u001b[34mgRandomBatchSampler'> is experimental, not\u001b[0m\n",
      "\u001b[34mready for production and is not fully su\u001b[0m\n",
      "\u001b[34mpported. Use at your own risk.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:\u001b[0m\n",
      "\u001b[34m38:21 experimental:27]\u001b[0m\n",
      "\u001b[34mModule <class 'nemo.collections.nlp.models.text_normaliza\u001b[0m\n",
      "\u001b[34mtion_as_tagging.thutmose_tagger.ThutmoseT\u001b[0m\n",
      "\u001b[34maggerModel'> is experimental, not ready f\u001b[0m\n",
      "\u001b[34mor production and is not fully supporte\u001b[0m\n",
      "\u001b[34md. Use at your own risk.\u001b[0m\n",
      "\u001b[34mprecisi\u001b[0m\n",
      "\u001b[34mon plugin:<pytorch_l\u001b[0m\n",
      "\u001b[34mightning.plugins.precision.tpu.TPUPrecisionPlugi\u001b[0m\n",
      "\u001b[34mn object at 0x7f547a68c370>\u001b[0m\n",
      "\u001b[34mprecis\u001b[0m\n",
      "\u001b[34mion plugin:<pyto\u001b[0m\n",
      "\u001b[34mrch_lightning.plugins.precision.tpu.T\u001b[0m\n",
      "\u001b[34mPUPrecisionPlugin object at 0x7f3e4e81\u001b[0m\n",
      "\u001b[34m66e0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning\u001b[0m\n",
      "\u001b[34m.plugins.precision.tpu.TPUPrecision\u001b[0m\n",
      "\u001b[34mPlugin object at 0x7f429fce97e0>\u001b[0m\n",
      "\u001b[34mpre\u001b[0m\n",
      "\u001b[34mcision plugin:<pytorch_lightning.plugi\u001b[0m\n",
      "\u001b[34mns.precision.tpu.TPUPrecisionPlugin object at 0x7fd\u001b[0m\n",
      "\u001b[34me46dc3310>\u001b[0m\n",
      "\u001b[34mprecisio\u001b[0m\n",
      "\u001b[34mn plugin:<pytorc\u001b[0m\n",
      "\u001b[34mh_lightning.plugins.precision.tpu.TP\u001b[0m\n",
      "\u001b[34mUPrecisionPlugin object at 0x7f992924f\u001b[0m\n",
      "\u001b[34ma00>\u001b[0m\n",
      "\u001b[34mprec\u001b[0m\n",
      "\u001b[34mision plugin:<p\u001b[0m\n",
      "\u001b[34mytorch_lightning.plugins.precision.\u001b[0m\n",
      "\u001b[34mtpu.TPUPrecisionPlugin object at 0x7f\u001b[0m\n",
      "\u001b[34md7578c33d0>\u001b[0m\n",
      "\u001b[34m[NeM\u001b[0m\n",
      "\u001b[34mo W 2023-12-09\u001b[0m\n",
      "\u001b[34m17:38:21 nemo_logging:349] /usr/local\u001b[0m\n",
      "\u001b[34m/lib/python3.10/site-packages/hydra/_\u001b[0m\n",
      "\u001b[34minternal/hydra.py:119: UserWarning: Fut\u001b[0m\n",
      "\u001b[34mure Hydra versions will no longer cha\u001b[0m\n",
      "\u001b[34mnge working directory at job runtime\u001b[0m\n",
      "\u001b[34mby default.\n",
      "    See https://hydra.cc/\u001b[0m\n",
      "\u001b[34mdocs/next/upgrades/1.1_to_1.2/changes\u001b[0m\n",
      "\u001b[34m_to_job_working_dir/ for more information.\n",
      "      ret\u001b[0m\n",
      "\u001b[34m= run_job(\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-\u001b[0m\n",
      "\u001b[34m12-09 17:38:21 m\u001b[0m\n",
      "\u001b[34megatron_gpt_pretraining:58] \n",
      "    \n",
      "    *\u001b[0m\n",
      "\u001b[34m************* Experiment configuration\u001b[0m\n",
      "\u001b[34m***********\u001b[0m\n",
      "\u001b[34m[NeMo I 2023\u001b[0m\n",
      "\u001b[34m-12-09 17:38:21\u001b[0m\n",
      "\u001b[34mmegatron_gpt_pretraining:59] \n",
      "    name: m\u001b[0m\n",
      "\u001b[34megatron_llama\n",
      "    restore_from_path: n\u001b[0m\n",
      "\u001b[34mull\n",
      "    trainer:\n",
      "      devices: 32\n",
      "      num_nodes: 1\u001b[0m\n",
      "\u001b[34maccelerator: tpu\n",
      "      precision: 32\n",
      "      log\u001b[0m\n",
      "\u001b[34mger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\u001b[0m\n",
      "\u001b[34mmax_epochs: null\n",
      "      max_steps: 3\n",
      "      log_every_n_steps: 1\u001b[0m\n",
      "\u001b[34mval_check_interval: 0.99\n",
      "      limit_val_batches: 1\n",
      "      limit_te\u001b[0m\n",
      "\u001b[34mst_batches: 1\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_va\u001b[0m\n",
      "\u001b[34ml: 1.0\n",
      "      benchmark: false\n",
      "      enable_model_summary: false\n",
      "    e\u001b[0m\n",
      "\u001b[34mxp_manager:\n",
      "      create_tensorboard_logger: false\n",
      "      explicit_lo\u001b[0m\n",
      "\u001b[34mg_dir: null\n",
      "      exp_dir: /tmp\n",
      "      name: megatron_llama\n",
      "      create_wandb_logg\u001b[0m\n",
      "\u001b[34mer: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        n\u001b[0m\n",
      "\u001b[34mame: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: false\u001b[0m\n",
      "\u001b[34mcreate_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        mon\u001b[0m\n",
      "\u001b[34mitor: step\n",
      "        save_top_k: -1\n",
      "        mode: max\n",
      "        save_last\u001b[0m\n",
      "\u001b[34m: true\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end\u001b[0m\n",
      "\u001b[34m: false\n",
      "        filename: megatron_llama--{step}-{consumed_samples}\n",
      "        model_p\u001b[0m\n",
      "\u001b[34marallel_size: ${multiply:${model.tensor_model_parallel_size}, ${mod\u001b[0m\n",
      "\u001b[34mel.pipeline_model_parallel_size}}\n",
      "        train_time_interval: 36000\u001b[0m\n",
      "\u001b[34mmodel:\n",
      "      micro_batch_size: 1\u001b[0m\n",
      "\u001b[34mglobal_batch_size: 256\n",
      "      tensor_model_parallel\u001b[0m\n",
      "\u001b[34m_size: 8\n",
      "      pipeline_model_parallel_size: 1\u001b[0m\n",
      "\u001b[34mvirtual_pipeline_model_parallel_size: null\n",
      "      en\u001b[0m\n",
      "\u001b[34mcoder_seq_length: 2048\n",
      "      max_position_embedding\u001b[0m\n",
      "\u001b[34ms: 2048\n",
      "      num_layers: 32\n",
      "      hidden_size: 4096\u001b[0m\n",
      "\u001b[34mffn_hidden_size: 11008\n",
      "      num_attention_head\u001b[0m\n",
      "\u001b[34ms: 32\n",
      "      init_metho\u001b[0m\n",
      "\u001b[34md_std: 0.021\n",
      "      use_scaled_init_method: true\u001b[0m\n",
      "\u001b[34mhidden_dropout: 0\n",
      "      attention_dropout: 0\u001b[0m\n",
      "\u001b[34mffn_dropout: 0\n",
      "      kv_channels: null\n",
      "      apply\u001b[0m\n",
      "\u001b[34m_query_key_layer_scaling: true\n",
      "      normalization: r\u001b[0m\n",
      "\u001b[34mmsnorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_\u001b[0m\n",
      "\u001b[34mdecay: false\n",
      "      make_vocab_size_divisible_by: 8\u001b[0m\n",
      "\u001b[34mpre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_\u001b[0m\n",
      "\u001b[34mnorm: true\n",
      "      share_embeddings_and_output_weights:\u001b[0m\n",
      "\u001b[34mfalse\n",
      "      position_embedding_type: rope\n",
      "      rotary_percentage: 1\u001b[0m\n",
      "\u001b[34mactivation: swiglu\n",
      "      transformer_block_type: pr\u001b[0m\n",
      "\u001b[34me_ln\n",
      "      has_bias: false\n",
      "      tokenizer:\n",
      "        library: hugging\u001b[0m\n",
      "\u001b[34mface\n",
      "        type: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mmodel: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        delim\u001b[0m\n",
      "\u001b[34miter: null\n",
      "        sentencepiece_legacy: false\u001b[0m\n",
      "\u001b[34muse_fast: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_i\u001b[0m\n",
      "\u001b[34mnterval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residua\u001b[0m\n",
      "\u001b[34ml_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron\u001b[0m\n",
      "\u001b[34m_amp_O2: true\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div\u001b[0m\n",
      "\u001b[34m_ar_fusion: false\n",
      "      gradient_accumulation_fusion: false\n",
      "      bi\u001b[0m\n",
      "\u001b[34mas_activation_fusion: false\n",
      "      bias_dropout_add_fusion: false\u001b[0m\n",
      "\u001b[34mmasked_softmax_fusion: false\n",
      "      seed: 1234\n",
      "      resume_from_\u001b[0m\n",
      "\u001b[34mcheckpoint: /opt/ml/code/tmp/nemo_checkpoint/mp_rank_0\u001b[0m\n",
      "\u001b[34m7/model_optim_rng.ckpt\n",
      "      use_cpu_initialization: false\n",
      "      onn\u001b[0m\n",
      "\u001b[34mx_safe: false\n",
      "      apex_transformer_l\u001b[0m\n",
      "\u001b[34mog_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_co\u001b[0m\n",
      "\u001b[34mmm: false\n",
      "      log_parameter_norm: true\n",
      "      log_gr\u001b[0m\n",
      "\u001b[34madient_norm: true\n",
      "      activations_checkpoint_granularity: full\u001b[0m\n",
      "\u001b[34mactivations_checkpoint_method: uniform\n",
      "      activations_checkpoint_\u001b[0m\n",
      "\u001b[34mnum_layers: 1\n",
      "      num_micro_batches_with_partial_acti\u001b[0m\n",
      "\u001b[34mvation_checkpoints: null\n",
      "      activations_checkpoint_\u001b[0m\n",
      "\u001b[34mlayers_per_pipeline: null\n",
      "      sequence_parallel: tr\u001b[0m\n",
      "\u001b[34mue\n",
      "      wrap_with_zero: false\n",
      "      transformer_engi\u001b[0m\n",
      "\u001b[34mne: false\n",
      "      fp8: false\n",
      "      fp8_e4m3: false\u001b[0m\n",
      "\u001b[34mfp8_hybrid: false\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      f\u001b[0m\n",
      "\u001b[34mp8_amax_history_len: 1\n",
      "      fp8_amax_compute_algo: most_recent\u001b[0m\n",
      "\u001b[34muse_emha: false\n",
      "      data:\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - /opt/ml\u001b[0m\n",
      "\u001b[34m/code/tmp/tokenized_data_text_document\n",
      "        index_ma\u001b[0m\n",
      "\u001b[34mpping_dir: null\n",
      "        data_impl: mmap\n",
      "        splits_string: 1000,0,0\n",
      "        seq_length: 2048\u001b[0m\n",
      "\u001b[34mskip_warmup: true\n",
      "        num_workers: 1\u001b[0m\n",
      "\u001b[34mdataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attenti\u001b[0m\n",
      "\u001b[34mon_mask: false\n",
      "        eod_mask_loss: false\n",
      "        v\u001b[0m\n",
      "\u001b[34malidation_drop_last: true\n",
      "        no_seqlen_plus_one_input_tokens: fal\u001b[0m\n",
      "\u001b[34mse\n",
      "        pad_samples_to_global_batch_size: false\n",
      "      nsys_profi\u001b[0m\n",
      "\u001b[34mle:\n",
      "        enabled: false\n",
      "        start_step: 10\u001b[0m\n",
      "\u001b[34mend_step: 10\n",
      "        ranks:\n",
      "        - 0\n",
      "        gen_shape: false\u001b[0m\n",
      "\u001b[34moptim:\n",
      "        name: adamw\n",
      "        lr: 0.0001\n",
      "        weight_\u001b[0m\n",
      "\u001b[34mdecay: 0.1\n",
      "        capturable: true\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\u001b[0m\n",
      "\u001b[34msched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 10\u001b[0m\n",
      "\u001b[34mconstant_steps: 0\n",
      "          min_lr: 1.0e-06\n",
      "      save_xser: true\n",
      "      load_xser: true\n",
      "    \u001b[0m\n",
      "\u001b[34mprec\u001b[0m\n",
      "\u001b[34mision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionP\u001b[0m\n",
      "\u001b[34mlugin object at 0x7f5489060880>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin obje\u001b[0m\n",
      "\u001b[34mct at 0x7ff0213916c0>\u001b[0m\n",
      "\u001b[34mprecision plugin:\u001b[0m\n",
      "\u001b[34m<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlu\u001b[0m\n",
      "\u001b[34mgin object at 0x7f7b88487430>\u001b[0m\n",
      "\u001b[34mpr\u001b[0m\n",
      "\u001b[34mecision plugin\u001b[0m\n",
      "\u001b[34m:<pytorch_lightning.plugins.precis\u001b[0m\n",
      "\u001b[34mion.tpu.TPUPrecisionPlugi\u001b[0m\n",
      "\u001b[34mn object at 0x7eff59997ee0>\u001b[0m\n",
      "\u001b[34mprecision p\u001b[0m\n",
      "\u001b[34mlugin:<pytorch_l\u001b[0m\n",
      "\u001b[34mightning.plugins.precision.tpu.TPUPr\u001b[0m\n",
      "\u001b[34mecisionPlugin object at 0x\u001b[0m\n",
      "\u001b[34m7f3486640850>\u001b[0m\n",
      "\u001b[34msetup_microb\u001b[0m\n",
      "\u001b[34match_calculator 1\u001b[0m\n",
      "\u001b[34m9 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_mi\u001b[0m\n",
      "\u001b[34mcrobatch_calcul\u001b[0m\n",
      "\u001b[34mator 23 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microb\u001b[0m\n",
      "\u001b[34match_calculator 29 None 2\u001b[0m\n",
      "\u001b[34m56 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 24 No\u001b[0m\n",
      "\u001b[34mne 256 1 4\u001b[0m\n",
      "\u001b[34msetup_mi\u001b[0m\n",
      "\u001b[34mcrobatch_calcula\u001b[0m\n",
      "\u001b[34mtor 26 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are usi\u001b[0m\n",
      "\u001b[34mng the legacy be\u001b[0m\n",
      "\u001b[34mhaviour of the <class 'transformers.m\u001b[0m\n",
      "\u001b[34models.llama.tokenization_llam\u001b[0m\n",
      "\u001b[34ma.LlamaTokenizer'>. Thi\u001b[0m\n",
      "\u001b[34ms means that tokens that\u001b[0m\n",
      "\u001b[34mcome after special tokens w\u001b[0m\n",
      "\u001b[34mill not be properly handled. We recom\u001b[0m\n",
      "\u001b[34mmend you to read th\u001b[0m\n",
      "\u001b[34me related pull request available at ht\u001b[0m\n",
      "\u001b[34mtps://github.com/huggingface/transfor\u001b[0m\n",
      "\u001b[34mmers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_ca\u001b[0m\n",
      "\u001b[34mlculator 21 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are usi\u001b[0m\n",
      "\u001b[34mng the legacy b\u001b[0m\n",
      "\u001b[34mehaviour of the <class 'transformers\u001b[0m\n",
      "\u001b[34m.models.llama.tokenization_llama.Ll\u001b[0m\n",
      "\u001b[34mamaTokenizer'>. This means that tokens\u001b[0m\n",
      "\u001b[34mthat come after special tokens will\u001b[0m\n",
      "\u001b[34mnot be properly handled. We recommen\u001b[0m\n",
      "\u001b[34md you to read the related pull reques\u001b[0m\n",
      "\u001b[34mt available at https://github.com/hu\u001b[0m\n",
      "\u001b[34mggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou\u001b[0m\n",
      "\u001b[34mare using the legacy behaviour of t\u001b[0m\n",
      "\u001b[34mhe <class 'transformers.models.llama.tokenization_l\u001b[0m\n",
      "\u001b[34mlama.LlamaTokenizer'>. This means tha\u001b[0m\n",
      "\u001b[34mt tokens that come after special token\u001b[0m\n",
      "\u001b[34ms will not be properly handled. We re\u001b[0m\n",
      "\u001b[34mcommend you to read the related pull\u001b[0m\n",
      "\u001b[34mrequest available at https://github.\u001b[0m\n",
      "\u001b[34mcom/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour o\u001b[0m\n",
      "\u001b[34mf the <class 'transformers.models.lla\u001b[0m\n",
      "\u001b[34mma.tokenization_llama.LlamaTokenizer'\u001b[0m\n",
      "\u001b[34m>. This means that tokens that come a\u001b[0m\n",
      "\u001b[34mfter special tokens will not be prop\u001b[0m\n",
      "\u001b[34merly handled. We recommend you to re\u001b[0m\n",
      "\u001b[34mad the related pull request available\u001b[0m\n",
      "\u001b[34mat https://github.com/huggingface/t\u001b[0m\n",
      "\u001b[34mransformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are usi\u001b[0m\n",
      "\u001b[34mng the legacy be\u001b[0m\n",
      "\u001b[34mhaviour of the <class 'transformers.\u001b[0m\n",
      "\u001b[34mmodels.llama.tokenization_llama.Llama\u001b[0m\n",
      "\u001b[34mTokenizer'>. This means that tokens th\u001b[0m\n",
      "\u001b[34mat come after special tokens will not\u001b[0m\n",
      "\u001b[34mbe properly handled. We recommend yo\u001b[0m\n",
      "\u001b[34mu to read the related pull request a\u001b[0m\n",
      "\u001b[34mvailable at https://github.com/huggin\u001b[0m\n",
      "\u001b[34mgface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are usi\u001b[0m\n",
      "\u001b[34mng the legacy b\u001b[0m\n",
      "\u001b[34mehaviour of the <class 'transformers.\u001b[0m\n",
      "\u001b[34mmodels.llama.tokenization_llama.Llam\u001b[0m\n",
      "\u001b[34maTokenizer'>. This means that tokens\u001b[0m\n",
      "\u001b[34mthat come after special tokens will n\u001b[0m\n",
      "\u001b[34mot be properly handled. We recommend\u001b[0m\n",
      "\u001b[34myou to read the related pull request\u001b[0m\n",
      "\u001b[34mavailable at https://github.com/hugg\u001b[0m\n",
      "\u001b[34mingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mprecision pl\u001b[0m\n",
      "\u001b[34mugin:<pytorch_l\u001b[0m\n",
      "\u001b[34mightning.plugins.precision.tpu.TPUPrecisi\u001b[0m\n",
      "\u001b[34monPlugin object at 0x7fa92470b730>\u001b[0m\n",
      "\u001b[34mUsing sep_to\u001b[0m\n",
      "\u001b[34mken, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it i\u001b[0m\n",
      "\u001b[34ms not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token\u001b[0m\n",
      "\u001b[34m, but it is not set ye\u001b[0m\n",
      "\u001b[34mt.\u001b[0m\n",
      "\u001b[34mUsing sep_t\u001b[0m\n",
      "\u001b[34moken, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsing\u001b[0m\n",
      "\u001b[34mcls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsi\u001b[0m\n",
      "\u001b[34mng cls_token, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it i\u001b[0m\n",
      "\u001b[34ms not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_tok\u001b[0m\n",
      "\u001b[34men, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_tok\u001b[0m\n",
      "\u001b[34men, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_tok\u001b[0m\n",
      "\u001b[34men, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_t\u001b[0m\n",
      "\u001b[34moken, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_t\u001b[0m\n",
      "\u001b[34moken, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsi\u001b[0m\n",
      "\u001b[34mng mask_tok\u001b[0m\n",
      "\u001b[34men, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing\u001b[0m\n",
      "\u001b[34msep_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_to\u001b[0m\n",
      "\u001b[34mken, but it is not set ye\u001b[0m\n",
      "\u001b[34mt.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_tok\u001b[0m\n",
      "\u001b[34men, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microb\u001b[0m\n",
      "\u001b[34match_calculator\u001b[0m\n",
      "\u001b[34m27 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calcul\u001b[0m\n",
      "\u001b[34mator 22 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microb\u001b[0m\n",
      "\u001b[34match_calculator\u001b[0m\n",
      "\u001b[34m10 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are usi\u001b[0m\n",
      "\u001b[34mng the legacy b\u001b[0m\n",
      "\u001b[34mehaviour of the <class 'transformers.\u001b[0m\n",
      "\u001b[34mmodels.llama.tokenizatio\u001b[0m\n",
      "\u001b[34mn_llama.LlamaTokenizer'>. This\u001b[0m\n",
      "\u001b[34mmeans that tokens that come\u001b[0m\n",
      "\u001b[34mafter special tokens will\u001b[0m\n",
      "\u001b[34mnot be properly handled. We r\u001b[0m\n",
      "\u001b[34mecommend you to read the rela\u001b[0m\n",
      "\u001b[34mted pull request available at\u001b[0m\n",
      "\u001b[34mhttps://github.com/huggingfac\u001b[0m\n",
      "\u001b[34me/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou\u001b[0m\n",
      "\u001b[34mare using the legacy behaviou\u001b[0m\n",
      "\u001b[34mr of the <class 'transformers.models.ll\u001b[0m\n",
      "\u001b[34mama.tokenization_llama.\u001b[0m\n",
      "\u001b[34mLlamaTokenizer'>. This me\u001b[0m\n",
      "\u001b[34mans that tokens that come after s\u001b[0m\n",
      "\u001b[34mpecial tokens will not be prop\u001b[0m\n",
      "\u001b[34merly handled. We recommend yo\u001b[0m\n",
      "\u001b[34mu to read the related pull re\u001b[0m\n",
      "\u001b[34mquest available at https://git\u001b[0m\n",
      "\u001b[34mhub.com/huggingface/transform\u001b[0m\n",
      "\u001b[34mers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_micro\u001b[0m\n",
      "\u001b[34mbatch_calculato\u001b[0m\n",
      "\u001b[34mr 20 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are usi\u001b[0m\n",
      "\u001b[34mng the legacy b\u001b[0m\n",
      "\u001b[34mehaviour of the <class 'transformers\u001b[0m\n",
      "\u001b[34m.models.llama.tokenization_llama.Llam\u001b[0m\n",
      "\u001b[34maTokenizer'>. This means that tokens\u001b[0m\n",
      "\u001b[34mthat come after special tokens will n\u001b[0m\n",
      "\u001b[34mot be properly handled. We recommend\u001b[0m\n",
      "\u001b[34myou to read the related pull request\u001b[0m\n",
      "\u001b[34mavailable at https://github.com/hug\u001b[0m\n",
      "\u001b[34mgingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mse\u001b[0m\n",
      "\u001b[34mtup_microbatch_calculator 17 None 25\u001b[0m\n",
      "\u001b[34m6 1 4\u001b[0m\n",
      "\u001b[34mYou are u\u001b[0m\n",
      "\u001b[34msing the legacy\u001b[0m\n",
      "\u001b[34mbehaviour of the <class 'transf\u001b[0m\n",
      "\u001b[34mormers.models.llama.token\u001b[0m\n",
      "\u001b[34mization_llama.LlamaToken\u001b[0m\n",
      "\u001b[34mizer'>. This means that token\u001b[0m\n",
      "\u001b[34ms that come after sp\u001b[0m\n",
      "\u001b[34mecial tokens will not be p\u001b[0m\n",
      "\u001b[34mroperly handled. We recommend\u001b[0m\n",
      "\u001b[34myou to read the related pull\u001b[0m\n",
      "\u001b[34mrequest available at http\u001b[0m\n",
      "\u001b[34ms://github.com/huggingface/tran\u001b[0m\n",
      "\u001b[34msformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou a\u001b[0m\n",
      "\u001b[34mre using the\u001b[0m\n",
      "\u001b[34mlegacy behaviour of the <class 'tr\u001b[0m\n",
      "\u001b[34mansformers.models.llama.to\u001b[0m\n",
      "\u001b[34mkenization_llama.LlamaTokeni\u001b[0m\n",
      "\u001b[34mzer'>. This means that tokens\u001b[0m\n",
      "\u001b[34mthat come after special tok\u001b[0m\n",
      "\u001b[34mens will not be properly ha\u001b[0m\n",
      "\u001b[34mndled. We recommend you to read\u001b[0m\n",
      "\u001b[34mthe related pull request\u001b[0m\n",
      "\u001b[34mavailable at https://github.c\u001b[0m\n",
      "\u001b[34mom/huggingface/transformers/pul\u001b[0m\n",
      "\u001b[34ml/24565\u001b[0m\n",
      "\u001b[34mUsing sep_t\u001b[0m\n",
      "\u001b[34moken, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_to\u001b[0m\n",
      "\u001b[34mken, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_tok\u001b[0m\n",
      "\u001b[34men, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing p\u001b[0m\n",
      "\u001b[34mad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask\u001b[0m\n",
      "\u001b[34m_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_t\u001b[0m\n",
      "\u001b[34moken, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mU\u001b[0m\n",
      "\u001b[34msing mask_t\u001b[0m\n",
      "\u001b[34moken, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprec\u001b[0m\n",
      "\u001b[34mision plugin:<\u001b[0m\n",
      "\u001b[34mpytorch_lightning.plugins.precision.\u001b[0m\n",
      "\u001b[34mtpu.TPUPrecisionPlugin object at 0x7\u001b[0m\n",
      "\u001b[34mf8b7115b670>\u001b[0m\n",
      "\u001b[34mprecision plu\u001b[0m\n",
      "\u001b[34mgin:<pytorch_li\u001b[0m\n",
      "\u001b[34mghtning.plugins.precision.tpu.TPUPrec\u001b[0m\n",
      "\u001b[34misionPlugin object at 0x7f3928b8b730>\u001b[0m\n",
      "\u001b[34mprecision plu\u001b[0m\n",
      "\u001b[34mgin:<pytorch_li\u001b[0m\n",
      "\u001b[34mghtning.plugins.precision.tpu.TPUPreci\u001b[0m\n",
      "\u001b[34msionPlugin object at 0x7f20ce860400>\u001b[0m\n",
      "\u001b[34mprecision plug\u001b[0m\n",
      "\u001b[34min:<pytorch_ligh\u001b[0m\n",
      "\u001b[34mtning.plugins.precision.tpu.TPUPrecisi\u001b[0m\n",
      "\u001b[34monPlugin object at 0x7fc80f603d30>\u001b[0m\n",
      "\u001b[34mUsi\u001b[0m\n",
      "\u001b[34mng sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls\u001b[0m\n",
      "\u001b[34m_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing\u001b[0m\n",
      "\u001b[34mpad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token\u001b[0m\n",
      "\u001b[34m, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing se\u001b[0m\n",
      "\u001b[34mp_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin\u001b[0m\n",
      "\u001b[34m:<pytorch_lightning.plugins.precisio\u001b[0m\n",
      "\u001b[34mn.tpu.TPUPrecisionPlugin object at 0x7f1bd7343010>U\u001b[0m\n",
      "\u001b[34msing cls_token, but it is not set yet\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is n\u001b[0m\n",
      "\u001b[34mot set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I\u001b[0m\n",
      "\u001b[34m2023-12-09 17:3\u001b[0m\n",
      "\u001b[34m8:22 nlp_overrides:418] NLPTrainer: Ini\u001b[0m\n",
      "\u001b[34mtializing trainer with parameters: {'s\u001b[0m\n",
      "\u001b[34melf': <nemo.collections.nlp.parts.nlp_\u001b[0m\n",
      "\u001b[34moverrides.NLPTrainer object at 0x7fd3c\u001b[0m\n",
      "\u001b[34m2d5c3d0>, 'logger': False, 'enable_che\u001b[0m\n",
      "\u001b[34mckpointing': False, 'callbacks': None\u001b[0m\n",
      "\u001b[34m, 'default_root_dir': None, 'gradient\u001b[0m\n",
      "\u001b[34m_clip_val': 1.0, 'gradient_clip_algo\u001b[0m\n",
      "\u001b[34mrithm': None, 'num_nodes': 1, 'num_pr\u001b[0m\n",
      "\u001b[34mocesses': None, 'devices': 32, 'gpus'\u001b[0m\n",
      "\u001b[34m: None, 'auto_select_gpus': False, 't\u001b[0m\n",
      "\u001b[34mpu_cores': None, 'ipus': None, 'enab\u001b[0m\n",
      "\u001b[34mle_progress_bar': True, 'overfit_batc\u001b[0m\n",
      "\u001b[34mhes': 0.0, 'track_grad_norm': -1, 'ch\u001b[0m\n",
      "\u001b[34meck_val_every_n_epoch': 1, 'fast_dev\u001b[0m\n",
      "\u001b[34m_run': False, 'accumulate_grad_batch\u001b[0m\n",
      "\u001b[34mes': 1, 'max_epochs': None, 'min_epo\u001b[0m\n",
      "\u001b[34mchs': None, 'max_steps': 3, 'min_ste\u001b[0m\n",
      "\u001b[34mps': None, 'max_time': None, 'limit_\u001b[0m\n",
      "\u001b[34mtrain_batches': None, 'limit_val_bat\u001b[0m\n",
      "\u001b[34mches': 1, 'limit_test_batches': 1,\u001b[0m\n",
      "\u001b[34m'limit_predict_batches': None, 'val_\u001b[0m\n",
      "\u001b[34mcheck_interval': 0.99, 'log_every_n_s\u001b[0m\n",
      "\u001b[34mteps': 1, 'accelerator': 'tpu', 'str\u001b[0m\n",
      "\u001b[34mategy': <nemo.collections.nlp.parts.\u001b[0m\n",
      "\u001b[34mnlp_overrides.NLPDDPStrategy object\u001b[0m\n",
      "\u001b[34mat 0x7fd3c2d5cdc0>, 'sync_batchnorm'\u001b[0m\n",
      "\u001b[34m: False, 'precision': 32, 'enable_mod\u001b[0m\n",
      "\u001b[34mel_summary': False, 'num_sanity_val_\u001b[0m\n",
      "\u001b[34msteps': 0, 'resume_from_checkpoint':\u001b[0m\n",
      "\u001b[34mNone, 'profiler': None, 'benchmark'\u001b[0m\n",
      "\u001b[34m: False, 'deterministic': None, 'rel\u001b[0m\n",
      "\u001b[34moad_dataloaders_every_n_epochs': 0, '\u001b[0m\n",
      "\u001b[34mauto_lr_find': False, 'replace_sampl\u001b[0m\n",
      "\u001b[34mer_ddp': False, 'detect_anomaly': Fal\u001b[0m\n",
      "\u001b[34mse, 'auto_scale_batch_size': False,\u001b[0m\n",
      "\u001b[34m'plugins': [], 'amp_backend': 'nativ\u001b[0m\n",
      "\u001b[34me', 'amp_level': None, 'move_metrics\u001b[0m\n",
      "\u001b[34m_to_cpu': False, 'multiple_trainloader\u001b[0m\n",
      "\u001b[34m_mode': 'max_size_cycle', 'inference\u001b[0m\n",
      "\u001b[34m_mode': True}\u001b[0m\n",
      "\u001b[34mprecision plugin:<p\u001b[0m\n",
      "\u001b[34mytorch_lightning.plugins.precision.t\u001b[0m\n",
      "\u001b[34mpu.TPUPrecisionPlugin object at 0x7fa272cf0220>\u001b[0m\n",
      "\u001b[34mpre\u001b[0m\n",
      "\u001b[34mcision plugin:<pytorch_lightning.plug\u001b[0m\n",
      "\u001b[34mins.precision.tpu.TPUPrecisionPlugin object at 0x7\u001b[0m\n",
      "\u001b[34mfc1bc435a80>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytor\u001b[0m\n",
      "\u001b[34mch_lightning.plugins.precision.tpu.T\u001b[0m\n",
      "\u001b[34mPUPrecisionPlugin object at 0x7f83e425c400>\u001b[0m\n",
      "\u001b[34mprecision plu\u001b[0m\n",
      "\u001b[34mgin:<pytorch_li\u001b[0m\n",
      "\u001b[34mghtning.plugins.precision.tpu.TPUPrec\u001b[0m\n",
      "\u001b[34misionPlugin object at 0x7f8\u001b[0m\n",
      "\u001b[34mff8dace50>\u001b[0m\n",
      "\u001b[34mprecision\u001b[0m\n",
      "\u001b[34mplugin:<pytorch\u001b[0m\n",
      "\u001b[34m_lightning.plugins.precision.tpu.TPUPr\u001b[0m\n",
      "\u001b[34mecisionPlugin object at 0x7f4518fec04\u001b[0m\n",
      "\u001b[34m0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.p\u001b[0m\n",
      "\u001b[34mlugins.precision.tpu.TPUPrecisionPlugi\u001b[0m\n",
      "\u001b[34mn object at 0x7fba79fb3310>\u001b[0m\n",
      "\u001b[34mprecision plu\u001b[0m\n",
      "\u001b[34mgin:<pytorch_lig\u001b[0m\n",
      "\u001b[34mhtning.plugins.precision.tpu.TPUPreci\u001b[0m\n",
      "\u001b[34msionPlugin object at 0x7f30dd9fd480>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.pl\u001b[0m\n",
      "\u001b[34mugins.precision.tpu.TPUPrecisionP\u001b[0m\n",
      "\u001b[34mlugin object at 0x7fd3c2f3b160>\u001b[0m\n",
      "\u001b[34mprecision pl\u001b[0m\n",
      "\u001b[34mugin:<pytorch_li\u001b[0m\n",
      "\u001b[34mghtning.plugins.precision.tpu.TPUPrecis\u001b[0m\n",
      "\u001b[34mionPlugin object at 0x7ff6a0759420>\u001b[0m\n",
      "\u001b[34mprecision plug\u001b[0m\n",
      "\u001b[34min:<pytorch_ligh\u001b[0m\n",
      "\u001b[34mtning.plugins.precision.tpu.TPUPrecisio\u001b[0m\n",
      "\u001b[34mnPlugin object at 0x7f6eb9c80040>\u001b[0m\n",
      "\u001b[34mprecision plu\u001b[0m\n",
      "\u001b[34mgin:<pytorch_lig\u001b[0m\n",
      "\u001b[34mhtning.plugins.precision.tpu.TPUPreci\u001b[0m\n",
      "\u001b[34msionPlugin object at 0x7fdd60a98490>\u001b[0m\n",
      "\u001b[34mGPU available:\u001b[0m\n",
      "\u001b[34mTrue (cuda), us\u001b[0m\n",
      "\u001b[34med: False\u001b[0m\n",
      "\u001b[34mTPU a\u001b[0m\n",
      "\u001b[34mvailable: True, using: 32 TPU cores\u001b[0m\n",
      "\u001b[34mI\u001b[0m\n",
      "\u001b[34mPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available:\u001b[0m\n",
      "\u001b[34mFalse, using: 0 HPUs\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-\u001b[0m\n",
      "\u001b[34m12-09 17:38:22 n\u001b[0m\n",
      "\u001b[34memo_logging:349] /usr/local/lib/python\u001b[0m\n",
      "\u001b[34m3.10/site-packages/pytorch_lightning/t\u001b[0m\n",
      "\u001b[34mrainer/setup.py:175: PossibleUserWarni\u001b[0m\n",
      "\u001b[34mng: GPU available but not used. Set `\u001b[0m\n",
      "\u001b[34maccelerator` and `devices` using `Tra\u001b[0m\n",
      "\u001b[34miner(accelerator='gpu', devices=1)`.\u001b[0m\n",
      "\u001b[34mrank_zero_warn(\n",
      "    \u001b[0m\n",
      "\u001b[34m`Trainer(\u001b[0m\n",
      "\u001b[34mlimit_val_batches=1)` was configured s\u001b[0m\n",
      "\u001b[34mo 1 batch will be used.\u001b[0m\n",
      "\u001b[34m`Trainer(limit_test_batches=1)` was conf\u001b[0m\n",
      "\u001b[34migured so 1 batch will be used.\u001b[0m\n",
      "\u001b[34msetu\u001b[0m\n",
      "\u001b[34mp_microbatch_ca\u001b[0m\n",
      "\u001b[34mlculator 18 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are usi\u001b[0m\n",
      "\u001b[34mng the legacy be\u001b[0m\n",
      "\u001b[34mhaviour of the <class 'transformers.mo\u001b[0m\n",
      "\u001b[34mdels.llama.tokenization_llama.LlamaTok\u001b[0m\n",
      "\u001b[34menizer'>. This means that tokens that\u001b[0m\n",
      "\u001b[34mcome after special tokens will not be\u001b[0m\n",
      "\u001b[34mproperly handled. We recommend you t\u001b[0m\n",
      "\u001b[34mo read the related pull request avail\u001b[0m\n",
      "\u001b[34mable at https://github.com/huggingfa\u001b[0m\n",
      "\u001b[34mce/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mprecision pl\u001b[0m\n",
      "\u001b[34mugin:<pytorch_l\u001b[0m\n",
      "\u001b[34mightning.plugins.precision.tpu.TPUPrec\u001b[0m\n",
      "\u001b[34misionPlugin object at 0x7fef8d7e0610>\u001b[0m\n",
      "\u001b[34mprecisio\u001b[0m\n",
      "\u001b[34mn plugin:<pytor\u001b[0m\n",
      "\u001b[34mch_lightning.plugins.precision.tpu.T\u001b[0m\n",
      "\u001b[34mPUPrecisionPlugin object at 0x7f2c333\u001b[0m\n",
      "\u001b[34m3f130>\u001b[0m\n",
      "\u001b[34m[NeMo E\u001b[0m\n",
      "\u001b[34m2023-12-09 17:38\u001b[0m\n",
      "\u001b[34m:22 exp_manager:465] You are running\u001b[0m\n",
      "\u001b[34mmulti-gpu without ddp.Please note that\u001b[0m\n",
      "\u001b[34mthis is not tested in NeMo and could\u001b[0m\n",
      "\u001b[34mresult in errors.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-\u001b[0m\n",
      "\u001b[34m12-09 17:38:22\u001b[0m\n",
      "\u001b[34mexp_manager:350] Experiments will be\u001b[0m\n",
      "\u001b[34mlogged at /tmp/megatron_llama/2023-12\u001b[0m\n",
      "\u001b[34m-09_17-38-15\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-\u001b[0m\n",
      "\u001b[34m12-09 17:38:22 ne\u001b[0m\n",
      "\u001b[34mmo_logging:349] /usr/local/lib/python3\u001b[0m\n",
      "\u001b[34m.10/site-packages/nemo/utils/exp_manage\u001b[0m\n",
      "\u001b[34mr.py:1035: UserWarning: Detected custom\u001b[0m\n",
      "\u001b[34mepoch loop. Skipping no validation o\u001b[0m\n",
      "\u001b[34mn restart support.\n",
      "      warnings.warn\u001b[0m\n",
      "\u001b[34m(\"Detected custom epoch loop. Skippin\u001b[0m\n",
      "\u001b[34mg no validation on restart support.\", UserWarning)\u001b[0m\n",
      "\u001b[34mUsing sep_\u001b[0m\n",
      "\u001b[34mtoken, but it i\u001b[0m\n",
      "\u001b[34ms not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but i\u001b[0m\n",
      "\u001b[34mt is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not\u001b[0m\n",
      "\u001b[34mset yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 20\u001b[0m\n",
      "\u001b[34m23-12-09 17:38\u001b[0m\n",
      "\u001b[34m:22 megatron_gpt_pretraining:103] Res\u001b[0m\n",
      "\u001b[34muming training from checkpoint: /opt/\u001b[0m\n",
      "\u001b[34mml/code/tmp/nemo_checkpoint/mp_rank_0\u001b[0m\n",
      "\u001b[34m7/model_optim_rng.ckpt\u001b[0m\n",
      "\u001b[34m[NeMo W 202\u001b[0m\n",
      "\u001b[34m3-12-09 17:38:2\u001b[0m\n",
      "\u001b[34m2 nemo_logging:349] /usr/local/lib/py\u001b[0m\n",
      "\u001b[34mthon3.10/site-packages/pytorch_lightn\u001b[0m\n",
      "\u001b[34ming/trainer/connectors/checkpoint_conn\u001b[0m\n",
      "\u001b[34mector.py:55: LightningDeprecationWarn\u001b[0m\n",
      "\u001b[34ming: Setting `Trainer(resume_from_che\u001b[0m\n",
      "\u001b[34mckpoint=)` is deprecated in v1.5 and\u001b[0m\n",
      "\u001b[34mwill be removed in v2.0. Please pass\u001b[0m\n",
      "\u001b[34m`Trainer.fit(ckpt_path=)` directly i\u001b[0m\n",
      "\u001b[34mnstead.\n",
      "      rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34msetup_\u001b[0m\n",
      "\u001b[34mmicrobatch_calcu\u001b[0m\n",
      "\u001b[34mlator 5 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_m\u001b[0m\n",
      "\u001b[34microbatch_calculator 30 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 3 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microba\u001b[0m\n",
      "\u001b[34mtch_calculator\u001b[0m\n",
      "\u001b[34m15 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 9\u001b[0m\n",
      "\u001b[34mNone 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy beh\u001b[0m\n",
      "\u001b[34maviour of the <class 'transformers.mo\u001b[0m\n",
      "\u001b[34mdels.llama.tokenization_llama.LlamaTo\u001b[0m\n",
      "\u001b[34mkenizer'>. This means that tokens that\u001b[0m\n",
      "\u001b[34mcome after special tokens will not be\u001b[0m\n",
      "\u001b[34mproperly handled. We recommend you t\u001b[0m\n",
      "\u001b[34mo read the related pull request availa\u001b[0m\n",
      "\u001b[34mble at https://github.com/huggingface/\u001b[0m\n",
      "\u001b[34mtransformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are usin\u001b[0m\n",
      "\u001b[34mg the legacy behaviour of the <class\u001b[0m\n",
      "\u001b[34m'transformers.models.llama.tokenization_llama.LlamaT\u001b[0m\n",
      "\u001b[34mokenizer'>. This means that tokens th\u001b[0m\n",
      "\u001b[34mat come after special tokens will not\u001b[0m\n",
      "\u001b[34mbe properly handled. We recommend yo\u001b[0m\n",
      "\u001b[34mu to read the related pull request av\u001b[0m\n",
      "\u001b[34mailable at https://github.com/hugging\u001b[0m\n",
      "\u001b[34mface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are u\u001b[0m\n",
      "\u001b[34msing the legacy behaviour of the <cla\u001b[0m\n",
      "\u001b[34mss 'transformers.models.llama.tokenization_llama.Lla\u001b[0m\n",
      "\u001b[34mmaTokenizer'>. This means that tokens\u001b[0m\n",
      "\u001b[34mthat come after s\u001b[0m\n",
      "\u001b[34mpecial tokens will not be properly\u001b[0m\n",
      "\u001b[34mhandled. We recommend you to read the\u001b[0m\n",
      "\u001b[34mrelated pull request available at htt\u001b[0m\n",
      "\u001b[34mps://github.com/huggingface/transforme\u001b[0m\n",
      "\u001b[34mrs/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the lega\u001b[0m\n",
      "\u001b[34mcy behaviour of the <class 'transforme\u001b[0m\n",
      "\u001b[34mrs.models.llama.tokenization_llama.LlamaTokenizer'>.\u001b[0m\n",
      "\u001b[34mThis means that tokens that come afte\u001b[0m\n",
      "\u001b[34mr special tokens will not be prope\u001b[0m\n",
      "\u001b[34mrly handled. We recommend you to read the related p\u001b[0m\n",
      "\u001b[34mull request ava\u001b[0m\n",
      "\u001b[34milable at https://github.com/huggingf\u001b[0m\n",
      "\u001b[34mace/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy beha\u001b[0m\n",
      "\u001b[34mviour of the <class 'transformers.mo\u001b[0m\n",
      "\u001b[34mdels.llama.tokenization_llama.LlamaTo\u001b[0m\n",
      "\u001b[34mkenizer'>. This means that tokens that\u001b[0m\n",
      "\u001b[34mcome after special tokens will not b\u001b[0m\n",
      "\u001b[34me properly handled. We recommend you\u001b[0m\n",
      "\u001b[34mto read the related pull request avai\u001b[0m\n",
      "\u001b[34mlable at https://github.com/huggingfa\u001b[0m\n",
      "\u001b[34mce/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microba\u001b[0m\n",
      "\u001b[34mtch_calculator 1\u001b[0m\n",
      "\u001b[34m6 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_mic\u001b[0m\n",
      "\u001b[34mrobatch_calculat\u001b[0m\n",
      "\u001b[34mor 13 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_micr\u001b[0m\n",
      "\u001b[34mobatch_calculat\u001b[0m\n",
      "\u001b[34mor 4 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 12\u001b[0m\n",
      "\u001b[34mNone 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy behav\u001b[0m\n",
      "\u001b[34miour of the <class 'transformers.mode\u001b[0m\n",
      "\u001b[34mls.llama.tokenization_llama.LlamaTokeni\u001b[0m\n",
      "\u001b[34mzer'>. This means that tokens that come\u001b[0m\n",
      "\u001b[34mafter special tokens will not be proper\u001b[0m\n",
      "\u001b[34mly handled. We recommend you to read\u001b[0m\n",
      "\u001b[34mthe related pull request available at\u001b[0m\n",
      "\u001b[34mhttps://github.com/huggingface/transf\u001b[0m\n",
      "\u001b[34mormers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy beha\u001b[0m\n",
      "\u001b[34mviour of the <class 'transformers.mode\u001b[0m\n",
      "\u001b[34mls.llama.tokenization_llama.LlamaToke\u001b[0m\n",
      "\u001b[34mnizer'>. This means that tokens that c\u001b[0m\n",
      "\u001b[34mome after special tokens will not be\u001b[0m\n",
      "\u001b[34mproperly handled. We recommend you to\u001b[0m\n",
      "\u001b[34mread the related pull request availa\u001b[0m\n",
      "\u001b[34mble at https://github.com/huggingface\u001b[0m\n",
      "\u001b[34m/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy beh\u001b[0m\n",
      "\u001b[34maviour of the <class 'transformers.mo\u001b[0m\n",
      "\u001b[34mdels.llama.tokenization_llama.LlamaTo\u001b[0m\n",
      "\u001b[34mkenizer'>. This means that tokens that\u001b[0m\n",
      "\u001b[34mcome after special tokens will not b\u001b[0m\n",
      "\u001b[34me properly handled. We recommend you\u001b[0m\n",
      "\u001b[34mto read the related pull request avai\u001b[0m\n",
      "\u001b[34mlable at https://github.com/huggingfa\u001b[0m\n",
      "\u001b[34mce/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behavio\u001b[0m\n",
      "\u001b[34mur of the <clas\u001b[0m\n",
      "\u001b[34ms 'transformers.models.llama.tokeniza\u001b[0m\n",
      "\u001b[34mtion_llama.LlamaTokenizer'>. This me\u001b[0m\n",
      "\u001b[34mans that tokens that come after specia\u001b[0m\n",
      "\u001b[34ml tokens will not be properly handle\u001b[0m\n",
      "\u001b[34md. We recommend you to read the relat\u001b[0m\n",
      "\u001b[34med pull request available at https://\u001b[0m\n",
      "\u001b[34mgithub.com/huggingface/transformers/p\u001b[0m\n",
      "\u001b[34mull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculat\u001b[0m\n",
      "\u001b[34mor 2 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_ca\u001b[0m\n",
      "\u001b[34mlculator 31 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 14\u001b[0m\n",
      "\u001b[34mNone 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy beha\u001b[0m\n",
      "\u001b[34mviour of the <class 'transformers.model\u001b[0m\n",
      "\u001b[34ms.llama.tokenization_llama.LlamaTokeni\u001b[0m\n",
      "\u001b[34mzer'>. This means that tokens that com\u001b[0m\n",
      "\u001b[34me after special tokens will not be pro\u001b[0m\n",
      "\u001b[34mperly handled. We recommend you to r\u001b[0m\n",
      "\u001b[34mead the related pull request availabl\u001b[0m\n",
      "\u001b[34me at https://github.com/huggingface/\u001b[0m\n",
      "\u001b[34mtransformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are usi\u001b[0m\n",
      "\u001b[34mng the legacy behaviour of the <class\u001b[0m\n",
      "\u001b[34m'transformers.models.llama.tokenization_llama.Llama\u001b[0m\n",
      "\u001b[34mTokenizer'>. This means that tokens th\u001b[0m\n",
      "\u001b[34mat come after special tokens will not\u001b[0m\n",
      "\u001b[34mbe properly handled. We recommend you\u001b[0m\n",
      "\u001b[34mto read the related pull request ava\u001b[0m\n",
      "\u001b[34milable at https://github.com/huggingfa\u001b[0m\n",
      "\u001b[34mce/transformers/pull/24565\u001b[0m\n",
      "\u001b[34m[NeMo I 20\u001b[0m\n",
      "\u001b[34m23-12-09 17:38:22 megatron_init:255]\u001b[0m\n",
      "\u001b[34mRank 0 has data parallel group: [0, 8, 16, 24]\u001b[0m\n",
      "\u001b[34m[NeM\u001b[0m\n",
      "\u001b[34mo I 2023-12-09 17:38:22 megatron_init:\u001b[0m\n",
      "\u001b[34m258] All data parallel group ranks: [[0, 8, 16, 24], [\u001b[0m\n",
      "\u001b[34m1, 9, 17, 25], [2, 10, 18, 26], [3, 11\u001b[0m\n",
      "\u001b[34m, 19, 27], [4, 12, 20, 28], [5, 13, 21,\u001b[0m\n",
      "\u001b[34m29], [6, 14, 22, 30], [7, 15, 23, 31]]\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of\u001b[0m\n",
      "\u001b[34mthe <class 'transformers.models.llama.\u001b[0m\n",
      "\u001b[34mtokenization_llama.LlamaTokenizer'>.\u001b[0m\n",
      "\u001b[34mThis means that tokens that come after\u001b[0m\n",
      "\u001b[34mspecial tokens will not be properly\u001b[0m\n",
      "\u001b[34mhandled. We recommend you to read th\u001b[0m\n",
      "\u001b[34me related pull request available at h\u001b[0m\n",
      "\u001b[34mttps://github.com/huggingface/transfo\u001b[0m\n",
      "\u001b[34mrmers/pull/24565\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09\u001b[0m\n",
      "\u001b[34m17:38:22 megatron_init:259] Ranks 0\u001b[0m\n",
      "\u001b[34mhas data parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:22\u001b[0m\n",
      "\u001b[34mmegatron_init:267] Rank 0 has model p\u001b[0m\n",
      "\u001b[34marallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-1\u001b[0m\n",
      "\u001b[34m2-09 17:38:22 megatron_init:268] All m\u001b[0m\n",
      "\u001b[34model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7],\u001b[0m\n",
      "\u001b[34m[8, 9, 10, 11, 12, 13, 14, 15], [16,\u001b[0m\n",
      "\u001b[34m17, 18, 19, 20, 21, 22, 23], [24, 25, 2\u001b[0m\n",
      "\u001b[34m6, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-1\u001b[0m\n",
      "\u001b[34m2-09 17:38:22 megatron_init:278] Rank\u001b[0m\n",
      "\u001b[34m0 has tensor model parallel group: [0, 1, 2, 3, 4, 5\u001b[0m\n",
      "\u001b[34m, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:22 m\u001b[0m\n",
      "\u001b[34megatron_init:282] All tensor model par\u001b[0m\n",
      "\u001b[34mallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9\u001b[0m\n",
      "\u001b[34m, 10, 11, 12, 13, 14, 15], [16, 17, 18\u001b[0m\n",
      "\u001b[34m, 19, 20, 21, 22, 23], [24, 25, 26, 27\u001b[0m\n",
      "\u001b[34m, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34mUsing sep_token, bu\u001b[0m\n",
      "\u001b[34mt it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-\u001b[0m\n",
      "\u001b[34m09 17:38:22 megatron_init:283] Rank 0 has tensor mod\u001b[0m\n",
      "\u001b[34mel parallel rank: 0\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it i\u001b[0m\n",
      "\u001b[34ms not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:22 megatro\u001b[0m\n",
      "\u001b[34mn_init:297] Rank 0 has pipeline model parallel group\u001b[0m\n",
      "\u001b[34m: [0]\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_m\u001b[0m\n",
      "\u001b[34microbatch_calculator 1 None 256 1 4\u001b[0m\n",
      "\u001b[34m[N\u001b[0m\n",
      "\u001b[34meMo I 2023-12-09 17:38:22 megatron_init:309] Rank 0 has embedding\u001b[0m\n",
      "\u001b[34mgroup: [0]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mU\u001b[0m\n",
      "\u001b[34msing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:22 megatron_init:31\u001b[0m\n",
      "\u001b[34m5] All pipeline model parallel group ranks: [[0], [1],\u001b[0m\n",
      "\u001b[34m[2], [3], [4], [5], [6], [7], [8], [9], [10], [11],\u001b[0m\n",
      "\u001b[34m[12], [13], [14], [15], [16], [17], [1\u001b[0m\n",
      "\u001b[34m8], [19], [20], [21], [22], [23], [24]\u001b[0m\n",
      "\u001b[34m, [25], [26], [27], [28], [29], [30],\u001b[0m\n",
      "\u001b[34m[31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:22 mega\u001b[0m\n",
      "\u001b[34mtron_init:316] Rank 0 has pipeline mod\u001b[0m\n",
      "\u001b[34mel parallel rank 0\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is n\u001b[0m\n",
      "\u001b[34mot set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:22\u001b[0m\n",
      "\u001b[34mmegatron_init:317] All embedding group ranks: [[0], [1\u001b[0m\n",
      "\u001b[34m], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]\u001b[0m\n",
      "\u001b[34m, [12], [13], [14], [15], [16], [17],\u001b[0m\n",
      "\u001b[34m[18], [19], [20], [21], [22], [23], [2\u001b[0m\n",
      "\u001b[34m4], [25], [26], [27], [28], [29], [30]\u001b[0m\n",
      "\u001b[34m, [31]]\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:22\u001b[0m\n",
      "\u001b[34mmegatron_init:318] Rank 0 has embedding rank: 0\u001b[0m\n",
      "\u001b[34mU\u001b[0m\n",
      "\u001b[34msing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_to\u001b[0m\n",
      "\u001b[34mken, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing c\u001b[0m\n",
      "\u001b[34mls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_cal\u001b[0m\n",
      "\u001b[34mculator 0 None 256 1 4\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m23\u001b[0m\n",
      "\u001b[34m-12-09 17:38:22 - PID:812 - rank:(0, 0, 0, 0) - micro\u001b[0m\n",
      "\u001b[34mbatches.py:39 - INFO - setting number of micro-batches to constant\u001b[0m\n",
      "\u001b[34m64\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not se\u001b[0m\n",
      "\u001b[34mt yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 8 No\u001b[0m\n",
      "\u001b[34mne 256 1 4\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:22 tokenizer_ut\u001b[0m\n",
      "\u001b[34mils:182] Getting HuggingFace AutoTokenizer with pre\u001b[0m\n",
      "\u001b[34mtrained_model_name: /opt/ml/additonals3data/tokenize\u001b[0m\n",
      "\u001b[34mr\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour o\u001b[0m\n",
      "\u001b[34mf the <class 'transformers.models.lla\u001b[0m\n",
      "\u001b[34mma.tokenization_llama.LlamaTokenizer'>. This means t\u001b[0m\n",
      "\u001b[34mhat tokens that come after special\u001b[0m\n",
      "\u001b[34mtokens will not be properly handled. We\u001b[0m\n",
      "\u001b[34mrecommend you to read the related pul\u001b[0m\n",
      "\u001b[34ml request available at https://github\u001b[0m\n",
      "\u001b[34m.com/huggingface/transformers/pull/245\u001b[0m\n",
      "\u001b[34m65\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 28 N\u001b[0m\n",
      "\u001b[34mone 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy b\u001b[0m\n",
      "\u001b[34mehaviour of the <class 'transformers.models.llama.t\u001b[0m\n",
      "\u001b[34mokenization_llama.LlamaTokenizer'>. This means that\u001b[0m\n",
      "\u001b[34mtokens that come after special tokens\u001b[0m\n",
      "\u001b[34mwill not be properly handled. We recom\u001b[0m\n",
      "\u001b[34mmend you to read the related pull requ\u001b[0m\n",
      "\u001b[34mest available at https://github.com/h\u001b[0m\n",
      "\u001b[34muggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy beh\u001b[0m\n",
      "\u001b[34maviour of the <class 'transformers.mod\u001b[0m\n",
      "\u001b[34mels.llama.tokenization_llama.LlamaToke\u001b[0m\n",
      "\u001b[34mnizer'>. This means that tokens that c\u001b[0m\n",
      "\u001b[34mome after special tokens will not be\u001b[0m\n",
      "\u001b[34mproperly handled. We recommend you to\u001b[0m\n",
      "\u001b[34mread the related pull request availab\u001b[0m\n",
      "\u001b[34mle at https://github.com/huggingface/\u001b[0m\n",
      "\u001b[34mtransformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy beha\u001b[0m\n",
      "\u001b[34mviour of the <class 'transformers.model\u001b[0m\n",
      "\u001b[34ms.llama.tokenization_llama.LlamaToken\u001b[0m\n",
      "\u001b[34mizer'>. This means that tokens that com\u001b[0m\n",
      "\u001b[34me after special tokens will not be pr\u001b[0m\n",
      "\u001b[34moperly handled. We recommend you to r\u001b[0m\n",
      "\u001b[34mead the related pull request available\u001b[0m\n",
      "\u001b[34mat https://github.com/huggingface/tr\u001b[0m\n",
      "\u001b[34mansformers/pull/24565\u001b[0m\n",
      "\u001b[34mprecision plug\u001b[0m\n",
      "\u001b[34min:<pytorch_lightning.plugins.precisio\u001b[0m\n",
      "\u001b[34mn.tpu.TPUPrecisionPlugin object at 0x7f2e3daf1510>\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mU\u001b[0m\n",
      "\u001b[34msing cls_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing\u001b[0m\n",
      "\u001b[34mmask_token, bu\u001b[0m\n",
      "\u001b[34mt it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_toke\u001b[0m\n",
      "\u001b[34mn, but it is not\u001b[0m\n",
      "\u001b[34mset yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_tok\u001b[0m\n",
      "\u001b[34men, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it i\u001b[0m\n",
      "\u001b[34ms not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token,\u001b[0m\n",
      "\u001b[34mbut it is not s\u001b[0m\n",
      "\u001b[34met yet.\u001b[0m\n",
      "\u001b[34mUsing sep_tok\u001b[0m\n",
      "\u001b[34men, but it is n\u001b[0m\n",
      "\u001b[34mot set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it i\u001b[0m\n",
      "\u001b[34ms not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_toke\u001b[0m\n",
      "\u001b[34mn, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mUsing s\u001b[0m\n",
      "\u001b[34mep_token, but it\u001b[0m\n",
      "\u001b[34mis not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, b\u001b[0m\n",
      "\u001b[34mut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token\u001b[0m\n",
      "\u001b[34m, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is n\u001b[0m\n",
      "\u001b[34mot set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set y\u001b[0m\n",
      "\u001b[34met.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask\u001b[0m\n",
      "\u001b[34m_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing\u001b[0m\n",
      "\u001b[34mcls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, bu\u001b[0m\n",
      "\u001b[34mt it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_toke\u001b[0m\n",
      "\u001b[34mn, but it is not\u001b[0m\n",
      "\u001b[34mset yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but\u001b[0m\n",
      "\u001b[34mit is not set y\u001b[0m\n",
      "\u001b[34met.\u001b[0m\n",
      "\u001b[34mUsi\u001b[0m\n",
      "\u001b[34mng sep_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cl\u001b[0m\n",
      "\u001b[34ms_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsin\u001b[0m\n",
      "\u001b[34mg pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_tok\u001b[0m\n",
      "\u001b[34men, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:\u001b[0m\n",
      "\u001b[34m22 megatron_bas\u001b[0m\n",
      "\u001b[34me_model:275] Padded vocab_size: 32000\u001b[0m\n",
      "\u001b[34m, original vocab_size: 32000, dummy t\u001b[0m\n",
      "\u001b[34mokens: 0.\u001b[0m\n",
      "\u001b[34mUsing\u001b[0m\n",
      "\u001b[34msep_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token,\u001b[0m\n",
      "\u001b[34mbut it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 1\u001b[0m\n",
      "\u001b[34m7:38:22 nemo_l\u001b[0m\n",
      "\u001b[34mogging:349] /usr/local/lib/python3.1\u001b[0m\n",
      "\u001b[34m0/site-packages/pytorch_lightning/tr\u001b[0m\n",
      "\u001b[34mainer/trainer.py:1908: LightningDeprec\u001b[0m\n",
      "\u001b[34mationWarning: `trainer.resume_from_ch\u001b[0m\n",
      "\u001b[34meckpoint` is deprecated in v1.5 and w\u001b[0m\n",
      "\u001b[34mill be removed in v2.0. Specify the f\u001b[0m\n",
      "\u001b[34mit checkpoint path with `trainer.fit\u001b[0m\n",
      "\u001b[34m(ckpt_path=)` instead.\n",
      "      rank_ze\u001b[0m\n",
      "\u001b[34mro_deprecation(\u001b[0m\n",
      "\u001b[34mUsing sep_to\u001b[0m\n",
      "\u001b[34mken, but it is n\u001b[0m\n",
      "\u001b[34mot set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it i\u001b[0m\n",
      "\u001b[34ms not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing m\u001b[0m\n",
      "\u001b[34mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 7 None 256 1 4\u001b[0m\n",
      "\u001b[34mprecision p\u001b[0m\n",
      "\u001b[34mlugin:<pytorch_lightning\u001b[0m\n",
      "\u001b[34m.plugins.precision.tpu.TPUPrecisionPlugin object at\u001b[0m\n",
      "\u001b[34m0x7fa08ed9b250>\u001b[0m\n",
      "\u001b[34msetup_microbatch_ca\u001b[0m\n",
      "\u001b[34mlculator 6 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy behav\u001b[0m\n",
      "\u001b[34miour of the <class 'transformers.models.\u001b[0m\n",
      "\u001b[34mllama.tokenization_llama.LlamaTokenizer'\u001b[0m\n",
      "\u001b[34m>. This means that tokens that come after\u001b[0m\n",
      "\u001b[34mspecial tokens will not be properly ha\u001b[0m\n",
      "\u001b[34mndled. We recommend you to read the rel\u001b[0m\n",
      "\u001b[34mated pull request available at https://\u001b[0m\n",
      "\u001b[34mgithub.com/huggingface/transformers/pu\u001b[0m\n",
      "\u001b[34mll/24565\u001b[0m\n",
      "\u001b[34mYou are using\u001b[0m\n",
      "\u001b[34mthe legacy behavi\u001b[0m\n",
      "\u001b[34mour of the <class 'transformers.models.l\u001b[0m\n",
      "\u001b[34mlama.tokenization_llama.LlamaTokenizer'\u001b[0m\n",
      "\u001b[34m>. This means that tokens that come afte\u001b[0m\n",
      "\u001b[34mr special tokens will not be properly h\u001b[0m\n",
      "\u001b[34mandled. We recommend you to read the r\u001b[0m\n",
      "\u001b[34melated pull request available at https\u001b[0m\n",
      "\u001b[34m://github.com/huggingface/transformers\u001b[0m\n",
      "\u001b[34m/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but\u001b[0m\n",
      "\u001b[34mit is not set yet.\u001b[0m\n",
      "\u001b[34mUsin\u001b[0m\n",
      "\u001b[34mg cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is\u001b[0m\n",
      "\u001b[34mnot set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token\u001b[0m\n",
      "\u001b[34m, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set y\u001b[0m\n",
      "\u001b[34met.\u001b[0m\n",
      "\u001b[34mUs\u001b[0m\n",
      "\u001b[34ming pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_to\u001b[0m\n",
      "\u001b[34mken, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 11 None\u001b[0m\n",
      "\u001b[34m256 1 4\u001b[0m\n",
      "\u001b[34mYou are usin\u001b[0m\n",
      "\u001b[34mg the legacy behav\u001b[0m\n",
      "\u001b[34miour of the <class 'transformers.models.\u001b[0m\n",
      "\u001b[34mllama.tokenization_llama.LlamaTokenizer\u001b[0m\n",
      "\u001b[34m'>. This means that tokens that come af\u001b[0m\n",
      "\u001b[34mter special tokens will not be properl\u001b[0m\n",
      "\u001b[34my handled. We recommend you to read th\u001b[0m\n",
      "\u001b[34me related pull request available at ht\u001b[0m\n",
      "\u001b[34mtps://github.com/huggingface/transform\u001b[0m\n",
      "\u001b[34mers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_toke\u001b[0m\n",
      "\u001b[34mn, but it is not set\u001b[0m\n",
      "\u001b[34myet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set y\u001b[0m\n",
      "\u001b[34met.\u001b[0m\n",
      "\u001b[34mUsing pa\u001b[0m\n",
      "\u001b[34md_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, bu\u001b[0m\n",
      "\u001b[34mt it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microba\u001b[0m\n",
      "\u001b[34mtch_calculator 25\u001b[0m\n",
      "\u001b[34mNone 256 1 4\u001b[0m\n",
      "\u001b[34mYou are u\u001b[0m\n",
      "\u001b[34msing the legacy\u001b[0m\n",
      "\u001b[34mbehaviour of the <class 'transformers.m\u001b[0m\n",
      "\u001b[34models.llama.tokenization_llama.LlamaTo\u001b[0m\n",
      "\u001b[34mkenizer'>. This means that tokens that\u001b[0m\n",
      "\u001b[34mcome after special tokens will not be\u001b[0m\n",
      "\u001b[34mproperly handled. We recommend you t\u001b[0m\n",
      "\u001b[34mo read the related pull request availa\u001b[0m\n",
      "\u001b[34mble at https://github.com/huggingface\u001b[0m\n",
      "\u001b[34m/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_to\u001b[0m\n",
      "\u001b[34mken, but it is not\u001b[0m\n",
      "\u001b[34mset yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is no\u001b[0m\n",
      "\u001b[34mt set yet.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:\u001b[0m\n",
      "\u001b[34m1 to store for rank: 8\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier\u001b[0m\n",
      "\u001b[34m_key:1 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier\u001b[0m\n",
      "\u001b[34m_key:1 to store for rank: 31\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barr\u001b[0m\n",
      "\u001b[34mier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 to store for rank: 18\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 to store for ran\u001b[0m\n",
      "\u001b[34mk: 11\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 to store for rank: 12\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:1 to store for rank: 20\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier_key\u001b[0m\n",
      "\u001b[34m:1 to store for rank: 21\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:1 to store for rank: 29\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:1 to store for rank:\u001b[0m\n",
      "\u001b[34m1\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier\u001b[0m\n",
      "\u001b[34m_key:1 to store for rank: 24\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:1 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:1 to store for rank: 26\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:1 to store for rank: 15\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier\u001b[0m\n",
      "\u001b[34m_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 to store for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:1 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store f\u001b[0m\n",
      "\u001b[34mor rank: 28\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: stor\u001b[0m\n",
      "\u001b[34me_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 to store for rank: 23\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier\u001b[0m\n",
      "\u001b[34m_key:1 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier\u001b[0m\n",
      "\u001b[34m_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:\u001b[0m\n",
      "\u001b[34m1 to store for rank: 19\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 14: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:2 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 29:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:2 to store for rank: 29\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barri\u001b[0m\n",
      "\u001b[34mer for key:store_based_barrier_key:1\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 12: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:2 to store for rank:\u001b[0m\n",
      "\u001b[34m12\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:2 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 20:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Comp\u001b[0m\n",
      "\u001b[34mleted store-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 0: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:2 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:2 to store for rank: 20\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank\u001b[0m\n",
      "\u001b[34m: 17\u001b[0m\n",
      "\u001b[34mRank 24\u001b[0m\n",
      "\u001b[34m: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 30\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:2\u001b[0m\n",
      "\u001b[34mto store for rank: 24\u001b[0m\n",
      "\u001b[34mAdded key: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:2 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store\u001b[0m\n",
      "\u001b[34mfor rank: 28\u001b[0m\n",
      "\u001b[34mRank 18: Com\u001b[0m\n",
      "\u001b[34mpleted store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-base\u001b[0m\n",
      "\u001b[34md barrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_key:1\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2\u001b[0m\n",
      "\u001b[34mto store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 21: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m2 to store for rank:\u001b[0m\n",
      "\u001b[34m5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for\u001b[0m\n",
      "\u001b[34mrank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:2 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:2 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Comp\u001b[0m\n",
      "\u001b[34mleted store-based barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 25:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 11:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barr\u001b[0m\n",
      "\u001b[34mier_key:2 to store for rank: 11\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_based_barrier_key:2 to\u001b[0m\n",
      "\u001b[34mstore for rank: 25\u001b[0m\n",
      "\u001b[34mRank 4\u001b[0m\n",
      "\u001b[34m: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 23: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34mA\u001b[0m\n",
      "\u001b[34mdded key: store_based_barrier_key:2 t\u001b[0m\n",
      "\u001b[34mo store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 10: Completed\u001b[0m\n",
      "\u001b[34mstore-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 8: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:2 to store for rank:\u001b[0m\n",
      "\u001b[34m10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for\u001b[0m\n",
      "\u001b[34mrank: 8\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:2 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barri\u001b[0m\n",
      "\u001b[34mer for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:2 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 26\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:2 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m6: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier\u001b[0m\n",
      "\u001b[34m_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 27: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Compl\u001b[0m\n",
      "\u001b[34meted store-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:2 to store for rank\u001b[0m\n",
      "\u001b[34m: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:2 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 27:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:3 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:3 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 14: Completed store-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAd\u001b[0m\n",
      "\u001b[34mded key: store_based_barrier_key:3 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 14\u001b[0m\n",
      "\u001b[34mRank 29: Com\u001b[0m\n",
      "\u001b[34mpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:3 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 29\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m2: Comple\u001b[0m\n",
      "\u001b[34mted store-based barr\u001b[0m\n",
      "\u001b[34mier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 19: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:2 w\u001b[0m\n",
      "\u001b[34mith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:3 to store for rank: 12\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for\u001b[0m\n",
      "\u001b[34mrank: 19\u001b[0m\n",
      "\u001b[34mRank 16\u001b[0m\n",
      "\u001b[34m: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 20: Completed store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 0: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:3 to st\u001b[0m\n",
      "\u001b[34more for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m3 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 17: Compl\u001b[0m\n",
      "\u001b[34meted store-based barrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:3 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 3\u001b[0m\n",
      "\u001b[34m0: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barri\u001b[0m\n",
      "\u001b[34mer for key:store_based_barrier_key:2\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m0\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for\u001b[0m\n",
      "\u001b[34mrank: 24\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:\u001b[0m\n",
      "\u001b[34m2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:3 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 18\u001b[0m\n",
      "\u001b[34m: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:3 to store for rank:\u001b[0m\n",
      "\u001b[34m18\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 2: Completed store-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 22: Completed store-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34mA\u001b[0m\n",
      "\u001b[34mdded key: store_based_barrier_key:3 to store for rank\u001b[0m\n",
      "\u001b[34m: 2\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:3 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-base\u001b[0m\n",
      "\u001b[34md barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:2 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:3 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 3\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m11: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrie\u001b[0m\n",
      "\u001b[34mr for key:store_based_barrier_key:2 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:3 to store for rank: 11\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:3 to store for rank: 25\u001b[0m\n",
      "\u001b[34mRank 4: C\u001b[0m\n",
      "\u001b[34mompleted store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:3 to store for\u001b[0m\n",
      "\u001b[34mrank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to stor\u001b[0m\n",
      "\u001b[34me for rank: 23\u001b[0m\n",
      "\u001b[34mRank 10\u001b[0m\n",
      "\u001b[34m: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m8: Completed store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:3 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 8\u001b[0m\n",
      "\u001b[34mRank 31: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_based_barrier_key:2\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3\u001b[0m\n",
      "\u001b[34mto store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-base\u001b[0m\n",
      "\u001b[34md barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:2 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:3 to store for\u001b[0m\n",
      "\u001b[34mrank: 26\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 15\u001b[0m\n",
      "\u001b[34mRank 6:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:3 to\u001b[0m\n",
      "\u001b[34mstore for rank: 6\u001b[0m\n",
      "\u001b[34mRank 6: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_based_barrier_key:4 to st\u001b[0m\n",
      "\u001b[34more for rank: 6\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrie\u001b[0m\n",
      "\u001b[34mr for key:store_based_barrier_key:3 w\u001b[0m\n",
      "\u001b[34mith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:4 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 9\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barr\u001b[0m\n",
      "\u001b[34mier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:4 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 29:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank\u001b[0m\n",
      "\u001b[34m: 29\u001b[0m\n",
      "\u001b[34mRank 12:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for\u001b[0m\n",
      "\u001b[34mrank: 12\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:3 with 32 node\u001b[0m\n",
      "\u001b[34ms.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for r\u001b[0m\n",
      "\u001b[34mank: 19\u001b[0m\n",
      "\u001b[34mRank 16: C\u001b[0m\n",
      "\u001b[34mompleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 20: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 0: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:4 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:4 to st\u001b[0m\n",
      "\u001b[34more for rank: 20\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key:store_based_barrier_key:3\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4\u001b[0m\n",
      "\u001b[34mto store for rank: 0\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:4 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 30: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:3 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:4 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:4 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:3 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:4 to store for rank:\u001b[0m\n",
      "\u001b[34m13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for\u001b[0m\n",
      "\u001b[34mrank: 1\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 18: C\u001b[0m\n",
      "\u001b[34mompleted store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_barrier_key:3 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m8\u001b[0m\n",
      "\u001b[34mRank 5: Comp\u001b[0m\n",
      "\u001b[34mleted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2: Compl\u001b[0m\n",
      "\u001b[34meted store-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34mA\u001b[0m\n",
      "\u001b[34mdded key: store_based_barrier_key:4 to store for rank:\u001b[0m\n",
      "\u001b[34m22\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed\u001b[0m\n",
      "\u001b[34mstore-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4\u001b[0m\n",
      "\u001b[34mto store for ra\u001b[0m\n",
      "\u001b[34mnk: 3\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m1: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 25: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:4 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 11\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4\u001b[0m\n",
      "\u001b[34mto store for rank: 25\u001b[0m\n",
      "\u001b[34mRank 4:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2\u001b[0m\n",
      "\u001b[34m3: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:4 to stor\u001b[0m\n",
      "\u001b[34me for rank: 23\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4\u001b[0m\n",
      "\u001b[34mto store for rank: 4\u001b[0m\n",
      "\u001b[34mRank 10\u001b[0m\n",
      "\u001b[34m: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m8: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m31: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:4 to store\u001b[0m\n",
      "\u001b[34mfor rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4\u001b[0m\n",
      "\u001b[34mto store for rank: 8\u001b[0m\n",
      "\u001b[34mAdded key: stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:4 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:3 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:4 to store\u001b[0m\n",
      "\u001b[34mfor rank: 26\u001b[0m\n",
      "\u001b[34mRank 15: Completed store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store f\u001b[0m\n",
      "\u001b[34mor rank: 15\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 15: Completed store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:5 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 6:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:5 to store for rank:\u001b[0m\n",
      "\u001b[34m6\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_key:4\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5\u001b[0m\n",
      "\u001b[34mto store for rank: 9\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:5 to store for rank: 27\u001b[0m\n",
      "\u001b[34mRank 14: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAd\u001b[0m\n",
      "\u001b[34mded key: store_based_barrier_key:5 to\u001b[0m\n",
      "\u001b[34mstore for rank: 14\u001b[0m\n",
      "\u001b[34mRank 29:\u001b[0m\n",
      "\u001b[34mCompleted sto\u001b[0m\n",
      "\u001b[34mre-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:4 w\u001b[0m\n",
      "\u001b[34mith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:5 to store for rank: 29\u001b[0m\n",
      "\u001b[34mRank 12:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m2\u001b[0m\n",
      "\u001b[34mRank 19:\u001b[0m\n",
      "\u001b[34mCompleted sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:5 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 16: C\u001b[0m\n",
      "\u001b[34mompleted store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 20:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 0: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:5 to store for\u001b[0m\n",
      "\u001b[34mrank: 16\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:4 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store\u001b[0m\n",
      "\u001b[34mfor rank: 0\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:5 to store for rank: 20\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:5 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 30\u001b[0m\n",
      "\u001b[34m: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2\u001b[0m\n",
      "\u001b[34m4: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:5 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:5 to store for ran\u001b[0m\n",
      "\u001b[34mk: 24\u001b[0m\n",
      "\u001b[34mR\u001b[0m\n",
      "\u001b[34mank 13: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:4 with 32 nodes\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:5 to st\u001b[0m\n",
      "\u001b[34more for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 28: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:5 to store\u001b[0m\n",
      "\u001b[34mfor rank: 28\u001b[0m\n",
      "\u001b[34mRank 18: Com\u001b[0m\n",
      "\u001b[34mpleted store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Compl\u001b[0m\n",
      "\u001b[34meted stor\u001b[0m\n",
      "\u001b[34me-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:5\u001b[0m\n",
      "\u001b[34mto store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 2:\u001b[0m\n",
      "\u001b[34mCompleted store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:4 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based ba\u001b[0m\n",
      "\u001b[34mrrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:5 to store for\u001b[0m\n",
      "\u001b[34mrank: 5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to stor\u001b[0m\n",
      "\u001b[34me for rank: 2\u001b[0m\n",
      "\u001b[34mRank 21: Completed store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:5 to store for rank: 22\u001b[0m\n",
      "\u001b[34mA\u001b[0m\n",
      "\u001b[34mdded key: store_based_barrier_key:5 to stor\u001b[0m\n",
      "\u001b[34me for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:5 to store for rank\u001b[0m\n",
      "\u001b[34m: 3\u001b[0m\n",
      "\u001b[34mRank 11\u001b[0m\n",
      "\u001b[34m: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:5 to store for rank:\u001b[0m\n",
      "\u001b[34m11\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:5 to store for rank: 25\u001b[0m\n",
      "\u001b[34mRank 23:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: C\u001b[0m\n",
      "\u001b[34mompleted store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:5 to store for ran\u001b[0m\n",
      "\u001b[34mk: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for\u001b[0m\n",
      "\u001b[34mrank: 23\u001b[0m\n",
      "\u001b[34mRank 10:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 8:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:5 to store\u001b[0m\n",
      "\u001b[34mfor rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 t\u001b[0m\n",
      "\u001b[34mo store for rank: 8\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:5 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 26:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barr\u001b[0m\n",
      "\u001b[34mier_key:5 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m26: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:6 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 26\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:5 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:6 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 6:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m6 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 9: Compl\u001b[0m\n",
      "\u001b[34meted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:6 to st\u001b[0m\n",
      "\u001b[34more for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m6 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 14: Compl\u001b[0m\n",
      "\u001b[34meted store-based barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:6 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 29: Com\u001b[0m\n",
      "\u001b[34mpleted store-base\u001b[0m\n",
      "\u001b[34md barrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for\u001b[0m\n",
      "\u001b[34mrank: 29\u001b[0m\n",
      "\u001b[34mRank 12: Co\u001b[0m\n",
      "\u001b[34mmpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdd\u001b[0m\n",
      "\u001b[34med key: store_based_barrier_key:6 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 12\u001b[0m\n",
      "\u001b[34mRank 19:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store\u001b[0m\n",
      "\u001b[34mfor rank: 19\u001b[0m\n",
      "\u001b[34mRank 0: Co\u001b[0m\n",
      "\u001b[34mmpleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 20: Comp\u001b[0m\n",
      "\u001b[34mleted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:6 to store for rank\u001b[0m\n",
      "\u001b[34m: 0\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for\u001b[0m\n",
      "\u001b[34mrank: 20\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_key:5 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to stor\u001b[0m\n",
      "\u001b[34me for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:6 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 3\u001b[0m\n",
      "\u001b[34m0: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6\u001b[0m\n",
      "\u001b[34mto store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 24: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:6 to store for r\u001b[0m\n",
      "\u001b[34mank: 24\u001b[0m\n",
      "\u001b[34mRank 13:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m1: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:6 to st\u001b[0m\n",
      "\u001b[34more for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 28: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for r\u001b[0m\n",
      "\u001b[34mank: 28\u001b[0m\n",
      "\u001b[34mRank 18: Comp\u001b[0m\n",
      "\u001b[34mleted store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_key:5\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:6 to store for r\u001b[0m\n",
      "\u001b[34mank: 18\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:5 with 32 no\u001b[0m\n",
      "\u001b[34mdes.\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 5\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key:store_based_barrier_key:5\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:5 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store\u001b[0m\n",
      "\u001b[34mfor rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:6 to store for rank: 22\u001b[0m\n",
      "\u001b[34mRank 21: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6\u001b[0m\n",
      "\u001b[34mto store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:5 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 11:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:6 to stor\u001b[0m\n",
      "\u001b[34me for rank: 11\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barr\u001b[0m\n",
      "\u001b[34mier for key:store_based_barrier_key:5\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 t\u001b[0m\n",
      "\u001b[34mo store for rank: 25\u001b[0m\n",
      "\u001b[34mRank 4:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:6 to store f\u001b[0m\n",
      "\u001b[34mor rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to st\u001b[0m\n",
      "\u001b[34more for rank: 23\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m10: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m8: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:6 to store\u001b[0m\n",
      "\u001b[34mfor rank: 10\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:5 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 8\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:6 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 8: Compl\u001b[0m\n",
      "\u001b[34meted store-based barrier for key:store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:7 to store for rank: 8\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:7 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 26:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:7 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m15: Completed store-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:7 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 6: Com\u001b[0m\n",
      "\u001b[34mpleted store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to\u001b[0m\n",
      "\u001b[34mstore for rank: 6\u001b[0m\n",
      "\u001b[34mRank 27: Co\u001b[0m\n",
      "\u001b[34mmpleted store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:7 to store\u001b[0m\n",
      "\u001b[34mfor rank: 9\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to\u001b[0m\n",
      "\u001b[34mstore for rank: 27\u001b[0m\n",
      "\u001b[34mRank 14: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based_barrier_key\u001b[0m\n",
      "\u001b[34m:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:7 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 29:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m7 to store for\u001b[0m\n",
      "\u001b[34mrank: 29\u001b[0m\n",
      "\u001b[34mRank 12:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 12\u001b[0m\n",
      "\u001b[34mRank 19:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank\u001b[0m\n",
      "\u001b[34m: 19\u001b[0m\n",
      "\u001b[34mRank 0: Co\u001b[0m\n",
      "\u001b[34mmpleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 20: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m7: Completed store-based barrier for ke\u001b[0m\n",
      "\u001b[34my:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:7 to store f\u001b[0m\n",
      "\u001b[34mor rank: 20\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to st\u001b[0m\n",
      "\u001b[34more for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:7 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 3\u001b[0m\n",
      "\u001b[34m0: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mto store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 24: Compl\u001b[0m\n",
      "\u001b[34meted store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:7 to store for\u001b[0m\n",
      "\u001b[34mrank: 24\u001b[0m\n",
      "\u001b[34mRank 13:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:7 to store\u001b[0m\n",
      "\u001b[34mfor rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mto store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 28: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key\u001b[0m\n",
      "\u001b[34m:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 18: C\u001b[0m\n",
      "\u001b[34mompleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:7 to store for rank\u001b[0m\n",
      "\u001b[34m: 18\u001b[0m\n",
      "\u001b[34mRank 5: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to st\u001b[0m\n",
      "\u001b[34more for rank: 5\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrie\u001b[0m\n",
      "\u001b[34mr for key:store_based_barrier_key:6\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 22\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:7 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:6 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:7 to store\u001b[0m\n",
      "\u001b[34mfor rank: 3\u001b[0m\n",
      "\u001b[34mRank 11:\u001b[0m\n",
      "\u001b[34mCompleted store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:7 to store for rank\u001b[0m\n",
      "\u001b[34m: 11\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for\u001b[0m\n",
      "\u001b[34mrank: 25\u001b[0m\n",
      "\u001b[34mRank 4: Com\u001b[0m\n",
      "\u001b[34mpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:7 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store\u001b[0m\n",
      "\u001b[34mfor rank: 23\u001b[0m\n",
      "\u001b[34mRank 10:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mto store for rank: 10\u001b[0m\n",
      "\u001b[34mRank 10: Compl\u001b[0m\n",
      "\u001b[34meted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 8: Comple\u001b[0m\n",
      "\u001b[34mted store-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:8 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAd\u001b[0m\n",
      "\u001b[34mded key: store_based_barrier_key:8 to store for rank:\u001b[0m\n",
      "\u001b[34m8\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for\u001b[0m\n",
      "\u001b[34mrank: 31\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_key:7 with 32\u001b[0m\n",
      "\u001b[34mnodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to stor\u001b[0m\n",
      "\u001b[34me for rank: 26\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:7 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:8 to store\u001b[0m\n",
      "\u001b[34mfor rank: 15\u001b[0m\n",
      "\u001b[34mRank 6:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 t\u001b[0m\n",
      "\u001b[34mo store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:7 with 32 nod\u001b[0m\n",
      "\u001b[34mes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barr\u001b[0m\n",
      "\u001b[34mier for key:store_based_barrier_key:7 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to s\u001b[0m\n",
      "\u001b[34mtore for rank: 9\u001b[0m\n",
      "\u001b[34mRank 14: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8\u001b[0m\n",
      "\u001b[34mto store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store\u001b[0m\n",
      "\u001b[34m_based_barrier_key:8 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 29: Co\u001b[0m\n",
      "\u001b[34mmpleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for\u001b[0m\n",
      "\u001b[34mrank: 29\u001b[0m\n",
      "\u001b[34mRank 12: Completed st\u001b[0m\n",
      "\u001b[34more-based barri\u001b[0m\n",
      "\u001b[34mer for key:store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:8 to st\u001b[0m\n",
      "\u001b[34more for rank: 12\u001b[0m\n",
      "\u001b[34mRank 19: C\u001b[0m\n",
      "\u001b[34mompleted store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:8 to store for\u001b[0m\n",
      "\u001b[34mrank: 19\u001b[0m\n",
      "\u001b[34mRank 0: Com\u001b[0m\n",
      "\u001b[34mpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 20: C\u001b[0m\n",
      "\u001b[34mompleted store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:8 to store for ran\u001b[0m\n",
      "\u001b[34mk: 20\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:7 with 32 nodes\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for\u001b[0m\n",
      "\u001b[34mrank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier\u001b[0m\n",
      "\u001b[34m_key:8 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 3\u001b[0m\n",
      "\u001b[34m0: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAd\u001b[0m\n",
      "\u001b[34mded key: store_based_barrier_key:8 to\u001b[0m\n",
      "\u001b[34mstore for rank: 30\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:8 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 13:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for\u001b[0m\n",
      "\u001b[34mrank: 13\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m1: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:8 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 1\u001b[0m\n",
      "\u001b[34mRank 28: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 18: Com\u001b[0m\n",
      "\u001b[34mpleted store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:8 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:8 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 5\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based bar\u001b[0m\n",
      "\u001b[34mrier for key:store_based_barrier_key:7\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 t\u001b[0m\n",
      "\u001b[34mo store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:8 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 21\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:8 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barri\u001b[0m\n",
      "\u001b[34mer_key:7 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:8 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 3\u001b[0m\n",
      "\u001b[34mRank 11:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:8 to store for r\u001b[0m\n",
      "\u001b[34mank: 11\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store\u001b[0m\n",
      "\u001b[34mfor rank: 25\u001b[0m\n",
      "\u001b[34mRank 4: Comp\u001b[0m\n",
      "\u001b[34mleted store-base\u001b[0m\n",
      "\u001b[34md barrier for key:store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:8 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m23: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdde\u001b[0m\n",
      "\u001b[34md key: store_based_barrier_key:9 to st\u001b[0m\n",
      "\u001b[34more for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:\u001b[0m\n",
      "\u001b[34m9 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:\u001b[0m\n",
      "\u001b[34m8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 8: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Complete\u001b[0m\n",
      "\u001b[34md store-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:9 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 8\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:9 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 26:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:9 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m5: Completed store-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:9 to store fo\u001b[0m\n",
      "\u001b[34mr rank: 15\u001b[0m\n",
      "\u001b[34mRank 6: Com\u001b[0m\n",
      "\u001b[34mpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for ra\u001b[0m\n",
      "\u001b[34mnk: 6\u001b[0m\n",
      "\u001b[34mRank 9:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27\u001b[0m\n",
      "\u001b[34m: Completed store-based barrier for ke\u001b[0m\n",
      "\u001b[34my:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key\u001b[0m\n",
      "\u001b[34m: store_based_barrier_key:9 to store f\u001b[0m\n",
      "\u001b[34mor rank: 9\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:8 with 3\u001b[0m\n",
      "\u001b[34m2 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to st\u001b[0m\n",
      "\u001b[34more for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:9 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 29:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:9 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 29\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier\u001b[0m\n",
      "\u001b[34m_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:9 to sto\u001b[0m\n",
      "\u001b[34mre for rank: 12\u001b[0m\n",
      "\u001b[34mRank 19:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for r\u001b[0m\n",
      "\u001b[34mank: 19\u001b[0m\n",
      "\u001b[34mRank 0: Comple\u001b[0m\n",
      "\u001b[34mted store-based b\u001b[0m\n",
      "\u001b[34marrier for key:store_based_barrier_key:8\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 20: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRan\u001b[0m\n",
      "\u001b[34mk 17: Completed store-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:9 to store for rank: 20\u001b[0m\n",
      "\u001b[34mAdded\u001b[0m\n",
      "\u001b[34mkey: store_based_barrier_key:9 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for k\u001b[0m\n",
      "\u001b[34mey:store_based_barrier_key:8 with 32 no\u001b[0m\n",
      "\u001b[34mdes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_ke\u001b[0m\n",
      "\u001b[34my:9 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 30:\u001b[0m\n",
      "\u001b[34mCompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded ke\u001b[0m\n",
      "\u001b[34my: store_based_barrier_key:9 to store\u001b[0m\n",
      "\u001b[34mfor rank: 30\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:8 wit\u001b[0m\n",
      "\u001b[34mh 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to\u001b[0m\n",
      "\u001b[34mstore for rank: 24\u001b[0m\n",
      "\u001b[34mRank 13: Co\u001b[0m\n",
      "\u001b[34mmpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 13\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank:\u001b[0m\n",
      "\u001b[34m1\u001b[0m\n",
      "\u001b[34mRank 28: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:s\u001b[0m\n",
      "\u001b[34mtore_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 18: Com\u001b[0m\n",
      "\u001b[34mpleted store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrier_\u001b[0m\n",
      "\u001b[34mkey:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m8\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:8\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank\u001b[0m\n",
      "\u001b[34m: 5\u001b[0m\n",
      "\u001b[34mRank 7: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:sto\u001b[0m\n",
      "\u001b[34mre_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Com\u001b[0m\n",
      "\u001b[34mpleted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2: Complet\u001b[0m\n",
      "\u001b[34med store-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 22\u001b[0m\n",
      "\u001b[34mR\u001b[0m\n",
      "\u001b[34mank 21: Completed store-based barrier for\u001b[0m\n",
      "\u001b[34mkey:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded k\u001b[0m\n",
      "\u001b[34mey: store_based_barrier_key:9 to store\u001b[0m\n",
      "\u001b[34mfor rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to\u001b[0m\n",
      "\u001b[34mstore for rank: 21\u001b[0m\n",
      "\u001b[34mRank 3: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_based_barrier_key:8\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 11\u001b[0m\n",
      "\u001b[34m: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25:\u001b[0m\n",
      "\u001b[34mCompleted store-based barrier for key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key:\u001b[0m\n",
      "\u001b[34mstore_based_barrier_key:9 to store for\u001b[0m\n",
      "\u001b[34mrank: 11\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to stor\u001b[0m\n",
      "\u001b[34me for rank: 25\u001b[0m\n",
      "\u001b[34mRank 25: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_barrier_key:9 w\u001b[0m\n",
      "\u001b[34mith 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_mi\u001b[0m\n",
      "\u001b[34mcrobatch_calcul\u001b[0m\n",
      "\u001b[34mator 25 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 4\u001b[0m\n",
      "\u001b[34m: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_bas\u001b[0m\n",
      "\u001b[34med_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRa\u001b[0m\n",
      "\u001b[34mnk 23: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_micr\u001b[0m\n",
      "\u001b[34mobatch_calculat\u001b[0m\n",
      "\u001b[34mor 4 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_\u001b[0m\n",
      "\u001b[34mcalculator 23 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m0: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier\u001b[0m\n",
      "\u001b[34mfor key:store_based_barrier_key:9 wi\u001b[0m\n",
      "\u001b[34mth 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_ba\u001b[0m\n",
      "\u001b[34mrrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calcula\u001b[0m\n",
      "\u001b[34mtor 10 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_\u001b[0m\n",
      "\u001b[34mcalculator 31 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculato\u001b[0m\n",
      "\u001b[34mr 8 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_barrier_key:9 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based\u001b[0m\n",
      "\u001b[34m_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 26 None\u001b[0m\n",
      "\u001b[34m256 1 4\u001b[0m\n",
      "\u001b[34msetup_mi\u001b[0m\n",
      "\u001b[34mcrobatch_calcu\u001b[0m\n",
      "\u001b[34mlator 15 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 6: Comp\u001b[0m\n",
      "\u001b[34mleted store-base\u001b[0m\n",
      "\u001b[34md barrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barri\u001b[0m\n",
      "\u001b[34mer for key:store_based_barrier_key:9 w\u001b[0m\n",
      "\u001b[34mith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed st\u001b[0m\n",
      "\u001b[34more-based barrier for key:store_based_\u001b[0m\n",
      "\u001b[34mbarrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 14: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculat\u001b[0m\n",
      "\u001b[34mor 6 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calcu\u001b[0m\n",
      "\u001b[34mlator 9 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 27 N\u001b[0m\n",
      "\u001b[34mone 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 14 None 256 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34mRank 29: Co\u001b[0m\n",
      "\u001b[34mmpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 12: Comp\u001b[0m\n",
      "\u001b[34mleted store-bas\u001b[0m\n",
      "\u001b[34med barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 29 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_micro\u001b[0m\n",
      "\u001b[34mbatch_calculator\u001b[0m\n",
      "\u001b[34m12 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 19: Com\u001b[0m\n",
      "\u001b[34mpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_mi\u001b[0m\n",
      "\u001b[34mcrobatch_calcul\u001b[0m\n",
      "\u001b[34mator 19 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 0: Co\u001b[0m\n",
      "\u001b[34mmpleted store-b\u001b[0m\n",
      "\u001b[34mased barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 20: C\u001b[0m\n",
      "\u001b[34mompleted store-based barrier for key:st\u001b[0m\n",
      "\u001b[34more_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Comp\u001b[0m\n",
      "\u001b[34mleted store-based barrier for key:stor\u001b[0m\n",
      "\u001b[34me_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 0 Non\u001b[0m\n",
      "\u001b[34me 256 1 4\u001b[0m\n",
      "\u001b[34mRank\u001b[0m\n",
      "\u001b[34m16: Completed store-based barrier fo\u001b[0m\n",
      "\u001b[34mr key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34ms\u001b[0m\n",
      "\u001b[34metup_microbatch_calculator 20 None 256\u001b[0m\n",
      "\u001b[34m1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 17 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_micr\u001b[0m\n",
      "\u001b[34mobatch_calculat\u001b[0m\n",
      "\u001b[34mor 16 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 3\u001b[0m\n",
      "\u001b[34m0: Completed\u001b[0m\n",
      "\u001b[34mstore-based barrier for key:store_\u001b[0m\n",
      "\u001b[34mbased_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 24:\u001b[0m\n",
      "\u001b[34mCompleted stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_base\u001b[0m\n",
      "\u001b[34md_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetu\u001b[0m\n",
      "\u001b[34mp_microbatch_calculator 30 None 256 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34msetup_mi\u001b[0m\n",
      "\u001b[34mcrobatch_calcul\u001b[0m\n",
      "\u001b[34mator 24 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barri\u001b[0m\n",
      "\u001b[34mer for key:store_based_barrier_key:9\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 1: C\u001b[0m\n",
      "\u001b[34mompleted store\u001b[0m\n",
      "\u001b[34m-based barrier for key:store_based_bar\u001b[0m\n",
      "\u001b[34mrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_micr\u001b[0m\n",
      "\u001b[34mobatch_calculator 13 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_micr\u001b[0m\n",
      "\u001b[34mobatch_calcula\u001b[0m\n",
      "\u001b[34mtor 1 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 28: Co\u001b[0m\n",
      "\u001b[34mmpleted store-based barrier f\u001b[0m\n",
      "\u001b[34mor key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbat\u001b[0m\n",
      "\u001b[34mch_calculator 28 None 256\u001b[0m\n",
      "\u001b[34m1 4\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based\u001b[0m\n",
      "\u001b[34mbarrier for key:store_based_barrier_k\u001b[0m\n",
      "\u001b[34mey:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Co\u001b[0m\n",
      "\u001b[34mmpleted store-ba\u001b[0m\n",
      "\u001b[34msed barrier for key:store_based_barrie\u001b[0m\n",
      "\u001b[34mr_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatc\u001b[0m\n",
      "\u001b[34mh_calculator 18 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 7: Completed st\u001b[0m\n",
      "\u001b[34more-based barri\u001b[0m\n",
      "\u001b[34mer for key:store_based_barrier_key:9\u001b[0m\n",
      "\u001b[34mwith 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2: Completed sto\u001b[0m\n",
      "\u001b[34mre-based barrier for key:store_based_b\u001b[0m\n",
      "\u001b[34marrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-\u001b[0m\n",
      "\u001b[34mbased barrier for key:store_based_barr\u001b[0m\n",
      "\u001b[34mier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator\u001b[0m\n",
      "\u001b[34m5 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 21: Completed stor\u001b[0m\n",
      "\u001b[34me-based barrier for key:store_based_barrier_key:9 with\u001b[0m\n",
      "\u001b[34m32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 22 None 256 1\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 7 None\u001b[0m\n",
      "\u001b[34m256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 2 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 21 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_b\u001b[0m\n",
      "\u001b[34mased_barrier_ke\u001b[0m\n",
      "\u001b[34my:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_micr\u001b[0m\n",
      "\u001b[34mobatch_calculato\u001b[0m\n",
      "\u001b[34mr 3 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 1\u001b[0m\n",
      "\u001b[34m1: Completed s\u001b[0m\n",
      "\u001b[34mtore-based barrier for key:store_ba\u001b[0m\n",
      "\u001b[34msed_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_mic\u001b[0m\n",
      "\u001b[34mrobatch_calcula\u001b[0m\n",
      "\u001b[34mtor 11 None 256 1 4\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron\u001b[0m\n",
      "\u001b[34m_gpt_model:940] Started profiling server\u001b[0m\n",
      "\u001b[34mfor dp_rank=0, tp_rank=0, pp_rank=0, vp_rank=None on port:9000\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:\u001b[0m\n",
      "\u001b[34m28 megatron_init:255] Rank 0 ha\u001b[0m\n",
      "\u001b[34ms data parallel group: [0, 8, 16, 24]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron_ini\u001b[0m\n",
      "\u001b[34mt:258] All data parallel group ranks: [[0, 8, 16, 24], [1, 9, 1\u001b[0m\n",
      "\u001b[34m7, 25], [2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22,\u001b[0m\n",
      "\u001b[34m30], [7, 15, 23, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron_init\u001b[0m\n",
      "\u001b[34m:259] Ranks 0 has data parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28\u001b[0m\n",
      "\u001b[34mmegatron_init:267] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023\u001b[0m\n",
      "\u001b[34m-12-09 17:38:28 megatron_init:268] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7],\u001b[0m\n",
      "\u001b[34m[8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30\u001b[0m\n",
      "\u001b[34m, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron_init:278] Rank 0 has tenso\u001b[0m\n",
      "\u001b[34mr model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-\u001b[0m\n",
      "\u001b[34m09 17:38:28 megatron_init:282] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5,\u001b[0m\n",
      "\u001b[34m6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28,\u001b[0m\n",
      "\u001b[34m29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron_init:283] Rank\u001b[0m\n",
      "\u001b[34m0 has tensor model parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megat\u001b[0m\n",
      "\u001b[34mron_init:297] Rank 0 has pipeline model parallel group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron\u001b[0m\n",
      "\u001b[34m_init:309] Rank 0 has embedding group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron_init:315] All\u001b[0m\n",
      "\u001b[34mpipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],\u001b[0m\n",
      "\u001b[34m[11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27\u001b[0m\n",
      "\u001b[34m], [28], [29], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron_init:316\u001b[0m\n",
      "\u001b[34m] Rank 0 has pipeline model parallel rank 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17\u001b[0m\n",
      "\u001b[34m:38:28 megatron_init:317] All embedding group ranks: [[0], [1], [2], [3], [\u001b[0m\n",
      "\u001b[34m4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20\u001b[0m\n",
      "\u001b[34m], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30],\u001b[0m\n",
      "\u001b[34m[31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:28 megatron_init:318] Rank 0 h\u001b[0m\n",
      "\u001b[34mas embedding rank: 0\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel ra\u001b[0m\n",
      "\u001b[34mnk (3, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel\u001b[0m\n",
      "\u001b[34mrank (5, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (1, 0): 842534912\u001b[0m\n",
      "\u001b[34m> numbe\u001b[0m\n",
      "\u001b[34mr of parameters on\u001b[0m\n",
      "\u001b[34m(tensor, pipeline) model parallel rank (4, 0):\u001b[0m\n",
      "\u001b[34m842534912\u001b[0m\n",
      "\u001b[34m> number\u001b[0m\n",
      "\u001b[34mof parameters on\u001b[0m\n",
      "\u001b[34m(tensor, pipeline) model parallel rank (\u001b[0m\n",
      "\u001b[34m0, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number\u001b[0m\n",
      "\u001b[34mof parameters on\u001b[0m\n",
      "\u001b[34m(tensor, pipeline) model parallel rank\u001b[0m\n",
      "\u001b[34m(2, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number\u001b[0m\n",
      "\u001b[34mof parameters on\u001b[0m\n",
      "\u001b[34m(tensor, pipeline) model parallel rank (\u001b[0m\n",
      "\u001b[34m6, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number\u001b[0m\n",
      "\u001b[34mof parameters o\u001b[0m\n",
      "\u001b[34mn (tensor, pipeline) model parallel r\u001b[0m\n",
      "\u001b[34mank (7, 0): 842534912\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:38:38 megatron_base_model:469] Cannot parse the checkpoint fi\u001b[0m\n",
      "\u001b[34mle to get the consumed samples. assume it is zero.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-0\u001b[0m\n",
      "\u001b[34m9 17:38:38 megatron_gpt_model:845] Building GPT datasets.\u001b[0m\n",
      "\u001b[34m[NeMo I 20\u001b[0m\n",
      "\u001b[34m23-12-09 17:38:38\u001b[0m\n",
      "\u001b[34mgpt_dataset:293]  > building dataset\u001b[0m\n",
      "\u001b[34mindex ...\u001b[0m\n",
      "\u001b[34m[NeMo I 2\u001b[0m\n",
      "\u001b[34m023-12-09 17:38:\u001b[0m\n",
      "\u001b[34m38 indexed_dataset:453]     reading\u001b[0m\n",
      "\u001b[34msizes...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:38\u001b[0m\n",
      "\u001b[34mindexed_dataset:455]     reading poi\u001b[0m\n",
      "\u001b[34mnters...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:38 indexed_datase\u001b[0m\n",
      "\u001b[34mt:459]     reading document index...\u001b[0m\n",
      "\u001b[34m[\u001b[0m\n",
      "\u001b[34mNeMo I 2023-12-09 17:38:38 indexed_dataset:512]     c\u001b[0m\n",
      "\u001b[34mreating numpy buffer of mmap...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 1\u001b[0m\n",
      "\u001b[34m7:38:38 indexed_dataset:514]     creati\u001b[0m\n",
      "\u001b[34mng memory view of numpy buffer...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09\u001b[0m\n",
      "\u001b[34m17:38:38 gpt_dataset:297]  > finished\u001b[0m\n",
      "\u001b[34mcreating indexed dataset in 0.000508 seconds\u001b[0m\n",
      "\u001b[34m[NeMo I 2\u001b[0m\n",
      "\u001b[34m023-12-09 17:38:38 gpt_dataset:298]\u001b[0m\n",
      "\u001b[34mnumber of documents: 1355\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38\u001b[0m\n",
      "\u001b[34m:38 gpt_dataset:249]  > dataset split\u001b[0m\n",
      "\u001b[34m:\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:38 gpt_dataset:252]     tra\u001b[0m\n",
      "\u001b[34min:\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:38 gpt_d\u001b[0m\n",
      "\u001b[34mataset:253]      document indices in [0, 1355) total\u001b[0m\n",
      "\u001b[34mof 1355 documents\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:38 gpt\u001b[0m\n",
      "\u001b[34m_dataset:252]     validation:\u001b[0m\n",
      "\u001b[34m[NeMo I\u001b[0m\n",
      "\u001b[34m2023-12-09 17:38:38 gpt_dataset:253]      document ind\u001b[0m\n",
      "\u001b[34mices in [1355, 1355) total of 0 documents\u001b[0m\n",
      "\u001b[34m[NeMo I 2\u001b[0m\n",
      "\u001b[34m023-12-09 17:38:38 gpt_dataset:252]\u001b[0m\n",
      "\u001b[34mtest:\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:38 gpt_dataset:253]\u001b[0m\n",
      "\u001b[34mdocument indices in [1355, 1355\u001b[0m\n",
      "\u001b[34m) total of 0 documents\u001b[0m\n",
      "\u001b[34m[NeMo I 20\u001b[0m\n",
      "\u001b[34m23-12-09 17:38:3\u001b[0m\n",
      "\u001b[34m8 gpt_dataset:548]  > WARNING: could no\u001b[0m\n",
      "\u001b[34mt find index map files, building the in\u001b[0m\n",
      "\u001b[34mdices on rank 0 ...\u001b[0m\n",
      "\u001b[34mnum_samples_from_\u001b[0m\n",
      "\u001b[34mepochs_minus_one: 621 || num_epochs: 3\u001b[0m\n",
      "\u001b[34m|| tokens_per_epoch: 636824 || add_extra_token: 1 |\u001b[0m\n",
      "\u001b[34m| seq_length: 2048\u001b[0m\n",
      "\u001b[34mnum_samples: 772 |\u001b[0m\n",
      "\u001b[34m|??? \n",
      " > last epoch number of samples\u001b[0m\n",
      "\u001b[34m(151) is smaller than 80% of number of samples per\u001b[0m\n",
      "\u001b[34mepoch (310), setting separate_last_epoch to True\u001b[0m\n",
      "\u001b[34m[NeMo I\u001b[0m\n",
      "\u001b[34m2023-12-09 17:3\u001b[0m\n",
      "\u001b[34m8:38 gpt_dataset:598]  > elasped tim\u001b[0m\n",
      "\u001b[34me to build and save doc-idx mapping\u001b[0m\n",
      "\u001b[34m(seconds): 0.000430\u001b[0m\n",
      "\u001b[34mmake: Ent\u001b[0m\n",
      "\u001b[34mering directory\u001b[0m\n",
      "\u001b[34m'/usr/local/lib/python3.10/site-pa\u001b[0m\n",
      "\u001b[34mckages/nemo/collections/nlp/data/lang\u001b[0m\n",
      "\u001b[34muage_modeling/megatron'\u001b[0m\n",
      "\u001b[34mg++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/local/include/python3.\u001b[0m\n",
      "\u001b[34m10 -I/usr/local/lib/python3.10/site-packages/pybind11/include helpers.cpp\u001b[0m\n",
      "\u001b[34m-o helpers.cpython-310-x86_64-linux-gnu.so\u001b[0m\n",
      "\u001b[34mmake: Leaving directory '/usr/local/lib/python3.10/site-packages/nemo/collections/nlp/data/language\u001b[0m\n",
      "\u001b[34m_modeling/megatron'\u001b[0m\n",
      "\u001b[34musing:\u001b[0m\n",
      "\u001b[34mnumber of doc\u001b[0m\n",
      "\u001b[34muments:       1355\n",
      "     number of epoch\u001b[0m\n",
      "\u001b[34ms:          3\n",
      "     sequence length:\u001b[0m\n",
      "\u001b[34m2048\n",
      "     total number of samples:   932\u001b[0m\n",
      "\u001b[34m[NeMo I\u001b[0m\n",
      "\u001b[34m2023-12-09 17:38\u001b[0m\n",
      "\u001b[34m:43 gpt_dataset:624]  > elasped time\u001b[0m\n",
      "\u001b[34mto build and save sample-idx mapping\u001b[0m\n",
      "\u001b[34m(seconds): 4.805221\n",
      " > building shuff\u001b[0m\n",
      "\u001b[34mle index with split [0, 621) and [621,\u001b[0m\n",
      "\u001b[34m932) ...\u001b[0m\n",
      "\u001b[34m[NeMo I 2\u001b[0m\n",
      "\u001b[34m023-12-09 17:38\u001b[0m\n",
      "\u001b[34m:43 gpt_dataset:638]  > elasped time\u001b[0m\n",
      "\u001b[34mto build and save shuffle-idx mappin\u001b[0m\n",
      "\u001b[34mg (seconds): 0.000267\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:43\u001b[0m\n",
      "\u001b[34mgpt_dataset:660]  > loading doc-idx\u001b[0m\n",
      "\u001b[34mmapping from /opt/ml/code/tmp/tokenized_data_text_document_train_indexmap_772ns\u001b[0m\n",
      "\u001b[34m_2048sl_1234s_doc_idx.npy\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:43 gpt_\u001b[0m\n",
      "\u001b[34mdataset:662]  > loading sample-idx\u001b[0m\n",
      "\u001b[34mmapping from /opt/ml/code/tmp/tokenized_data_text_docume\u001b[0m\n",
      "\u001b[34mnt_train_indexmap_772ns_2048\u001b[0m\n",
      "\u001b[34msl_1234s_sample_idx.npy\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-\u001b[0m\n",
      "\u001b[34m12-09 17:38:43 gp\u001b[0m\n",
      "\u001b[34mt_dataset:664]  > loading shuffle-idx\u001b[0m\n",
      "\u001b[34mmapping from /opt/ml/code/tmp/tokeniz\u001b[0m\n",
      "\u001b[34med_data_text_document_train_indexmap_772\u001b[0m\n",
      "\u001b[34mns_2048sl_1234s_shuffle_idx.npy\u001b[0m\n",
      "\u001b[34m[NeMo I 2\u001b[0m\n",
      "\u001b[34m023-12-09 17:\u001b[0m\n",
      "\u001b[34m38:43 gpt_dataset:666]     loaded\u001b[0m\n",
      "\u001b[34mindexed file in 0.002 seconds\u001b[0m\n",
      "\u001b[34m[N\u001b[0m\n",
      "\u001b[34meMo I 2023-12-09 17:38:43 gpt_data\u001b[0m\n",
      "\u001b[34mset:667]     total number of samples: 933\u001b[0m\n",
      "\u001b[34m[NeMo\u001b[0m\n",
      "\u001b[34mI 2023-12-09 17:38:43 gpt_datase\u001b[0m\n",
      "\u001b[34mt:668]     total number of epochs: 3\u001b[0m\n",
      "\u001b[34mmake: Entering directory '/usr/local\u001b[0m\n",
      "\u001b[34m/lib/python3.10/site-packages/nemo/colle\u001b[0m\n",
      "\u001b[34mctions/nlp/data/language_modeling/megatron'\u001b[0m\n",
      "\u001b[34mmake: Nothing to\u001b[0m\n",
      "\u001b[34mbe done for 'default'.\u001b[0m\n",
      "\u001b[34mmake: Leaving directory '/usr/local/lib/python\u001b[0m\n",
      "\u001b[34m3.10/site-packages/nemo/collections/nlp/da\u001b[0m\n",
      "\u001b[34mta/language_modeling/megatron'\u001b[0m\n",
      "\u001b[34m> building i\u001b[0m\n",
      "\u001b[34mndices for blend\u001b[0m\n",
      "\u001b[34mable datasets ...\n",
      " > sample ratios:\u001b[0m\n",
      "\u001b[34mdataset 0, input: 1, achieved: 1\u001b[0m\n",
      "\u001b[34m[N\u001b[0m\n",
      "\u001b[34meMo I 2023-12-09 17:38:43 blendable_dataset:67] > elap\u001b[0m\n",
      "\u001b[34msed time for building blendable dataset indices: 0.03\u001b[0m\n",
      "\u001b[34m(sec)\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:4\u001b[0m\n",
      "\u001b[34m3 megatron_gpt_model:877] Length of tr\u001b[0m\n",
      "\u001b[34main dataset: 772\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:43 megatro\u001b[0m\n",
      "\u001b[34mn_gpt_model:882] Finished building GPT\u001b[0m\n",
      "\u001b[34mdatasets.\u001b[0m\n",
      "\u001b[34m[NeMo I 202\u001b[0m\n",
      "\u001b[34m3-12-09 17:38:4\u001b[0m\n",
      "\u001b[34m3 megatron_gpt_model:1018] Setting\u001b[0m\n",
      "\u001b[34mup train dataloader with len(len(self\u001b[0m\n",
      "\u001b[34m._train_ds)): 772 and consumed sample\u001b[0m\n",
      "\u001b[34ms: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:38:43 meg\u001b[0m\n",
      "\u001b[34matron_gpt_model:891] Building dataloa\u001b[0m\n",
      "\u001b[34mder with consumed samples: 0\u001b[0m\n",
      "\u001b[34mRestoring states fr\u001b[0m\n",
      "\u001b[34mom the checkpoin\u001b[0m\n",
      "\u001b[34mt path at /opt/ml/code/tmp/nemo_ch\u001b[0m\n",
      "\u001b[34meckpoint/mp_rank_07/model\u001b[0m\n",
      "\u001b[34m_optim_rng.ckpt\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:39:04 modelPT:614] Optimizer config = AdamW (\n",
      "    Parameter\u001b[0m\n",
      "\u001b[34mGroup 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.95]\n",
      "        captu\u001b[0m\n",
      "\u001b[34mrable: True\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        lr: 0.0001\n",
      "        maximize: False\n",
      "        weight_decay: 0.1\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.95]\n",
      "        capturable: True\u001b[0m\n",
      "\u001b[34meps: 1e-08\n",
      "        foreach: None\n",
      "        lr: 0.0001\n",
      "        maximize: False\n",
      "        weight_decay: 0.0\n",
      "    )\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-\u001b[0m\n",
      "\u001b[34m12-09 17:39:04 lr_\u001b[0m\n",
      "\u001b[34mscheduler:910] Scheduler \"<nemo.core.optim.\u001b[0m\n",
      "\u001b[34mlr_scheduler.CosineAnnealing object at 0x\u001b[0m\n",
      "\u001b[34m7fd3beb5ad70>\" \n",
      "    will be used during\u001b[0m\n",
      "\u001b[34mtraining (effective maximum steps = 3) -\u001b[0m\n",
      "\u001b[34mParameters : \n",
      "    (warmup_steps: 10\n",
      "    constant\u001b[0m\n",
      "\u001b[34m_steps: 0\n",
      "    min_lr: 1.0e-06\n",
      "    max_s\u001b[0m\n",
      "\u001b[34mteps: 3\n",
      "    )\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-\u001b[0m\n",
      "\u001b[34m12-09 17:39:04 lr\u001b[0m\n",
      "\u001b[34m_scheduler:910] Scheduler \"<nemo.core.opt\u001b[0m\n",
      "\u001b[34mim.lr_scheduler.CosineAnnealing object\u001b[0m\n",
      "\u001b[34mat 0x7fd3bebec970>\" \n",
      "    will be used du\u001b[0m\n",
      "\u001b[34mring training (effective maximum steps\u001b[0m\n",
      "\u001b[34m= 3) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 10\u001b[0m\n",
      "\u001b[34mconstant_steps: 0\n",
      "    min_lr: 1.0e-06\u001b[0m\n",
      "\u001b[34mmax_steps: 3\n",
      "    )\u001b[0m\n",
      "\u001b[34mRestored all\u001b[0m\n",
      "\u001b[34mstates from the\u001b[0m\n",
      "\u001b[34mcheckpoint file at /opt/ml/code/tmp\u001b[0m\n",
      "\u001b[34m/nemo_checkpoint/mp_rank_07/model_opti\u001b[0m\n",
      "\u001b[34mm_rng.ckpt\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-\u001b[0m\n",
      "\u001b[34m09 17:39:04 nemo_logg\u001b[0m\n",
      "\u001b[34ming:349] /usr/local/lib/python3.10/site-packages/p\u001b[0m\n",
      "\u001b[34mytorch_lightning/trainer/connectors/data_\u001b[0m\n",
      "\u001b[34mconnector.py:208: UserWarning: num_workers\u001b[0m\n",
      "\u001b[34m>0, persistent_workers=False, and strategy=\u001b[0m\n",
      "\u001b[34mddp_spawn may result in data loading bo\u001b[0m\n",
      "\u001b[34mttlenecks. Consider setting persistent_w\u001b[0m\n",
      "\u001b[34morkers=True (this is a limitation of Py\u001b[0m\n",
      "\u001b[34mthon .spawn() and PyTorch)\n",
      "      rank_z\u001b[0m\n",
      "\u001b[34mero_warn(\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:04.000212:  69683  INFO ||NEURON_CACHE||: Compile cache path: /tmp/P\u001b[0m\n",
      "\u001b[34mRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09\u001b[0m\n",
      "\u001b[34m17:39:04.000213:\u001b[0m\n",
      "\u001b[34m69683  INFO ||NEURON_CC_WRAPPER||: Using\u001b[0m\n",
      "\u001b[34ma cached neff at /tmp/PRE_COMPILED_NEURON\u001b[0m\n",
      "\u001b[34m_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c\u001b[0m\n",
      "\u001b[34m8c/MODULE_310851798465585165+f9c53d58/\u001b[0m\n",
      "\u001b[34mmodel.neff. Exiting with a successfull\u001b[0m\n",
      "\u001b[34my compiled graph.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:39:04 nemo_logging:349] /\u001b[0m\n",
      "\u001b[34musr/local/lib/python3.10/site-packages/py\u001b[0m\n",
      "\u001b[34mtorch_lightning/trainer/connectors/data_connector.py:366:\u001b[0m\n",
      "\u001b[34mUserWarning: One of given\u001b[0m\n",
      "\u001b[34mdataloaders is None an\u001b[0m\n",
      "\u001b[34md it will be skipped\u001b[0m\n",
      "\u001b[34m.\n",
      "      rank_zero_wa\u001b[0m\n",
      "\u001b[34mrn(\"One of given datal\u001b[0m\n",
      "\u001b[34moaders is None and it will be s\u001b[0m\n",
      "\u001b[34mkipped.\")\u001b[0m\n",
      "\u001b[34mTrainin\u001b[0m\n",
      "\u001b[34mg: 0it [00:0\u001b[0m\n",
      "\u001b[34m0, ?it/s]\u001b[0m\n",
      "\u001b[34mTrain\u001b[0m\n",
      "\u001b[34ming:   0%|\u001b[0m\n",
      "\u001b[34m| 0/3 [00:00<?,\u001b[0m\n",
      "\u001b[34m?it/s]#015Epoch 0:   0%|\u001b[0m\n",
      "\u001b[34m| 0/3 [00:00<?, ?it\u001b[0m\n",
      "\u001b[34m/s]\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:04.000793:  72160  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_\u001b[0m\n",
      "\u001b[34mCOMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:\u001b[0m\n",
      "\u001b[34m39:04.000802:  7216\u001b[0m\n",
      "\u001b[34m0  INFO ||NEURON_CC_WRAPPER||: Using a cac\u001b[0m\n",
      "\u001b[34mhed neff at /tmp/PRE_COMPILED_NEURON_GRA\u001b[0m\n",
      "\u001b[34mPH_TRAIN/neuronxcc\u001b[0m\n",
      "\u001b[34m-2.10.0.35+3817a0c8c/MODULE_1455418593\u001b[0m\n",
      "\u001b[34m0996491893+f9c53d58/model.neff. Exitin\u001b[0m\n",
      "\u001b[34mg with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:06.000537:  76914  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILE\u001b[0m\n",
      "\u001b[34mD_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:\u001b[0m\n",
      "\u001b[34m06.000579:  76914  IN\u001b[0m\n",
      "\u001b[34mFO ||NEURON_CC_WRAPPER||: Using a cached neff at\u001b[0m\n",
      "\u001b[34m/tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/n\u001b[0m\n",
      "\u001b[34meuronxcc-2.10.0.35+3817a0c8c/MODULE_16192\u001b[0m\n",
      "\u001b[34m82520863546690+f9c53d58/model.neff. Ex\u001b[0m\n",
      "\u001b[34miting with a successfully compiled grap\u001b[0m\n",
      "\u001b[34mh.\u001b[0m\n",
      "\u001b[34m20\u001b[0m\n",
      "\u001b[34m23-12-09 17:39:0\u001b[0m\n",
      "\u001b[34m6.000591:  76916  INFO ||NEURON_CACHE||\u001b[0m\n",
      "\u001b[34m: Compile cache path: /tmp/PRE_COMPI\u001b[0m\n",
      "\u001b[34mLED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m202\u001b[0m\n",
      "\u001b[34m3-12-09 17:39:06\u001b[0m\n",
      "\u001b[34m.000610:  76918  INFO ||NEURON_CACHE||:\u001b[0m\n",
      "\u001b[34mCompile cache path: /tmp/PRE_COMPILE\u001b[0m\n",
      "\u001b[34mD_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-\u001b[0m\n",
      "\u001b[34m12-09 17:39:06.00\u001b[0m\n",
      "\u001b[34m0616:  76920  INFO ||NEURON_CACHE|\u001b[0m\n",
      "\u001b[34m|: Compile cache path: /tmp/PRE_COMPILE\u001b[0m\n",
      "\u001b[34mD_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09\u001b[0m\n",
      "\u001b[34m17:39:06.000627\u001b[0m\n",
      "\u001b[34m:  76922  INFO ||NEURON_CACHE||: C\u001b[0m\n",
      "\u001b[34mompile cache path: /tmp/PRE_COMPILED_\u001b[0m\n",
      "\u001b[34mNEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 1\u001b[0m\n",
      "\u001b[34m7:39:06.000635:\u001b[0m\n",
      "\u001b[34m76924  INFO ||NEURON_CACHE||: Com\u001b[0m\n",
      "\u001b[34mpile cache path: /tmp/PRE_COMPILED_NEUR\u001b[0m\n",
      "\u001b[34mON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09\u001b[0m\n",
      "\u001b[34m17:39:06.000638\u001b[0m\n",
      "\u001b[34m:  76928  INFO ||NEURON_CACHE||:\u001b[0m\n",
      "\u001b[34mCompile cache path: /tmp/PRE_COMPILED\u001b[0m\n",
      "\u001b[34m_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09\u001b[0m\n",
      "\u001b[34m17:39:06.000639\u001b[0m\n",
      "\u001b[34m:  76927  INFO ||NEURON_CACHE||:\u001b[0m\n",
      "\u001b[34mCompile cache path: /tmp/PRE_COMPIL\u001b[0m\n",
      "\u001b[34mED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 1\u001b[0m\n",
      "\u001b[34m7:39:06.000753:  76916\u001b[0m\n",
      "\u001b[34mINFO ||NEURON_CC_WRAPPER||: Using a cache\u001b[0m\n",
      "\u001b[34md neff at /tmp/PRE_COMPILED_NEU\u001b[0m\n",
      "\u001b[34mRON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3\u001b[0m\n",
      "\u001b[34m817a0c8c/MODULE_10045816615887\u001b[0m\n",
      "\u001b[34m760514+f9c53d58/model.neff. Exiting with\u001b[0m\n",
      "\u001b[34ma successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:3\u001b[0m\n",
      "\u001b[34m9:06.000867:  76918  I\u001b[0m\n",
      "\u001b[34mNFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tm\u001b[0m\n",
      "\u001b[34mp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuron\u001b[0m\n",
      "\u001b[34mxcc-2.10.0.35+3817a0c8c/MODULE_12598746\u001b[0m\n",
      "\u001b[34m364335896506+f9c53d58/model.neff. Exiti\u001b[0m\n",
      "\u001b[34mng with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:06.000867:  76920\u001b[0m\n",
      "\u001b[34mINFO ||NEURON_CC_WRAPPER||: Using a\u001b[0m\n",
      "\u001b[34mcached neff at /tmp/PRE_COMPILED_NEURO\u001b[0m\n",
      "\u001b[34mN_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817\u001b[0m\n",
      "\u001b[34ma0c8c/MODULE_10441317650604297217+f9c\u001b[0m\n",
      "\u001b[34m53d58/model.neff. Exiting with a succ\u001b[0m\n",
      "\u001b[34messfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09\u001b[0m\n",
      "\u001b[34m17:39:06.000867:  76922  INFO ||NEURON\u001b[0m\n",
      "\u001b[34m_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMP\u001b[0m\n",
      "\u001b[34mILED_NEURON_GRAPH_TRAIN/neuronxcc-2.1\u001b[0m\n",
      "\u001b[34m0.0.35+3817a0c8c/MODULE_360088867487222\u001b[0m\n",
      "\u001b[34m4049+f9c53d58/model.neff. Exiting wit\u001b[0m\n",
      "\u001b[34mh a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09\u001b[0m\n",
      "\u001b[34m17:39:06.000873:\u001b[0m\n",
      "\u001b[34m76924  INFO ||NEURON_CC_WRAPPER||: Using\u001b[0m\n",
      "\u001b[34ma cached neff at /tmp/PRE_COMPI\u001b[0m\n",
      "\u001b[34mLED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0\u001b[0m\n",
      "\u001b[34m.35+3817a0c8c/MODULE_525051\u001b[0m\n",
      "\u001b[34m2035884166464+f9c53d58/model.neff. Exi\u001b[0m\n",
      "\u001b[34mting with a successfully compiled\u001b[0m\n",
      "\u001b[34mgraph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:\u001b[0m\n",
      "\u001b[34m06.000873:  76928  INFO ||NEURON_CC_W\u001b[0m\n",
      "\u001b[34mRAPPER||: Using a cached neff at /tmp/PRE_C\u001b[0m\n",
      "\u001b[34mOMPILED_NEURON_GRAPH_TRAIN/neuro\u001b[0m\n",
      "\u001b[34mnxcc-2.10.0.35+3817a0c8c/MODULE_55307\u001b[0m\n",
      "\u001b[34m81884043658869+f9c53d58/model.nef\u001b[0m\n",
      "\u001b[34mf. Exiting with a successfull\u001b[0m\n",
      "\u001b[34my compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:06\u001b[0m\n",
      "\u001b[34m.000873:  76927  INFO ||NEURON_CC\u001b[0m\n",
      "\u001b[34m_WRAPPER||: Using a cached neff at /tmp/PRE_COM\u001b[0m\n",
      "\u001b[34mPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.\u001b[0m\n",
      "\u001b[34m10.0.35+3817a0c8c/MODULE_54933\u001b[0m\n",
      "\u001b[34m23624958044841+f9c53d58/model.neff. Exit\u001b[0m\n",
      "\u001b[34ming with a successfully compile\u001b[0m\n",
      "\u001b[34md graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:08.000908:  77254  INFO ||NEURON_CACHE||: Compile cache path: /tmp/\u001b[0m\n",
      "\u001b[34mPRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:08.\u001b[0m\n",
      "\u001b[34m000973:  77254  INFO |\u001b[0m\n",
      "\u001b[34m|NEURON_CC_WRAPPER||: Using a cached neff\u001b[0m\n",
      "\u001b[34mat /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neu\u001b[0m\n",
      "\u001b[34mronxcc-2.10.0.35+3817a0c8c/MODULE_1781972121099695\u001b[0m\n",
      "\u001b[34m8436+f9c53d58/model.neff. Exiting wit\u001b[0m\n",
      "\u001b[34mh a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:12.000201:  77894  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE\u001b[0m\n",
      "\u001b[34m_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:\u001b[0m\n",
      "\u001b[34m12.000270:  77894  IN\u001b[0m\n",
      "\u001b[34mFO ||NEURON_CC_WRAPPER||: Using a cached n\u001b[0m\n",
      "\u001b[34meff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRA\u001b[0m\n",
      "\u001b[34mIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_9946\u001b[0m\n",
      "\u001b[34m645543341646997+f9c53d58/model.neff. Exiting\u001b[0m\n",
      "\u001b[34mwith a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:28.000215:  24177  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON\u001b[0m\n",
      "\u001b[34m_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:28.000217:  24177  INFO ||NEURON_CC_WRAPPER\u001b[0m\n",
      "\u001b[34m||: Using a cached neff at /tmp/PRE_COMPILED_NEU\u001b[0m\n",
      "\u001b[34mRON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_14947573939663078061+f9\u001b[0m\n",
      "\u001b[34mc53d58/model.neff. Exiting with a successfu\u001b[0m\n",
      "\u001b[34mlly compiled graph\u001b[0m\n",
      "\u001b[34m.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:28.000595:  25425  INFO ||NEURON_CACHE||: Compil\u001b[0m\n",
      "\u001b[34me cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:28.000609:  25425  INFO ||N\u001b[0m\n",
      "\u001b[34mEURON_CC_WRAPPER||: Using a cached neff at /tmp\u001b[0m\n",
      "\u001b[34m/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_140008778\u001b[0m\n",
      "\u001b[34m48722700329+f9c53d58/model.neff. Exiting wit\u001b[0m\n",
      "\u001b[34mh a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:29.000157:  28468  INFO ||NEURON_CACHE||: Comp\u001b[0m\n",
      "\u001b[34mile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-0\u001b[0m\n",
      "\u001b[34m9 17:39:29.000181:\u001b[0m\n",
      "\u001b[34m28468  INFO ||NEURON_CC_WRAPPER||: Using a\u001b[0m\n",
      "\u001b[34mcached neff at /tmp/PRE_COMPILED_NEURON_\u001b[0m\n",
      "\u001b[34mGRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8\u001b[0m\n",
      "\u001b[34mc/MODULE_5837248630630320423+f9c53d58/mo\u001b[0m\n",
      "\u001b[34mdel.neff. Exiting with a successfully c\u001b[0m\n",
      "\u001b[34mompiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:29.000312:  28911  INFO ||\u001b[0m\n",
      "\u001b[34mNEURON_CACHE||: Compile cache path: /tmp/PRE\u001b[0m\n",
      "\u001b[34m_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:29.00\u001b[0m\n",
      "\u001b[34m0338:  28911  INFO ||NE\u001b[0m\n",
      "\u001b[34mURON_CC_WRAPPER||: Using a cached neff at /tmp/\u001b[0m\n",
      "\u001b[34mPRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.\u001b[0m\n",
      "\u001b[34m10.0.35+3817a0c8c/MODULE_61428339565062137\u001b[0m\n",
      "\u001b[34m88+f9c53d58/model.neff. Exiting with a su\u001b[0m\n",
      "\u001b[34mccessfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:29.000540:  29208  INFO ||NEURON_CACHE||: C\u001b[0m\n",
      "\u001b[34mompile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17\u001b[0m\n",
      "\u001b[34m:39:29.000542:  29\u001b[0m\n",
      "\u001b[34m208  INFO ||NEURON_CC_WRAPPER||: Using a c\u001b[0m\n",
      "\u001b[34mached neff at /tmp/PRE_COMPILED_NEURON_GR\u001b[0m\n",
      "\u001b[34mAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/\u001b[0m\n",
      "\u001b[34mMODULE_16178973479619687874+f9c53d58/mod\u001b[0m\n",
      "\u001b[34mel.neff. Exiting with a successfully co\u001b[0m\n",
      "\u001b[34mmpiled graph.\u001b[0m\n",
      "\u001b[34m2023-12\u001b[0m\n",
      "\u001b[34m-09 17:39:29.0005\u001b[0m\n",
      "\u001b[34m57:  29320  INFO ||NEURON_CACHE||: Comp\u001b[0m\n",
      "\u001b[34mile cache path: /tmp/PRE_COMPILED_NEURON_GR\u001b[0m\n",
      "\u001b[34mAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:3\u001b[0m\n",
      "\u001b[34m9:29.000558:  29320\u001b[0m\n",
      "\u001b[34mINFO ||NEURON_CC_WRAPPER||: Using a cached nef\u001b[0m\n",
      "\u001b[34mf at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m/neuronxcc-2.10.0.35+3817a0c8c/MODULE_18\u001b[0m\n",
      "\u001b[34m180170502840997799+f9c53d58/model.neff.\u001b[0m\n",
      "\u001b[34mExiting with a successfully compiled\u001b[0m\n",
      "\u001b[34mgraph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:29.000747:  29748  INFO ||NEURON_CACHE||\u001b[0m\n",
      "\u001b[34m: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_T\u001b[0m\n",
      "\u001b[34mRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09\u001b[0m\n",
      "\u001b[34m17:39:29.000748:\u001b[0m\n",
      "\u001b[34m29748  INFO ||NEURON_CC_WRAPPER||: Usi\u001b[0m\n",
      "\u001b[34mng a cached neff at /tmp/PRE_COMPILED_N\u001b[0m\n",
      "\u001b[34mEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+38\u001b[0m\n",
      "\u001b[34m17a0c8c/MODULE_349647913097389165+f9c53\u001b[0m\n",
      "\u001b[34md58/model.neff. Exiting with a successf\u001b[0m\n",
      "\u001b[34mully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:29.000774:  29920  INFO ||NEU\u001b[0m\n",
      "\u001b[34mRON_CACHE||: Compile cache path: /tmp/PRE_CO\u001b[0m\n",
      "\u001b[34mMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-1\u001b[0m\n",
      "\u001b[34m2-09 17:39:\u001b[0m\n",
      "\u001b[34m29.000776:  29920  INFO ||\u001b[0m\n",
      "\u001b[34mNEURON_CC_WRAPPER||: Using\u001b[0m\n",
      "\u001b[34ma cached neff at /tmp/PR\u001b[0m\n",
      "\u001b[34mE_COMPILED_NEURON_GRAPH_T\u001b[0m\n",
      "\u001b[34mRAIN/neuronxcc-2.10.0.35\u001b[0m\n",
      "\u001b[34m+3817a0c8c/MODULE_400514\u001b[0m\n",
      "\u001b[34m7696708003337+f9c53d58/mod\u001b[0m\n",
      "\u001b[34mel.neff. Exiting with a successfully c\u001b[0m\n",
      "\u001b[34mompiled graph.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:39:29 nemo_logging:349\u001b[0m\n",
      "\u001b[34m] /usr/local/lib/python3.10/site-pack\u001b[0m\n",
      "\u001b[34mages/pytorch_lightning/trainer/connectors/logger_connector/result\u001b[0m\n",
      "\u001b[34m.py:233: UserWarning: Yo\u001b[0m\n",
      "\u001b[34mu called `self.log('thr\u001b[0m\n",
      "\u001b[34moughput_peak', ...)`\u001b[0m\n",
      "\u001b[34min your `configure_grad\u001b[0m\n",
      "\u001b[34mient_clipping` but the value need\u001b[0m\n",
      "\u001b[34ms to be floating point. Converting it\u001b[0m\n",
      "\u001b[34mto torch.float32.\n",
      "      warning_cache\u001b[0m\n",
      "\u001b[34m.warn(\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:30.000339:  31853  INFO ||NEURON_CACHE||: Compile cache path: /tmp/\u001b[0m\n",
      "\u001b[34mPRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:\u001b[0m\n",
      "\u001b[34m30.000357:  31855  I\u001b[0m\n",
      "\u001b[34mNFO ||NEURON_CACHE||: Compile cache path: /tm\u001b[0m\n",
      "\u001b[34mp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:3\u001b[0m\n",
      "\u001b[34m9:30.000378:  318\u001b[0m\n",
      "\u001b[34m53  INFO ||NEURON_CC_WRAPPER||: Using a c\u001b[0m\n",
      "\u001b[34mached neff at /tmp/PRE_COMPILED_NEURON_\u001b[0m\n",
      "\u001b[34mGRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c\u001b[0m\n",
      "\u001b[34m8c/MODULE_5521626181480929843+f9c53d58\u001b[0m\n",
      "\u001b[34m/model.neff. Exiting with a successfu\u001b[0m\n",
      "\u001b[34mlly compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39\u001b[0m\n",
      "\u001b[34m:30.000378:  31855  INFO ||NEURON_CC_WR\u001b[0m\n",
      "\u001b[34mAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NE\u001b[0m\n",
      "\u001b[34mURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+\u001b[0m\n",
      "\u001b[34m3817a0c8c/MODULE_12384099603961419910+\u001b[0m\n",
      "\u001b[34mf9c53d58/model.neff. Exiting with a s\u001b[0m\n",
      "\u001b[34muccessfully compiled graph.\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 1/3 [00:26<00:53, 26.60s/it]\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 1/3\u001b[0m\n",
      "\u001b[34m[00:26<00:53, 26.60s/it, loss=nan, reduce\u001b[0m\n",
      "\u001b[34md_train_loss=0.000, global_step=0.000, consumed_samples=0.000, iteration_time=0.\u001b[0m\n",
      "\u001b[34m000]\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:32.000382:  33177  INFO ||NEURON_CACHE||: Compile cache path: /tmp/\u001b[0m\n",
      "\u001b[34mPRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:32.0\u001b[0m\n",
      "\u001b[34m00450:  33177  INFO ||NE\u001b[0m\n",
      "\u001b[34mURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_CO\u001b[0m\n",
      "\u001b[34mMPILED_NEURON_GRAPH_TRAIN/\u001b[0m\n",
      "\u001b[34mneuronxcc-2.10.0.35+3817a0c8c/MODULE_6143121921458704412+f9c5\u001b[0m\n",
      "\u001b[34m3d58/model.neff.\u001b[0m\n",
      "\u001b[34mExiting with a successfully compiled gr\u001b[0m\n",
      "\u001b[34maph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:49.000019:  100419  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_C\u001b[0m\n",
      "\u001b[34mOMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09\u001b[0m\n",
      "\u001b[34m17:39:49.000020:\u001b[0m\n",
      "\u001b[34m100419  INFO ||NEURON_CC_WRAPPER||: Using\u001b[0m\n",
      "\u001b[34ma cached neff at /tmp/PRE_COMPILED_NEUR\u001b[0m\n",
      "\u001b[34mON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0\u001b[0m\n",
      "\u001b[34mc8c/MODULE_4658971354919886479+f9c53d5\u001b[0m\n",
      "\u001b[34m8/model.neff. Exiting with a successful\u001b[0m\n",
      "\u001b[34mly compiled graph.\u001b[0m\n",
      "\u001b[34mEpoch 0:  67%|██████▋   | 2/3 [00:46<00:23, 23.16s/it, loss=nan, reduced_train_loss=0.0\u001b[0m\n",
      "\u001b[34m00, global_step=0.000, consumed_samples=0.000, iteration_time=0.000]\u001b[0m\n",
      "\u001b[34mEpoch 0:  67%|█�\u001b[0m\n",
      "\u001b[34m��████▋   | 2/3 [0\u001b[0m\n",
      "\u001b[34m0:46<00:23, 23.16s/it, loss=0, reduced_train_loss=0.000, global_step=0.000, cons\u001b[0m\n",
      "\u001b[34mumed_samples=0.000, iteration_time=0.000]\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:51.000114:  109620  INFO ||NEURON_CACHE||: Compile cache path: /tmp\u001b[0m\n",
      "\u001b[34m/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:39:51.000221:  1096\u001b[0m\n",
      "\u001b[34m20  INFO ||NEURON_CC_WRAPPER|\u001b[0m\n",
      "\u001b[34m|: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neur\u001b[0m\n",
      "\u001b[34monxcc-2.10.0.35+3817a0c8c/MODULE_2812950311\u001b[0m\n",
      "\u001b[34m548070509+f9c53d58/model.neff. Exiting wit\u001b[0m\n",
      "\u001b[34mh a successfully compiled graph.\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████\u001b[0m\n",
      "\u001b[34m| 3/3 [01:05<00:00, 21.78s/it, loss=0, red\u001b[0m\n",
      "\u001b[34muced_train_loss=0.000, global_step=0.000, consumed_samples=0.000, ite\u001b[0m\n",
      "\u001b[34mration_time=0.000]\u001b[0m\n",
      "\u001b[34mEpoch\u001b[0m\n",
      "\u001b[34m0: 100%|█\u001b[0m\n",
      "\u001b[34m█████████| 3\u001b[0m\n",
      "\u001b[34m/3 [01:05<00:00, 21.78s/it, l\u001b[0m\n",
      "\u001b[34moss=0, reduced_train_loss=0.000,\u001b[0m\n",
      "\u001b[34mglobal_step=0.000, consumed_s\u001b[0m\n",
      "\u001b[34mamples=0.000, iteration_\u001b[0m\n",
      "\u001b[34mtime=0.000]\u001b[0m\n",
      "\u001b[34mEp\u001b[0m\n",
      "\u001b[34moch 0: 100\u001b[0m\n",
      "\u001b[34m%|███████\u001b[0m\n",
      "\u001b[34m███| 3/3 [01:05<00\u001b[0m\n",
      "\u001b[34m:00, 21.78s/it, loss=0,\u001b[0m\n",
      "\u001b[34mreduced_train_loss=0.00\u001b[0m\n",
      "\u001b[34m0, global_step=0.000, c\u001b[0m\n",
      "\u001b[34monsumed_samples=0.000,\u001b[0m\n",
      "\u001b[34miteration_time=0.000]\u001b[0m\n",
      "\u001b[34m`Tr\u001b[0m\n",
      "\u001b[34mainer.fit\u001b[0m\n",
      "\u001b[34m` stopped: `max_steps=3\u001b[0m\n",
      "\u001b[34m` reached.\u001b[0m\n",
      "\u001b[34mEpo\u001b[0m\n",
      "\u001b[34mch 0: 10\u001b[0m\n",
      "\u001b[34m0%|██████\u001b[0m\n",
      "\u001b[34m████| 3/3 [01:\u001b[0m\n",
      "\u001b[34m05<00:00, 21.78s/it, l\u001b[0m\n",
      "\u001b[34moss=0, reduced_train_lo\u001b[0m\n",
      "\u001b[34mss=0.000, global_step=0\u001b[0m\n",
      "\u001b[34m.000, consumed_samples\u001b[0m\n",
      "\u001b[34m=0.000, iteration_time\u001b[0m\n",
      "\u001b[34m=0.000]\u001b[0m\n",
      "\u001b[34m2023-12-09 17:40:18.000696:  739  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:40:22.000266:  739  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 0, failed are 0, done are 94, total is 94\u001b[0m\n",
      "\u001b[34m2023-12-09 17:40:22.000266:  739  INFO ||NEURON_PARALLEL_COMPILE||: {\n",
      "    \"compilation_summary\": {},\n",
      "    \"compilation_report\": {}\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2023-12-09 17:40:22.000266:  739  INFO ||NEURON_PARALLEL_COMPILE||: Total graphs: 0\u001b[0m\n",
      "\u001b[34m2023-12-09 17:40:22.000266:  739  INFO ||NEURON_PARALLEL_COMPILE||: Total successful compilations: 0\u001b[0m\n",
      "\u001b[34m2023-12-09 17:40:22.000266:  739  INFO ||NEURON_PARALLEL_COMPILE||: Total failed compilations: 0\u001b[0m\n",
      "\u001b[34m./fine_tuning_trn1.sh: line 6: sudo: command not found\u001b[0m\n",
      "\u001b[34m--nproc_per_node 32 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 41000\u001b[0m\n",
      "\u001b[34mSEQ_LEN=2048, HS=4096, FFN_HS=11008 TP=8 PP=1 N_LAYERS=32 N_AH=32 GBS=256 UBS=1 MIN_LR=1e-06\u001b[0m\n",
      "\u001b[34mINIT_METHOD_STD=, HIDDEN_DROPOUT=, LAYERNORM_EPSILON=1e-05, OPTIM_NAME=, OPTIM_LR=0.0001\u001b[0m\n",
      "\u001b[34mOPTIM_WEIGHT_DECAY=0.1, OPTIM_SCHED_NAME=CosineAnnealing\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:26 optimizers:67] Could not import distributed_fused_adam optimizer from Apex\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:27 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:27 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7ff33aaf0bb0>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 19 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:28 nemo_logging:349] /usr/local/lib/python3.10/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_gpt_pretraining:58] \n",
      "    \n",
      "    ************** Experiment configuration ***********\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fc7f5633e80>\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_gpt_pretraining:59] \n",
      "    name: megatron_llama\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 32\n",
      "      num_nodes: 1\n",
      "      accelerator: tpu\n",
      "      precision: 32\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: null\n",
      "      max_steps: 25\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 0.99\n",
      "      limit_val_batches: 1\n",
      "      limit_test_batches: 1\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "      enable_model_summary: false\n",
      "    exp_manager:\n",
      "      create_tensorboard_logger: true\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: /tmp\n",
      "      name: megatron_llama\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: false\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: step\n",
      "        save_top_k: -1\n",
      "        mode: max\n",
      "        save_last: true\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: megatron_llama--{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "        train_time_interval: 36000\n",
      "    model:\n",
      "      micro_batch_size: 1\n",
      "      global_batch_size: 256\n",
      "      tensor_model_parallel_size: 8\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 2048\n",
      "      max_position_embeddings: 2048\n",
      "      num_layers: 32\n",
      "      hidden_size: 4096\n",
      "      ffn_hidden_size: 11008\n",
      "      num_attention_heads: 32\n",
      "      init_method_std: 0.021\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0\n",
      "      attention_dropout: 0\n",
      "      ffn_dropout: 0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      normalization: rmsnorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 8\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      share_embeddings_and_output_weights: false\n",
      "      position_embedding_type: rope\n",
      "      rotary_percentage: 1\n",
      "      activation: swiglu\n",
      "      transformer_block_type: pre_ln\n",
      "      has_bias: false\n",
      "      tokenizer:\n",
      "        library: huggingface\n",
      "        type: /opt/ml/additonals3data/tokenizer\n",
      "        model: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        delimiter: null\n",
      "        sentencepiece_legacy: false\n",
      "        use_fast: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: true\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: false\n",
      "      gradient_accumulation_fusion: false\n",
      "      bias_activation_fusion: false\n",
      "      bias_dropout_add_fusion: false\n",
      "      masked_softmax_fusion: false\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: /opt/ml/code/tmp/nemo_checkpoint/mp_rank_07/model_optim_rng.ckpt\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      log_parameter_norm: true\n",
      "      log_gradient_norm: true\n",
      "      activations_checkpoint_granularity: full\n",
      "      activations_checkpoint_method: uniform\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: true\n",
      "      wrap_with_zero: false\n",
      "      transformer_engine: false\n",
      "      fp8: false\n",
      "      fp8_e4m3: false\n",
      "      fp8_hybrid: false\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1\n",
      "      fp8_amax_compute_algo: most_recent\n",
      "      use_emha: false\n",
      "      data:\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - /opt/ml/code/tmp/tokenized_data_text_document\n",
      "        index_mapping_dir: null\n",
      "        data_impl: mmap\n",
      "        splits_string: 1000,0,0\n",
      "        seq_length: 2048\n",
      "        skip_warmup: true\n",
      "        num_workers: 1\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        validation_drop_last: true\n",
      "        no_seqlen_plus_one_input_tokens: false\n",
      "        pad_samples_to_global_batch_size: false\n",
      "      nsys_profile:\n",
      "        enabled: false\n",
      "        start_step: 10\n",
      "        end_step: 10\n",
      "        ranks:\n",
      "        - 0\n",
      "        gen_shape: false\n",
      "      optim:\n",
      "        name: adamw\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.1\n",
      "        capturable: true\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 10\n",
      "          constant_steps: 0\n",
      "          min_lr: 1.0e-06\n",
      "      save_xser: true\n",
      "      load_xser: true\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f228fcf7df0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7efe2c5b2a40>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f8dd79b0ee0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fbafa0ff2b0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fbc251805b0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f496ebf1d80>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f9da5996da0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f02c30d8a90>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 24 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 25 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 15 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 28 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 20 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 30 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 4 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7ff043defac0>\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 nlp_overrides:418] NLPTrainer: Initializing trainer with parameters: {'self': <nemo.collections.nlp.parts.nlp_overrides.NLPTrainer object at 0x7f78b7e58c70>, 'logger': False, 'enable_checkpointing': False, 'callbacks': None, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm': None, 'num_nodes': 1, 'num_processes': None, 'devices': 32, 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': 1, 'max_epochs': None, 'min_epochs': None, 'max_steps': 25, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': 1, 'limit_test_batches': 1, 'limit_predict_batches': None, 'val_check_interval': 0.99, 'log_every_n_steps': 1, 'accelerator': 'tpu', 'strategy': <nemo.collections.nlp.parts.nlp_overrides.NLPDDPStrategy object at 0x7f78b800a530>, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': False, 'num_sanity_val_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': False, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': [], 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'inference_mode': True}\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f35cc192e60>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f78b7f4b970>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f0be1b272b0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fd6ea6fef20>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 18 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 3 None 256 1 4\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f873a7f3100>\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fd82be018d0>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fd91712cd90>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f7c6607b370>\u001b[0m\n",
      "\u001b[34mGPU available: True (cuda), used: False\u001b[0m\n",
      "\u001b[34mTPU available: True, using: 32 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:28 nemo_logging:349] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "      rank_zero_warn(\u001b[0m\n",
      "\u001b[34m`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\u001b[0m\n",
      "\u001b[34m`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f414673b250>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fbbcc2a8610>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f3845353d60>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f0153e81b10>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fbe2f2bb220>\u001b[0m\n",
      "\u001b[34m[NeMo E 2023-12-09 17:40:28 exp_manager:465] You are running multi-gpu without ddp.Please note that this is not tested in NeMo and could result in errors.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fdf0968b610>\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 exp_manager:350] Experiments will be logged at /tmp/megatron_llama/2023-12-09_17-40-22\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 exp_manager:725] TensorboardLogger has been set up\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f017d4c0e80>\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:28 nemo_logging:349] /usr/local/lib/python3.10/site-packages/nemo/utils/exp_manager.py:1035: UserWarning: Detected custom epoch loop. Skipping no validation on restart support.\n",
      "      warnings.warn(\"Detected custom epoch loop. Skipping no validation on restart support.\", UserWarning)\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_gpt_pretraining:103] Resuming training from checkpoint: /opt/ml/code/tmp/nemo_checkpoint/mp_rank_07/model_optim_rng.ckpt\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:28 nemo_logging:349] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:55: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v2.0. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "      rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f5159a97610>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fc0074d1f60>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f335644eaa0>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 5 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 1 None 256 1 4\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7fea7e1cb040>\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 29 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 27 None 256 1 4\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:255] Rank 0 has data parallel group: [0, 8, 16, 24]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:258] All data parallel group ranks: [[0, 8, 16, 24], [1, 9, 17, 25], [2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:259] Ranks 0 has data parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:267] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:268] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:278] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:282] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:283] Rank 0 has tensor model parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:297] Rank 0 has pipeline model parallel group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:309] Rank 0 has embedding group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:315] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:316] Rank 0 has pipeline model parallel rank 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:317] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_init:318] Rank 0 has embedding rank: 0\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 0 None 256 1 4\u001b[0m\n",
      "\u001b[34m23-12-09 17:40:28 - PID:77984 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 64\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 16 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /opt/ml/additonals3data/tokenizer\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 13 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 9 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 8 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:28 megatron_base_model:275] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 17 None 256 1 4\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:28 nemo_logging:349] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1908: LightningDeprecationWarning: `trainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v2.0. Specify the fit checkpoint path with `trainer.fit(ckpt_path=)` instead.\n",
      "      rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 2 None 256 1 4\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 11 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 22 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 10 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 31 None 256 1 4\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 26 None 256 1 4\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 23 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 7 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 12 None 256 1 4\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f8b4a533400>\u001b[0m\n",
      "\u001b[34mprecision plugin:<pytorch_lightning.plugins.precision.tpu.TPUPrecisionPlugin object at 0x7f5d4ce99db0>\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 21 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 6 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 14 None 256 1 4\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing sep_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing cls_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing mask_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 15\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 24\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 21\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 20\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 29\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 8\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 26\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 31\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 14\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 19\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 11\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 12\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 18\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 23\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 23\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 30\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 26\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 29\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 10\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 8\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 22\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 14\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 31\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:1 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:2 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 23\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 29\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 10\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 15\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:2 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:3 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 29\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 10\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 22\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 12\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 27\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 17\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 15\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:3 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:4 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 29\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 22\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 14\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 19\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:4 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 15\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:5 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 26\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 29\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 12\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 27\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 28\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 19\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:5 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:6 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 16\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 10\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 29\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 12\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:6 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 19\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:7 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 15\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 10\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 29\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 22\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 12\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:7 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:8 to store for rank: 19\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 19\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 17\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 25\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 15\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 31\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 11\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 18\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 20\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 16\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 23\u001b[0m\n",
      "\u001b[34mRank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 21\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 26\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 30\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 10\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 13\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 29\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 8\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 9\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 22\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 12\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 27\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 28\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 14\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:8 with 32 nodes.\u001b[0m\n",
      "\u001b[34mAdded key: store_based_barrier_key:9 to store for rank: 24\u001b[0m\n",
      "\u001b[34mRank 24: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 24 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 19: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 17: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 19 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 17 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 15: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 25: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 31: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 15 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 25 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 31 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 11: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 11 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 18: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 18 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 20: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 20 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 1 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 3 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 16: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 23: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 16 None 256 1 4Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 23 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 21: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 4 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 21 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 26: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 30: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 26 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 30 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 10: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 13: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 29: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 10 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 13 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 29 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 8: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 9: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 8 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 9 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 22: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 7 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 0 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 22 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 5 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 2 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 6 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 27: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 12: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34mRank 28: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 27 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 12 None 256 1 4\u001b[0m\n",
      "\u001b[34mRank 14: Completed store-based barrier for key:store_based_barrier_key:9 with 32 nodes.\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 28 None 256 1 4\u001b[0m\n",
      "\u001b[34msetup_microbatch_calculator 14 None 256 1 4\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_gpt_model:940] Started profiling server for dp_rank=0, tp_rank=0, pp_rank=0, vp_rank=None on port:9000\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:255] Rank 0 has data parallel group: [0, 8, 16, 24]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:258] All data parallel group ranks: [[0, 8, 16, 24], [1, 9, 17, 25], [2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:259] Ranks 0 has data parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:267] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:268] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:278] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:282] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:283] Rank 0 has tensor model parallel rank: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:297] Rank 0 has pipeline model parallel group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:309] Rank 0 has embedding group: [0]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:315] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:316] Rank 0 has pipeline model parallel rank 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:317] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:34 megatron_init:318] Rank 0 has embedding rank: 0\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (3, 0): 842534912\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (0, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (1, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (2, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (5, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (4, 0): 842534912\u001b[0m\n",
      "\u001b[34m> number of parameters on (tensor, pipeline) model parallel rank (7, 0): 842534912\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:40:45 megatron_base_model:469] Cannot parse the checkpoint file to get the consumed samples. assume it is zero.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 megatron_gpt_model:845] Building GPT datasets.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:293]  > building dataset index ...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 indexed_dataset:453]     reading sizes...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 indexed_dataset:455]     reading pointers...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 indexed_dataset:459]     reading document index...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 indexed_dataset:512]     creating numpy buffer of mmap...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 indexed_dataset:514]     creating memory view of numpy buffer...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:297]  > finished creating indexed dataset in 0.000500 seconds\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:298]     number of documents: 1355\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:249]  > dataset split:\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:252]     train:\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:253]      document indices in [0, 1355) total of 1355 documents\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:252]     validation:\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:253]      document indices in [1355, 1355) total of 0 documents\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:252]     test:\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:253]      document indices in [1355, 1355) total of 0 documents\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:548]  > WARNING: could not find index map files, building the indices on rank 0 ...\u001b[0m\n",
      "\u001b[34mnum_samples_from_epochs_minus_one: 6218 || num_epochs: 21 || tokens_per_epoch: 636824 || add_extra_token: 1 || seq_length: 2048\u001b[0m\n",
      "\u001b[34mnum_samples: 6432 ||???\u001b[0m\n",
      "\u001b[34m> last epoch number of samples (214) is smaller than 80% of number of samples per epoch (310), setting separate_last_epoch to True\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:598]  > elasped time to build and save doc-idx mapping (seconds): 0.001046\u001b[0m\n",
      "\u001b[34mmake: Entering directory '/usr/local/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron'\u001b[0m\n",
      "\u001b[34mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[34mmake: Leaving directory '/usr/local/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron'\u001b[0m\n",
      "\u001b[34musing:\n",
      "     number of documents:       1355\n",
      "     number of epochs:          21\n",
      "     sequence length:           2048\n",
      "     total number of samples:   6529\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:624]  > elasped time to build and save sample-idx mapping (seconds): 0.031623\u001b[0m\n",
      "\u001b[34m> building shuffle index with split [0, 6218) and [6218, 6529) ...\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:638]  > elasped time to build and save shuffle-idx mapping (seconds): 0.000315\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:660]  > loading doc-idx mapping from /opt/ml/code/tmp/tokenized_data_text_document_train_indexmap_6432ns_2048sl_1234s_doc_idx.npy\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:662]  > loading sample-idx mapping from /opt/ml/code/tmp/tokenized_data_text_document_train_indexmap_6432ns_2048sl_1234s_sample_idx.npy\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:664]  > loading shuffle-idx mapping from /opt/ml/code/tmp/tokenized_data_text_document_train_indexmap_6432ns_2048sl_1234s_shuffle_idx.npy\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:666]     loaded indexed file in 0.001 seconds\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:667]     total number of samples: 6530\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 gpt_dataset:668]     total number of epochs: 21\u001b[0m\n",
      "\u001b[34mmake: Entering directory '/usr/local/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron'\u001b[0m\n",
      "\u001b[34mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[34mmake: Leaving directory '/usr/local/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron'\u001b[0m\n",
      "\u001b[34m> building indices for blendable datasets ...\u001b[0m\n",
      "\u001b[34m> sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.03 (sec)\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 megatron_gpt_model:877] Length of train dataset: 6432\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 megatron_gpt_model:882] Finished building GPT datasets.\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 megatron_gpt_model:1018] Setting up train dataloader with len(len(self._train_ds)): 6432 and consumed samples: 0\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:40:45 megatron_gpt_model:891] Building dataloader with consumed samples: 0\u001b[0m\n",
      "\u001b[34mRestoring states from the checkpoint path at /opt/ml/code/tmp/nemo_checkpoint/mp_rank_07/model_optim_rng.ckpt\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:41:05 modelPT:614] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.95]\n",
      "        capturable: True\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        lr: 0.0001\n",
      "        maximize: False\n",
      "        weight_decay: 0.1\n",
      "    \n",
      "    Parameter Group 1\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.95]\n",
      "        capturable: True\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        lr: 0.0001\n",
      "        maximize: False\n",
      "        weight_decay: 0.0\n",
      "    )\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:41:05 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f78b3c436a0>\" \n",
      "    will be used during training (effective maximum steps = 25) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 10\n",
      "    constant_steps: 0\n",
      "    min_lr: 1.0e-06\n",
      "    max_steps: 25\n",
      "    )\u001b[0m\n",
      "\u001b[34m[NeMo I 2023-12-09 17:41:05 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f78b049cfd0>\" \n",
      "    will be used during training (effective maximum steps = 25) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 10\n",
      "    constant_steps: 0\n",
      "    min_lr: 1.0e-06\n",
      "    max_steps: 25\n",
      "    )\u001b[0m\n",
      "\u001b[34mRestored all states from the checkpoint file at /opt/ml/code/tmp/nemo_checkpoint/mp_rank_07/model_optim_rng.ckpt\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:41:05 nemo_logging:349] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:208: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "      rank_zero_warn(\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:05.000776:  16101  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:05.000777:  16101  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_310851798465585165+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:41:05 nemo_logging:349] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:366: UserWarning: One of given dataloaders is None and it will be skipped.\n",
      "      rank_zero_warn(\"One of given dataloaders is None and it will be skipped.\")\u001b[0m\n",
      "\u001b[34mTraining: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:06.000361:  18600  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:06.000370:  18600  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_14554185930996491893+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000133:  22529  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000133:  22530  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000134:  22531  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000165:  22534  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000166:  22535  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000172:  22537  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000173:  22539  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000195:  22541  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000372:  22529  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_10045816615887760514+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000448:  22530  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_12598746364335896506+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000448:  22531  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_5493323624958044841+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000448:  22534  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_10441317650604297217+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000448:  22535  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_3600888674872224049+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000448:  22537  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_5250512035884166464+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000452:  22539  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_1619282520863546690+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:08.000452:  22541  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_5530781884043658869+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:18.000520:  22872  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:18.000585:  22872  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_17819721210996958436+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:26.000416:  23442  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:41:26.000485:  23442  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_9946645543341646997+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:00.000700:  58623  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:00.000702:  58623  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_14947573939663078061+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:01.000057:  58986  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:01.000071:  58986  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_14000877848722700329+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:03.000149:  60198  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:03.000173:  60198  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_5837248630630320423+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:03.000231:  60200  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:03.000257:  60200  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_6142833956506213788+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:04.000484:  60438  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:04.000485:  60438  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_16178973479619687874+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:04.000711:  60715  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:04.000712:  60715  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_349647913097389165+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:04.000917:  61048  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:04.000918:  61048  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_18180170502840997799+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:05.000124:  61451  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:05.000125:  61451  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_4005147696708003337+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m[NeMo W 2023-12-09 17:42:05 nemo_logging:349] /usr/local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('throughput_peak', ...)` in your `configure_gradient_clipping` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:05.000658:  62238  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:05.000677:  62238  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_5521626181480929843+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:05.000678:  62240  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:05.000698:  62240  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_12384099603961419910+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34mEpoch 0:   4%|▍         | 1/25 [01:00<24:06, 60.28s/it]\u001b[0m\n",
      "\u001b[34mEpoch 0:   4%|▍         | 1/25 [01:00<24:06, 60.28s/it, loss=nan, v_num=0-22, reduced_train_loss=1.340, gradient_norm=1.560, parameter_norm=1600.0, global_step=0.000, consumed_samples=0.000, iteration_time=55.20]\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:08.000478:  63493  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:08.000545:  63493  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_6143121921458704412+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:42.000283:  318  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:42.000284:  318  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_4658971354919886479+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34mEpoch 0:   8%|▊         | 2/25 [01:36<18:31, 48.33s/it, loss=nan, v_num=0-22, reduced_train_loss=1.340, gradient_norm=1.560, parameter_norm=1600.0, global_step=0.000, consumed_samples=0.000, iteration_time=55.20]\u001b[0m\n",
      "\u001b[34mEpoch 0:   8%|▊         | 2/25 [01:36<18:31, 48.33s/it, loss=1.34, v_num=0-22, reduced_train_loss=1.260, gradient_norm=1.280, parameter_norm=1584.0, global_step=1.000, consumed_samples=256.0, iteration_time=34.80]\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:43.000902:  3090  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:42:44.000007:  3090  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_2812950311548070509+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 3/25 [02:12<16:11, 44.15s/it, loss=1.34, v_num=0-22, reduced_train_loss=1.260, gradient_norm=1.280, parameter_norm=1584.0, global_step=1.000, consumed_samples=256.0, iteration_time=34.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 3/25 [02:12<16:11, 44.15s/it, loss=1.3, v_num=0-22, reduced_train_loss=1.140, gradient_norm=3.940, parameter_norm=1568.0, global_step=2.000, consumed_samples=512.0, iteration_time=34.20]\u001b[0m\n",
      "\u001b[34mEpoch 0:  16%|█▌        | 4/25 [02:45<14:28, 41.37s/it, loss=1.3, v_num=0-22, reduced_train_loss=1.140, gradient_norm=3.940, parameter_norm=1568.0, global_step=2.000, consumed_samples=512.0, iteration_time=34.20]\u001b[0m\n",
      "\u001b[34mEpoch 0:  16%|█▌        | 4/25 [02:45<14:28, 41.37s/it, loss=1.25, v_num=0-22, reduced_train_loss=1.150, gradient_norm=3.620, parameter_norm=1584.0, global_step=3.000, consumed_samples=768.0, iteration_time=31.90]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|██        | 5/25 [03:18<13:12, 39.61s/it, loss=1.25, v_num=0-22, reduced_train_loss=1.150, gradient_norm=3.620, parameter_norm=1584.0, global_step=3.000, consumed_samples=768.0, iteration_time=31.90]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|██        | 5/25 [03:18<13:12, 39.61s/it, loss=1.22, v_num=0-22, reduced_train_loss=0.996, gradient_norm=1.750, parameter_norm=1584.0, global_step=4.000, consumed_samples=1024.0, iteration_time=31.40]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 6/25 [03:50<12:09, 38.39s/it, loss=1.22, v_num=0-22, reduced_train_loss=0.996, gradient_norm=1.750, parameter_norm=1584.0, global_step=4.000, consumed_samples=1024.0, iteration_time=31.40]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 6/25 [03:50<12:09, 38.39s/it, loss=1.18, v_num=0-22, reduced_train_loss=0.977, gradient_norm=5.030, parameter_norm=1592.0, global_step=5.000, consumed_samples=1280.0, iteration_time=31.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  28%|██▊       | 7/25 [04:21<11:13, 37.43s/it, loss=1.18, v_num=0-22, reduced_train_loss=0.977, gradient_norm=5.030, parameter_norm=1592.0, global_step=5.000, consumed_samples=1280.0, iteration_time=31.10]\u001b[0m\n",
      "\u001b[34mEpoch 0:  28%|██▊       | 7/25 [04:21<11:13, 37.43s/it, loss=1.14, v_num=0-22, reduced_train_loss=0.883, gradient_norm=1.150, parameter_norm=1592.0, global_step=6.000, consumed_samples=1536.0, iteration_time=30.00]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 8/25 [04:53<10:24, 36.71s/it, loss=1.14, v_num=0-22, reduced_train_loss=0.883, gradient_norm=1.150, parameter_norm=1592.0, global_step=6.000, consumed_samples=1536.0, iteration_time=30.00]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 8/25 [04:53<10:24, 36.71s/it, loss=1.11, v_num=0-22, reduced_train_loss=0.824, gradient_norm=2.250, parameter_norm=1592.0, global_step=7.000, consumed_samples=1792.0, iteration_time=30.00]\u001b[0m\n",
      "\u001b[34mEpoch 0:  36%|███▌      | 9/25 [05:25<09:38, 36.14s/it, loss=1.11, v_num=0-22, reduced_train_loss=0.824, gradient_norm=2.250, parameter_norm=1592.0, global_step=7.000, consumed_samples=1792.0, iteration_time=30.00]\u001b[0m\n",
      "\u001b[34mEpoch 0:  36%|███▌      | 9/25 [05:25<09:38, 36.14s/it, loss=1.07, v_num=0-22, reduced_train_loss=0.723, gradient_norm=2.390, parameter_norm=1584.0, global_step=8.000, consumed_samples=2048.0, iteration_time=29.90]\u001b[0m\n",
      "\u001b[34mEpoch 0:  40%|████      | 10/25 [05:56<08:54, 35.65s/it, loss=1.07, v_num=0-22, reduced_train_loss=0.723, gradient_norm=2.390, parameter_norm=1584.0, global_step=8.000, consumed_samples=2048.0, iteration_time=29.90]\u001b[0m\n",
      "\u001b[34mEpoch 0:  40%|████      | 10/25 [05:56<08:54, 35.65s/it, loss=1.03, v_num=0-22, reduced_train_loss=0.613, gradient_norm=2.020, parameter_norm=1584.0, global_step=9.000, consumed_samples=2304.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 11/25 [06:27<08:13, 35.26s/it, loss=1.03, v_num=0-22, reduced_train_loss=0.613, gradient_norm=2.020, parameter_norm=1584.0, global_step=9.000, consumed_samples=2304.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 11/25 [06:27<08:13, 35.26s/it, loss=0.991, v_num=0-22, reduced_train_loss=0.570, gradient_norm=1.230, parameter_norm=1584.0, global_step=10.00, consumed_samples=2560.0, iteration_time=29.60]\u001b[0m\n",
      "\u001b[34mEpoch 0:  48%|████▊     | 12/25 [06:58<07:33, 34.92s/it, loss=0.991, v_num=0-22, reduced_train_loss=0.570, gradient_norm=1.230, parameter_norm=1584.0, global_step=10.00, consumed_samples=2560.0, iteration_time=29.60]\u001b[0m\n",
      "\u001b[34mEpoch 0:  48%|████▊     | 12/25 [06:58<07:33, 34.92s/it, loss=0.952, v_num=0-22, reduced_train_loss=0.463, gradient_norm=1.720, parameter_norm=1576.0, global_step=11.00, consumed_samples=2816.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  52%|█████▏    | 13/25 [07:30<06:55, 34.63s/it, loss=0.952, v_num=0-22, reduced_train_loss=0.463, gradient_norm=1.720, parameter_norm=1576.0, global_step=11.00, consumed_samples=2816.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  52%|█████▏    | 13/25 [07:30<06:55, 34.63s/it, loss=0.912, v_num=0-22, reduced_train_loss=0.400, gradient_norm=1.200, parameter_norm=1584.0, global_step=12.00, consumed_samples=3072.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 14/25 [08:01<06:18, 34.38s/it, loss=0.912, v_num=0-22, reduced_train_loss=0.400, gradient_norm=1.200, parameter_norm=1584.0, global_step=12.00, consumed_samples=3072.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 14/25 [08:01<06:18, 34.38s/it, loss=0.872, v_num=0-22, reduced_train_loss=0.346, gradient_norm=1.050, parameter_norm=1600.0, global_step=13.00, consumed_samples=3328.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  60%|██████    | 15/25 [08:32<05:41, 34.18s/it, loss=0.872, v_num=0-22, reduced_train_loss=0.346, gradient_norm=1.050, parameter_norm=1600.0, global_step=13.00, consumed_samples=3328.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  60%|██████    | 15/25 [08:32<05:41, 34.18s/it, loss=0.835, v_num=0-22, reduced_train_loss=0.275, gradient_norm=0.945, parameter_norm=1584.0, global_step=14.00, consumed_samples=3584.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  64%|██████▍   | 16/25 [09:03<05:05, 34.00s/it, loss=0.835, v_num=0-22, reduced_train_loss=0.275, gradient_norm=0.945, parameter_norm=1584.0, global_step=14.00, consumed_samples=3584.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  64%|██████▍   | 16/25 [09:03<05:05, 34.00s/it, loss=0.797, v_num=0-22, reduced_train_loss=0.227, gradient_norm=0.617, parameter_norm=1592.0, global_step=15.00, consumed_samples=3840.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  68%|██████▊   | 17/25 [09:35<04:30, 33.85s/it, loss=0.797, v_num=0-22, reduced_train_loss=0.227, gradient_norm=0.617, parameter_norm=1592.0, global_step=15.00, consumed_samples=3840.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  68%|██████▊   | 17/25 [09:35<04:30, 33.85s/it, loss=0.762, v_num=0-22, reduced_train_loss=0.191, gradient_norm=0.840, parameter_norm=1592.0, global_step=16.00, consumed_samples=4096.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  72%|███████▏  | 18/25 [10:06<03:55, 33.70s/it, loss=0.762, v_num=0-22, reduced_train_loss=0.191, gradient_norm=0.840, parameter_norm=1592.0, global_step=16.00, consumed_samples=4096.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  72%|███████▏  | 18/25 [10:06<03:55, 33.70s/it, loss=0.728, v_num=0-22, reduced_train_loss=0.164, gradient_norm=0.512, parameter_norm=1592.0, global_step=17.00, consumed_samples=4352.0, iteration_time=29.60]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 19/25 [10:37<03:21, 33.57s/it, loss=0.728, v_num=0-22, reduced_train_loss=0.164, gradient_norm=0.512, parameter_norm=1592.0, global_step=17.00, consumed_samples=4352.0, iteration_time=29.60]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 19/25 [10:37<03:21, 33.57s/it, loss=0.697, v_num=0-22, reduced_train_loss=0.138, gradient_norm=0.346, parameter_norm=1592.0, global_step=18.00, consumed_samples=4608.0, iteration_time=29.50]\u001b[0m\n",
      "\u001b[34mEpoch 0:  80%|████████  | 20/25 [11:08<02:47, 33.44s/it, loss=0.697, v_num=0-22, reduced_train_loss=0.138, gradient_norm=0.346, parameter_norm=1592.0, global_step=18.00, consumed_samples=4608.0, iteration_time=29.50]\u001b[0m\n",
      "\u001b[34mEpoch 0:  80%|████████  | 20/25 [11:08<02:47, 33.44s/it, loss=0.667, v_num=0-22, reduced_train_loss=0.117, gradient_norm=0.281, parameter_norm=1600.0, global_step=19.00, consumed_samples=4864.0, iteration_time=29.60]\u001b[0m\n",
      "\u001b[34mEpoch 0:  84%|████████▍ | 21/25 [11:40<02:13, 33.34s/it, loss=0.667, v_num=0-22, reduced_train_loss=0.117, gradient_norm=0.281, parameter_norm=1600.0, global_step=19.00, consumed_samples=4864.0, iteration_time=29.60]\u001b[0m\n",
      "\u001b[34mEpoch 0:  84%|████████▍ | 21/25 [11:40<02:13, 33.34s/it, loss=0.64, v_num=0-22, reduced_train_loss=0.105, gradient_norm=0.336, parameter_norm=1576.0, global_step=20.00, consumed_samples=5120.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  88%|████████▊ | 22/25 [12:11<01:39, 33.25s/it, loss=0.64, v_num=0-22, reduced_train_loss=0.105, gradient_norm=0.336, parameter_norm=1576.0, global_step=20.00, consumed_samples=5120.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  88%|████████▊ | 22/25 [12:11<01:39, 33.25s/it, loss=0.578, v_num=0-22, reduced_train_loss=0.0981, gradient_norm=0.375, parameter_norm=1584.0, global_step=21.00, consumed_samples=5376.0, iteration_time=29.50]\u001b[0m\n",
      "\u001b[34mEpoch 0:  92%|█████████▏| 23/25 [12:42<01:06, 33.16s/it, loss=0.578, v_num=0-22, reduced_train_loss=0.0981, gradient_norm=0.375, parameter_norm=1584.0, global_step=21.00, consumed_samples=5376.0, iteration_time=29.50]\u001b[0m\n",
      "\u001b[34mEpoch 0:  92%|█████████▏| 23/25 [12:42<01:06, 33.16s/it, loss=0.52, v_num=0-22, reduced_train_loss=0.0923, gradient_norm=0.354, parameter_norm=1592.0, global_step=22.00, consumed_samples=5632.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  96%|█████████▌| 24/25 [13:14<00:33, 33.10s/it, loss=0.52, v_num=0-22, reduced_train_loss=0.0923, gradient_norm=0.354, parameter_norm=1592.0, global_step=22.00, consumed_samples=5632.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0:  96%|█████████▌| 24/25 [13:14<00:33, 33.10s/it, loss=0.468, v_num=0-22, reduced_train_loss=0.0884, gradient_norm=0.322, parameter_norm=1584.0, global_step=23.00, consumed_samples=5888.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 25/25 [13:45<00:00, 33.04s/it, loss=0.468, v_num=0-22, reduced_train_loss=0.0884, gradient_norm=0.322, parameter_norm=1584.0, global_step=23.00, consumed_samples=5888.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 25/25 [13:45<00:00, 33.04s/it, loss=0.415, v_num=0-22, reduced_train_loss=0.0859, gradient_norm=0.231, parameter_norm=1584.0, global_step=24.00, consumed_samples=6144.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 25/25 [13:45<00:00, 33.04s/it, loss=0.415, v_num=0-22, reduced_train_loss=0.0859, gradient_norm=0.231, parameter_norm=1584.0, global_step=24.00, consumed_samples=6144.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34m`Trainer.fit` stopped: `max_steps=25` reached.\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 25/25 [13:45<00:00, 33.04s/it, loss=0.415, v_num=0-22, reduced_train_loss=0.0859, gradient_norm=0.231, parameter_norm=1584.0, global_step=24.00, consumed_samples=6144.0, iteration_time=29.80]\u001b[0m\n",
      "\u001b[34m2023-12-09 17:54:53.000149:  40339  INFO ||NEURON_CACHE||: Compile cache path: /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN\u001b[0m\n",
      "\u001b[34m2023-12-09 17:54:53.000250:  40339  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/PRE_COMPILED_NEURON_GRAPH_TRAIN/neuronxcc-2.10.0.35+3817a0c8c/MODULE_1058818878314341441+f9c53d58/model.neff. Exiting with a successfully compiled graph.\u001b[0m\n",
      "\u001b[34mTP: 8, PP: 1\u001b[0m\n",
      "\u001b[34mLoading PP=0\u001b[0m\n",
      "\u001b[34m>> model.language_model.embedding.word_embeddings.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.0.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.0.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.0.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.0.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.0.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.0.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.1.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.1.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.1.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.1.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.1.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.1.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.2.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.2.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.2.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.2.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.2.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.2.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.3.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.3.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.3.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.3.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.3.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.3.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.4.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.4.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.4.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.4.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.4.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.4.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.5.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.5.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.5.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.5.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.5.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.5.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.6.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.6.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.6.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.6.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.6.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.6.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.7.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.7.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.7.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.7.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.7.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.7.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.8.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.8.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.8.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.8.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.8.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.8.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.9.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.9.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.9.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.9.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.9.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.9.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.10.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.10.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.10.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.10.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.10.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.10.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.11.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.11.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.11.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.11.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.11.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.11.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.12.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.12.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.12.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.12.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.12.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.12.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.13.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.13.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.13.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.13.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.13.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.13.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.14.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.14.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.14.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.14.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.14.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.14.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.15.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.15.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.15.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.15.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.15.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.15.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.16.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.16.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.16.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.16.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.16.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.16.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.17.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.17.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.17.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.17.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.17.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.17.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.18.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.18.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.18.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.18.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.18.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.18.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.19.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.19.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.19.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.19.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.19.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.19.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.20.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.20.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.20.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.20.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.20.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.20.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.21.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.21.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.21.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.21.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.21.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.21.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.22.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.22.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.22.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.22.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.22.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.22.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.23.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.23.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.23.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.23.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.23.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.23.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.24.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.24.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.24.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.24.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.24.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.24.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.25.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.25.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.25.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.25.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.25.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.25.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.26.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.26.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.26.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.26.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.26.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.26.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.27.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.27.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.27.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.27.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.27.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.27.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.28.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.28.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.28.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.28.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.28.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.28.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.29.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.29.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.29.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.29.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.29.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.29.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.30.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.30.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.30.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.30.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.30.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.30.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.31.input_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.31.self_attention.query_key_value.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.31.self_attention.core_attention.rotary_emb.inv_freq\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.31.self_attention.dense.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.31.post_attention_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.31.mlp.dense_h_to_4h_2.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.encoder.final_layernorm.weight\u001b[0m\n",
      "\u001b[34m>> model.language_model.output_layer.weight\u001b[0m\n",
      "\u001b[34mLoading the checkpoint in a Llama model.\u001b[0m\n",
      "\u001b[34mRemove the pytorch_model.bin file\u001b[0m\n",
      "\u001b[34mModel type 7B is identified. Splitting it into sub-checkpoints for deployment on inf2 instance.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:58:12,503 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:58:12,503 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-12-09 17:58:12,503 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1802\n",
      "Billable seconds: 1802\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "training_job_name = ''\n",
    "attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "attached_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3889d9-1567-41ad-9375-fb738db629fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Studio Kernel idle issue:  If your studio kernel goes idle and you lose reference to the estimator object, please see section [4. Studio Kernel Dead/Creating JumpStart Model from the training Job](#4.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job) on how to deploy endpoint using the training job name and the model id. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy the fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = attached_estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c01e892-a70e-487b-989f-c1bbe5caba23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_inference = (\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExtract the factors that influence the cost of building a house\\n\\n### Input:\\nThe cost of building a house varies by country widely. According to data from the National Association of Realtors, the median cost of buying an existing single-family house in the United States is $274,600, whereas the average cost to build is $296,652. Several different factors can impact the cost of building a house, including the size of the dwelling, the location, and availability of resources, the slope of the land, the quality of the fixtures and fittings, and the difficulty in finding construction and building materials talent\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'pretrained_predictor' is not defined\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt_inference.format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[\"generated_text\"])\n",
    "    finetuned_response = finetuned_predictor.predict(payload)\n",
    "    responses_after_finetuning.append(finetuned_response[\"generated_text\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a0ce96-abac-4bd7-9a06-aa75b222dd79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for urllib3: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/urllib3-2.0.7.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "awscli 1.29.63 requires botocore==1.31.63, but you have botocore 1.33.11 which is incompatible.\n",
      "awscli 1.29.63 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.0.7 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker 2.199.0 requires boto3<2.0,>=1.33.3, but you have boto3 1.33.2 which is incompatible.\n",
      "sagemaker 2.199.0 requires urllib3<1.27, but you have urllib3 2.0.7 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "awscli 1.29.63 requires botocore==1.31.63, but you have botocore 1.33.2 which is incompatible.\n",
      "awscli 1.29.63 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires dataclasses-json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.0.7 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker 2.199.0 requires boto3<2.0,>=1.33.3, but you have boto3 1.33.2 which is incompatible.\n",
      "sagemaker 2.199.0 requires urllib3<1.27, but you have urllib3 2.0.7 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "awscli 1.29.63 requires botocore==1.31.63, but you have botocore 1.33.2 which is incompatible.\n",
      "awscli 1.29.63 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "botocore 1.33.2 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
      "datasets 2.15.0 requires fsspec[http]<=2023.10.0,>=2023.1.0, but you have fsspec 2023.12.1 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "fastapi 0.95.2 requires pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2, but you have pydantic 2.5.2 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires dataclasses-json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.1.0 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.2 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker 2.199.0 requires boto3<2.0,>=1.33.3, but you have boto3 1.33.2 which is incompatible.\n",
      "sagemaker 2.199.0 requires urllib3<1.27, but you have urllib3 2.1.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: pandas 2.1.4 does not provide the extra 'jinja2'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "autovizwidget 0.21.0 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.1.4 which is incompatible.\n",
      "datasets 2.15.0 requires fsspec[http]<=2023.10.0,>=2023.1.0, but you have fsspec 2023.12.1 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "fastapi 0.95.2 requires pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2, but you have pydantic 2.5.2 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "langchain 0.0.342 requires anyio<4.0, but you have anyio 4.1.0 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.2 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker 2.199.0 requires boto3<2.0,>=1.33.3, but you have boto3 1.33.2 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3==1.33.2 --force-reinstall --quiet\n",
    "%pip install botocore==1.33.2 --force-reinstall --quiet\n",
    "%pip install langchain==0.0.342 --force-reinstall --quiet\n",
    "%pip install llama-index==0.9.3.post1 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff534ea-6238-4ca9-8d95-9f3b8c53e29f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcfeaf15-31a5-41a6-94de-e5cac00c7ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c33b223-7a29-4132-a2a7-47b37717e5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import retrieve_default\n",
    "\n",
    "endpoint_name_pretrained = \"\"\n",
    "\n",
    "finetuned_predictor = retrieve_default(model_id=\"meta-textgenerationneuron-llama-2-7b\", model_version=\"1.*\", endpoint_name=endpoint_name_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17b545-2167-40e0-bad3-78ffbf40f3ba",
   "metadata": {},
   "source": [
    "# Use UpTrain for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833d7036-ddee-461e-820d-9bebac623a09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uptrain\n",
      "  Downloading uptrain-0.4.6-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pydantic<1.10.10 (from uptrain)\n",
      "  Downloading pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting loguru (from uptrain)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting lazy-loader (from uptrain)\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from uptrain) (2.8.4)\n",
      "Collecting polars>=0.18 (from uptrain)\n",
      "  Downloading polars-0.19.19-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from uptrain) (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from uptrain) (1.26.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from uptrain) (0.25.2)\n",
      "Requirement already satisfied: plotly>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from uptrain) (5.9.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->uptrain) (4.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->uptrain) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->uptrain) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->uptrain) (3.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->uptrain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->uptrain) (0.14.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=5.0.0->uptrain) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<1.10.10->uptrain) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->uptrain) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->uptrain) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->uptrain) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->uptrain) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->uptrain) (1.2.0)\n",
      "Downloading uptrain-0.4.6-py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.4/228.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading polars-0.19.19-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.5/28.5 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic, polars, loguru, lazy-loader, uptrain\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.2\n",
      "    Uninstalling pydantic-2.5.2:\n",
      "      Successfully uninstalled pydantic-2.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.0.342 requires anyio<4.0, but you have anyio 4.1.0 which is incompatible.\n",
      "sagemaker 2.199.0 requires boto3<2.0,>=1.33.3, but you have boto3 1.33.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed lazy-loader-0.3 loguru-0.7.2 polars-0.19.19 pydantic-1.10.9 uptrain-0.4.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install uptrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773a51e-1820-4e6e-9533-9df6ddd0de9d",
   "metadata": {},
   "source": [
    "## Now, evaluate the LLM finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "796fffba-2258-480f-81bf-9260f680d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"Extract the factors that influence the cost of building a house\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = finetuned_predictor.predict(payload)\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b13e50-ca1b-4fb4-950b-c588c9069915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uptrain import APIClient, Evals, CritiqueTone\n",
    "import json\n",
    "\n",
    "UPTRAIN_API_KEY = \"\"\n",
    "\n",
    "data = [{\n",
    "    'question': 'Extract the factors that influence the cost of building a house',\n",
    "    'context': \"The cost of building a house varies by country widely. According to data from the National Association of Realtors, the median cost of buying an existing single-family house in the United States is $274,600, whereas the average cost to build is $296,652. Several different factors can impact the cost of building a house, including the size of the dwelling, the location, and availability of resources, the slope of the land, the quality of the fixtures and fittings, and the difficulty in finding construction and building materials talent\",\n",
    "    'response': \n",
    "}]\n",
    "\n",
    "client = APIClient(uptrain_api_key=UPTRAIN_API_KEY)\n",
    "\n",
    "results = client.log_and_evaluate(\n",
    "    project_name=\"Sample-Project\",\n",
    "    data=data,\n",
    "    checks=[Evals.CONTEXT_RELEVANCE, Evals.FACTUAL_ACCURACY, Evals.RESPONSE_RELEVANCE, CritiqueTone(persona=\"teacher\")]\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "# pretrained_predictor.delete_model()\n",
    "# pretrained_predictor.delete_endpoint()\n",
    "# finetuned_predictor.delete_model()\n",
    "# finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {},
   "source": [
    "### 1. Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **stop**: If specified, it must be a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db86872-cca4-4ec5-8574-6876668ebf12",
   "metadata": {},
   "source": [
    "### 2. Use text file as input to fine-tune LLaMA-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85c93a9-ebe2-4966-a5d6-af4c053f69f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# model_id = \"meta-textgenerationneuron-llama-2-7b\" #or  \"meta-textgenerationneuron-llama-2-13b\"\n",
    "\n",
    "# estimator = JumpStartEstimator(model_id=model_id,  environment={\"accept_eula\": \"false\"})\n",
    "# estimator.set_hyperparameters(max_steps=30)\n",
    "# estimator.fit({\"training\": f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### 3. Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. Default: 2048.\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 6e-6.\n",
    "- min_learning_rate: The learning rate at the last step of learning rate scheduler 'CosineAnnealing'. Default: 1e-06.\n",
    "- global_train_batch_size: The global batch size for training. Based on global_train_batch_size, the gradient accumulation is calculated as global_train_batch_size / (data_parallel_degree * per_device_train_batch_size), where data_parallel_degree is calculated as total number of neuron cores / (tensor_parallel_degree * pipeline_parallel_degree). Default: 256.\n",
    "- per_device_train_batch_size: The batch size per Neuron core for training. Default: 1\n",
    "- layer_norm_epilson: During layer normalization, a value added to the denominator for numerical stability. See [documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). Default: 0.00001.\n",
    "- preprocessing_num_workers: The number of processors to use for the preprocessing. If None, all of workers (number of vCPUs) are used for preprocessing. Default: \"None\"\n",
    "- weight_decay: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in `AdamW` optimizer. Default: 0.1.\n",
    "- lr_scheduler_type: Learning rate scheduler type. Default: 'CosineAnnealing' (currently we only support 'CosineAnnealing' scheduler type).\n",
    "- warmup_steps: Linear warmup over warmup steps. Default: 10.\n",
    "- constant_steps: The number of steps for learning rate to be constant after warmup_steps in 'CosineAnnealing' scheduler type. Default: 0.\n",
    "- adam_beta1: The beta1 hyperparameter (exponential decay rate for the first moment estimates) for the AdamW optimizer. Default: 0.9.\n",
    "- adam_beta2: The beta2 hyperparameter (exponential decay rate for the first moment estimates) for the AdamW optimizer. Default: 0.95.\n",
    "- mixed_precision: Whether to use mixed precision. If mixed_precision to be 'True', it means that master weights and optimizer states are stored in fp32, and model weights are saved in bf16. For details, see [reference](https://arxiv.org/pdf/1710.03740.pdf). Default: 'True'.\n",
    "- tensor_parallel_degree: The number of neuron cores which specific model weights, gradients, and optimizer states are split across. For details, see [reference](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html). Default: \"8\" (currently we only support parallel degree as 8).\n",
    "- pipeline_parallel_degree: The number of neuron cores which the layers of a model are partitioned across. Default: \"1\" (currently we only support \"1\" for LLaMA-2 7B and \"4\" for LLaMA-2 13B).\n",
    "- append_eod: Whether to append an `<eod>` token to the end of each example. By setting it to 'True', the fine-tuned model tends to generate succinct output. Default: 'False'.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce841-3a2c-4c08-a102-b94148036a5a",
   "metadata": {},
   "source": [
    "### 4. Studio Kernel goes idle/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Training job may take several hours due to setting of hyperparameters and the studio kernel may be in idle stage during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60a66-1c2f-42df-8079-191319e28a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
