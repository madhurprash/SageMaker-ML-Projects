{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yoga Recommender: Retrieval Augmented Question & Answering with Amazon Bedrock using LangChain \n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "Previously we saw that the model told us how to to change the tire, however we had to manually provide it with the relevant data and provide the contex ourselves. We explored the approach to leverage the model availabe under Bedrock and ask questions based on it's knowledge learned during training as well as providing manual context. While that approach works with short documents or single-ton applications, it fails to scale to enterprise level question answering where there could be large enterprise documents which cannot all be fit into the prompt sent to the model. \n",
    "\n",
    "### Pattern\n",
    "We can improve upon this process by implementing an architecure called Retreival Augmented Generation (RAG). RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "In this notebook we explain how to approach the pattern of Question Answering to find and leverage the documents to provide answers to the user questions.\n",
    "\n",
    "### Challenges\n",
    "- How to manage large document(s) that exceed the token limit\n",
    "- How to find the document(s) relevant to the question being asked\n",
    "\n",
    "### Proposal\n",
    "To the above challenges, this notebook proposes the following strategy\n",
    "#### Prepare documents\n",
    "![Embeddings](./images/Embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Load the documents\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "#### Ask question\n",
    "![Question](./images/Chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase\n",
    "#### Dataset\n",
    "To explain this architecture pattern we are using the documents from IRS. These documents explain topics such as:\n",
    "- Original Issue Discount (OID) Instruments\n",
    "- Reporting Cash Payments of Over $10,000 to IRS\n",
    "- Employer's Tax Guide\n",
    "\n",
    "#### Persona\n",
    "Let's assume a persona of a layman who doesn't have an understanding of how IRS works and if some actions have implications or not.\n",
    "\n",
    "The model will try to answer from the documents in easy language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "In order to follow the RAG approach this notebook is using the LangChain framework where it has integrations with different services and tools that allow efficient building of patterns such as RAG. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: Anthropic Claude V1 available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "- **Embeddings Model**: Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to generate a numerical representation of the textual documents\n",
    "- **Document Loader**: PDF Loader available through LangChain\n",
    "\n",
    "  This is the loader that can load the documents from a source, for the sake of this notebook we are loading the sample files from a local path. This could easily be replaced with a loader to load documents from enterprise internal systems.\n",
    "\n",
    "- **Vector Store**: FAISS available through LangChain\n",
    "\n",
    "  In this notebook we are using this in-memory vector-store to store both the embeddings and the documents. In an enterprise context this could be replaced with a persistent store such as AWS OpenSearch, RDS Postgres with pgVector, ChromaDB, Pinecone or Weaviate.\n",
    "- **Index**: VectorIndex\n",
    "\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document\n",
    "- **Wrapper**: wraps index, vector store, embeddings model and the LLM to abstract away the logic from the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock.\n",
    "\n",
    "For more details on how the setup works and ⚠️ **whether you might need to make any changes**, refer to the [Bedrock boto3 setup notebook](../00_Intro/bedrock_boto3_setup.ipynb) notebook.\n",
    "\n",
    "In this notebook, we'll also need some extra dependencies:\n",
    "\n",
    "- [FAISS](https://github.com/facebookresearch/faiss), to store vector embeddings\n",
    "- [PyPDF](https://pypi.org/project/pypdf/), for handling PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /root/dependencies/awscli-1.29.21-py3-none-any.whl\n",
      "Processing /root/dependencies/boto3-1.28.21-py3-none-any.whl\n",
      "Processing /root/dependencies/botocore-1.31.21-py3-none-any.whl\n",
      "Collecting docutils<0.17,>=0.10 (from awscli==1.29.21)\n",
      "  Using cached docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from awscli==1.29.21)\n",
      "  Obtaining dependency information for s3transfer<0.7.0,>=0.6.0 from https://files.pythonhosted.org/packages/d9/17/a3b666f5ef9543cfd3c661d39d1e193abb9649d0cfbbfee3cf3b51d5af02/s3transfer-0.6.2-py3-none-any.whl.metadata\n",
      "  Using cached s3transfer-0.6.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting PyYAML<6.1,>=3.10 (from awscli==1.29.21)\n",
      "  Obtaining dependency information for PyYAML<6.1,>=3.10 from https://files.pythonhosted.org/packages/29/61/bf33c6c85c55bc45a29eee3195848ff2d518d84735eb0e2d8cb42e0d285e/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting colorama<0.4.5,>=0.2.5 (from awscli==1.29.21)\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli==1.29.21)\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.31.21)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore==1.31.21)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore==1.31.21)\n",
      "  Obtaining dependency information for urllib3<1.27,>=1.25.4 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore==1.31.21)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli==1.29.21)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
      "Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, six, PyYAML, pyasn1, jmespath, docutils, colorama, rsa, python-dateutil, botocore, s3transfer, boto3, awscli\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.0\n",
      "    Uninstalling pyasn1-0.5.0:\n",
      "      Successfully uninstalled pyasn1-0.5.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.7.2\n",
      "    Uninstalling rsa-4.7.2:\n",
      "      Successfully uninstalled rsa-4.7.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.21\n",
      "    Uninstalling botocore-1.31.21:\n",
      "      Successfully uninstalled botocore-1.31.21\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.6.2\n",
      "    Uninstalling s3transfer-0.6.2:\n",
      "      Successfully uninstalled s3transfer-0.6.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.28.21\n",
      "    Uninstalling boto3-1.28.21:\n",
      "      Successfully uninstalled boto3-1.28.21\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.29.21\n",
      "    Uninstalling awscli-1.29.21:\n",
      "      Successfully uninstalled awscli-1.29.21\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.2 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.0 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.0 which is incompatible.\n",
      "notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.3.0 which is incompatible.\n",
      "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 25.1.0 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.2.1 which is incompatible.\n",
      "pyasn1-modules 0.2.8 requires pyasn1<0.5.0,>=0.4.6, but you have pyasn1 0.5.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.14.0 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.0a6 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.14.0 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 awscli-1.29.21 boto3-1.28.21 botocore-1.31.21 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 pyasn1-0.5.0 python-dateutil-2.8.2 rsa-4.7.2 s3transfer-0.6.2 six-1.16.0 urllib3-1.26.16\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you ran `download-dependencies.sh` from the root of the repository first!\n",
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    ../dependencies/awscli-*-py3-none-any.whl \\\n",
    "    ../dependencies/boto3-*-py3-none-any.whl \\\n",
    "    ../dependencies/botocore-*-py3-none-any.whl\n",
    "\n",
    "%pip install --quiet \"faiss-cpu>=1.7,<2\" langchain==0.0.249 \"pypdf>=3.8,<4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::706553727873:role/service-role/AmazonSageMaker-ExecutionRole-20230713T143270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "os.environ['BEDROCK_ASSUME_ROLE'] = role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/testing_customer_data\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \".\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "print(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \".\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"https://bedrockForYoga\"  # E.g. \"https://...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure langchain\n",
    "\n",
    "We begin with instantiating the LLM and the Embeddings model. Here we are using Anthropic Claude for text generation and Amazon Titan for text embedding.\n",
    "\n",
    "Note: It is possible to choose other models available with Bedrock. You can replace the `model_id` as follows to change the model.\n",
    "\n",
    "`llm = Bedrock(model_id=\"amazon.titan-tg1-large\")`\n",
    "\n",
    "Available model IDs include:\n",
    "\n",
    "- `amazon.titan-tg1-large`\n",
    "- `ai21.j2-grande-instruct`\n",
    "- `ai21.j2-jumbo-instruct`\n",
    "- `anthropic.claude-instant-v1`\n",
    "- `anthropic.claude-v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# - create the Anthropic Model\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':2000, 'temperature':0})\n",
    "bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BedrockEmbeddingsCustom(client=<botocore.client.Bedrock object at 0x7efd81935ae0>, region_name=None, credentials_profile_name=None, model_id='amazon.titan-e1t-medium', model_kwargs=None, endpoint_url=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import traceback\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "class BedrockEmbeddingsCustom(BedrockEmbeddings):\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a Bedrock model.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        print(f\"BedrockEmbeddingsCustom: embed_docs():: lenght of texts={len(texts)}::\")\n",
    "        results = []\n",
    "        counter = 1\n",
    "        errors = []\n",
    "        for text in texts:\n",
    "            try:\n",
    "                response = self._embedding_func(text)\n",
    "                results.append(response)\n",
    "                #print(f\"BedrockEmbeddingsCustom: embed_docs()::processed doc_{counter}:\")\n",
    "                counter+=1\n",
    "            except:\n",
    "                print(f\"BedrockEmbeddingsCustom: ERROR ={traceback.format_exc()}:: WAITING for 20 SEC\")\n",
    "                time.sleep(20) # 20 sec\n",
    "                errors.append(text)\n",
    "        \n",
    "        print(f\"BedrockEmbeddingsCustom: embed_docs(): TRYING Errors now:len={len(errors)}:\")\n",
    "        for text in errors:\n",
    "            print(f\"BedrockEmbeddingsCustom: embed_docs(): error :text={text}:\")\n",
    "            try:\n",
    "                response = self._embedding_func(text)\n",
    "                results.append(response)\n",
    "                #print(f\"BedrockEmbeddingsCustom: embed_docs()::processed doc_{counter}:\")\n",
    "                counter+=1\n",
    "            except:\n",
    "                print(f\"BedrockEmbeddingsCustom: ERROR ={text}:: WAITING for 20 SEC\")\n",
    "                time.sleep(20) # 20 sec\n",
    "                    \n",
    "        return results\n",
    "    \n",
    "bedrock_embeddings = BedrockEmbeddingsCustom(client=boto3_bedrock)\n",
    "bedrock_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's first download some of the files to build our document store. For this example we will be using public IRS documents from [here](https://www.irs.gov/publications)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 512 tokens, which roughly translates to ~2000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./yoga_data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 18 documents loaded is 2344 characters.\n",
      "After the split we have 52 documents more than the original 18.\n",
      "Average length among 52 documents (after split) is 844 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had 3 PDF documents which have been split into smaller ~500 chunks.\n",
    "\n",
    "Now we can see how a sample embedding would look like for one of those chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding of a document chunk:  [ 0.47460938 -0.33789062 -0.50390625 ... -0.67578125  0.48242188\n",
      "  0.24316406]\n",
      "Size of the embedding:  (4096,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the similar pattern embeddings could be generated for the entire corpus and stored in a vector store.\n",
    "\n",
    "This can be easily done using [FAISS](https://github.com/facebookresearch/faiss) implementation inside [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) which takes  input the embeddings model and the documents to create the entire vector store. Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. [VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation) helps us with that.\n",
    "\n",
    "**⚠️⚠️⚠️ NOTE: it might take few minutes to run the following cell ⚠️⚠️⚠️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BedrockEmbeddingsCustom: embed_docs():: lenght of texts=52::\n",
      "BedrockEmbeddingsCustom: embed_docs(): TRYING Errors now:len=0:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)\n",
    "\n",
    "vectorstore_faiss.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore_faiss = FAISS.load_local(\"faiss_index\", bedrock_embeddings)\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "Now that we have our vector store in place, we can start asking questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Yoga for anxiety and back pain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step would be to create an embedding of the query such that it could be compared with the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47851562, -0.5546875 , -0.46679688, ..., -0.671875  ,\n",
       "        0.2890625 , -0.1796875 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = vectorstore_faiss.embedding_function(query)\n",
    "np.array(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this embedding of the query to then fetch relevant documents.\n",
    "Now our query is represented as embeddings we can do a similarity search of our query against our data store providing us with the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "----\n",
      "## Document 1: right Peloton Yoga class to fit your individual needs.\n",
      "Find out more about Peloton’s on-demand yoga classes, including those specifically designed to\n",
      "release stress.\n",
      "Why it’s beneficial\n",
      "If you’re dealing with back pain, yoga may be just what the doctor ordered. Yoga is a mind-body\n",
      "therapy that’s often\n",
      "recommended to treat not only back pain but the stress that accompanies it. The appropriate poses\n",
      "can relax and strengthen\n",
      "your body.\n",
      "Practicing yoga for even a few minutes a day can help you gain more awareness of your body. This\n",
      "will help you notice where\n",
      "you’re holding tension and where you have imbalances. You can use this awareness to bring yourself\n",
      "into balance and\n",
      "alignment.\n",
      "Keep reading to learn more about how these poses may be useful in treating back pain.\n",
      "1. Cat-Cow\n",
      "This gentle, accessible backbend stretches and mobilizes the spine. Practicing this pose also\n",
      "stretches your torso, shoulders,\n",
      "and neck.\n",
      "Muscles worked:\n",
      "●\n",
      "●erector spinae\n",
      "rectus abdominis.......\n",
      "---\n",
      "## Document 2: 1.\n",
      "2.\n",
      "3.\n",
      "4.\n",
      "5.\n",
      "6.\n",
      "7.Sit back on your heels with your knees together.\n",
      "You can use a bolster or blanket under your thighs, torso, or forehead for support.\n",
      "Bend forward and walk your hands in front of you.\n",
      "Rest your forehead gently on the floor.\n",
      "Keep your arms extended in front of you or bring your arms alongside your body with your palms\n",
      "facing up.\n",
      "Focus on releasing tension in your back as your upper body falls heavy into your knees.\n",
      "Remain in this pose for up to 5 minutes.\n",
      "Does it really work?\n",
      "One small  study from 2017\n",
      "Trusted Source  assessed the effects of either yoga practice or physical therapy over the course of\n",
      "one year. The participants\n",
      "had chronic back pain and showed similar improvement in pain and activity limitation. Both groups\n",
      "were less likely to use pain\n",
      "medications after three months.\n",
      "Separate  research from 2017\n",
      "Trusted Source  found that people who practiced yoga showed small to moderate decreases in pain\n",
      "intensity in the short term........\n",
      "---\n",
      "## Document 3: Yoga stress info\n",
      "What is Stress?\n",
      "Stress is a natural reaction that our bodies experience when we face\n",
      "challenges or demands in\n",
      "our lives. It’s our body’s way of responding to situations that require us to\n",
      "adapt or take action.\n",
      "While a little stress can be motivating and help us perform better, too much\n",
      "stress can be\n",
      "harmful to our overall well-being.\n",
      "Benefits of Yoga for stress relief\n",
      "Yoga is a powerful practice that can significantly help in relieving stress.\n",
      "Through a combination\n",
      "of physical postures, deep breathing, and mindfulness, yoga promotes\n",
      "relaxation and reduces\n",
      "the production of stress hormones. This leads to a decrease in anxiety and a\n",
      "greater sense of\n",
      "calm. The gentle stretching and controlled movements in yoga help release\n",
      "tension stored in the\n",
      "body, particularly in areas prone to stress accumulation such as the neck,\n",
      "shoulders, and back.\n",
      "Additionally, the mindfulness aspect of yoga encourages being present at the moment and.......\n",
      "---\n",
      "## Document 4: Additionally, the mindfulness aspect of yoga encourages being present at the moment\n",
      "and\n",
      "letting go of worries about the past or future. Regular practice of yoga can improve overall\n",
      "well-being, increase resilience to stress, and provide a sense of inner peace and balance.\n",
      "9 Calming Yoga Poses for Stress Relief\n",
      "1. Standing Forward Fold (Uttanasana)\n",
      "Standing Forward Fold, also known as Uttanasana, helps calm the nervous system, soothes the mind by\n",
      "releasing\n",
      "tension in the back and hamstring muscles, and promotes relaxation.\n",
      "To perform Uttanasana (Standing Forward Fold), stand with feet hip-width apart, fold forward.......\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print_ww(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the relevant documents, it's time to use the LLM to generate an answer based on these documents. \n",
    "\n",
    "We will take our inital prompt, together with our relevant documents which were retreived based on the results of our similarity search. We then by combining these create a prompt that we feed back to the model to get our result. At this point our model should give us highly informed information on how we can change the tire of our specific car as it was outlined in our manual.\n",
    "\n",
    "LangChain provides an abstraction of how this can be done easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick way\n",
    "You have the possibility to use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM.\n",
    "This wrapper performs the following steps behind the scences:\n",
    "- Takes input the question\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided, yoga can be beneficial for both anxiety and back pain for the\n",
      "following reasons:\n",
      "\n",
      "- Yoga promotes relaxation and reduces stress hormones through physical postures, breathing\n",
      "techniques, and mindfulness. This can help decrease anxiety levels and promote a greater sense of\n",
      "calm.\n",
      "\n",
      "- For back pain, yoga can relax and strengthen the body through gentle stretching and controlled\n",
      "movements. Certain poses help release tension stored in the back and improve alignment.\n",
      "\n",
      "- Practicing yoga increases body awareness which allows you to notice where you hold tension. This\n",
      "awareness can then help bring your body into balance.\n",
      "\n",
      "- Research shows yoga can provide small to moderate decreases in back pain intensity. One study\n",
      "found similar improvements in chronic back pain from yoga and physical therapy over one year.\n",
      "\n",
      "- The mindfulness aspect of yoga also encourages being present and letting go of worries, which can\n",
      "reduce anxiety. Regular yoga practice improves overall well-being and resilience to stress.\n",
      "\n",
      "So in summary, the evidence indicates yoga can be helpful for both anxiety relief through its\n",
      "relaxing effects and back pain relief through targeted poses, increased body awareness, and stress\n",
      "reduction. The combination of physical, mental and breathing practices make yoga a great option for\n",
      "managing both anxiety and back pain.\n"
     ]
    }
   ],
   "source": [
    "query = \"Yoga for anxiety and back pain\"\n",
    "#query = \"What are the different ways that the medication lithium can be administered?\"\n",
    "answer = wrapper_store_faiss.query(question=query, llm=llm)\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask a different question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customisable option\n",
    "In the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the helpf of [RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html) which can be specific to the model.\n",
    "\n",
    "Note: In this example we are using Anthropic Claude as the LLM under Amazon Bedrock, this particular model performs best if the inputs are provided under `Human:` and the model is requested to generate an output after `Assistant:`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some tips to reduce work stress through yoga asanas:\n",
      "\n",
      "- Practice standing forward folds like Uttanasana to release tension in the back and calm the\n",
      "nervous system. Bend forward from your hips, letting your head and arms hang heavy.\n",
      "\n",
      "- Do a few rounds of Cat-Cow stretches. As you arch and round your back with your breath, you'll\n",
      "release tension in the spine.\n",
      "\n",
      "- Try seated forward folds like Paschimottanasana. Sitting with legs extended, hinge at the hips to\n",
      "fold forward over your legs. This stretches the hamstrings and back.\n",
      "\n",
      "- Savasana, or Corpse Pose, is deeply relaxing. Lie on your back and let all tension melt away as\n",
      "you focus on your breath. Stay for 5-10 minutes.\n",
      "\n",
      "- Incorporate twisting poses like Ardha Matsyendrasana to stimulate your digestive system and\n",
      "relieve tension in the back and shoulders.\n",
      "\n",
      "- Try standing or seated wide-legged forward folds like Prasarita Padottanasana to open the hips and\n",
      "hamstrings.\n",
      "\n",
      "- End your practice with Sukhasana (Easy Pose) to promote tranquility. Sit cross-legged with your\n",
      "spine straight and hands resting on your knees.\n",
      "\n",
      "A daily yoga practice with these calming, restorative asanas can help relieve work stress and\n",
      "cultivate relaxation. Be sure to focus on your breath in each pose as well.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: You will be acting as a Yoga Instructor.  Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 9}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "#query = \"What are the different ways that the medication lithium can be given?\"\n",
    "query = \"How do I reduce my daily stress from work doing yoga asanas?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Try Titan models - PROMPT EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \".\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"https://bedrockForYoga\"  # E.g. \"https://...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    ")    \n",
    "# - create the Titan  Model\n",
    "llm_titan = Bedrock(model_id=\"amazon.titan-tg1-large\", client=boto3_bedrock, model_kwargs={'maxTokenCount':200, 'temperature':0})\n",
    "bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock)\n",
    "vectorstore_faiss_titan = FAISS.load_local(\"faiss_index\", bedrock_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry - this model is designed to avoid profanity. Please see our content limitations page for more\n",
      "information.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: You will be acting as a Yoga Instructor.  Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm_titan,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss_titan.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 9}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "#query = \"What are the different ways that the medication lithium can be given?\"\n",
    "query = \"How can I cure both back pain and anxiety?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: JSON prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON file up here is not completed, missed out the proposed indication section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More optimal prompts for Titan - Try These!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry - this model is designed to avoid profanity. Please see our content limitations page for more\n",
      "information.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"You will be acting as a Yoga Instructor. Use the follow [instructions] to answer the [question] based on the [context] provided. Don't answer if the answer is not present in the [context]. Follow the [output_format] given below while responding.\n",
    "\n",
    "context = {context} \n",
    "\n",
    "instructions = Use following instructions to answer the question above. \n",
    "Make sure to include following [attributes] in your answer as applicable.\n",
    "- Teach one yoga pose for the {question} asked.\n",
    "- Teach the benefit of the yoga pose for mental health\n",
    "- Teach the benefit of the yoga pose for physical health\n",
    "\n",
    "\n",
    "output_format = Provide your output as a text based paragraph that follows the instructions below\n",
    "- Each and every sentence is complete, ending with a full stop\n",
    "\n",
    "\n",
    "\n",
    "question = {question} \n",
    "\n",
    "\n",
    "output_format = Provide your output as a detailed paragraph that contains all [attributes] following all [instructions] above\n",
    "\n",
    "answer: \"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm_titan,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss_titan.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 9}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "#query = \"What are the different ways that the medication lithium can be given?\"\n",
    "query = \"I am old and I have back pain in the lower back. How can I cure this using Yoga?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above titan response is not fully completed. Let's try some more prompting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry - this model is designed to avoid profanity. Please see our content limitations page for more\n",
      "information.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"You will be acting as a Yoga Instructor. Use the follow [instructions] to answer the [question] based on the [context] provided. Don't answer if the answer is not present in the [context]. Follow the [output_format] given below while responding.\n",
    "\n",
    "context = {context} \n",
    "\n",
    "instructions = Use following instructions to answer the question above. \n",
    "Complete all the sentences and create a sense of comfort and understanding\n",
    "Make sure to include following in your answer as applicable.\n",
    "- Yoga Pose = Teach one yoga pose for the {question} asked.\n",
    "- Mental Health Benefit = Teach the benefit of the yoga pose for mental health\n",
    "- Physical Health Benefit = Teach the benefit of the yoga pose for physical health\n",
    "Make sure to add all of these above in the [answer]\n",
    "\n",
    "\n",
    "output_format = Provide your output as a text based paragraph that follows the instructions below\n",
    "- Each and every sentence is complete, ending with a full stop\n",
    "- All attributes are contained in the [answer]\n",
    "- Give the [answer] with the emotion of care and concern for your students\n",
    "\n",
    "\n",
    "\n",
    "question = {question} \n",
    "\n",
    "\n",
    "output_format = Provide your output as a detailed paragraph that contains all [attributes] following all [instructions] above\n",
    "Make sure to think step by step and give a detailed response\n",
    "\n",
    "answer: \"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm_titan,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss_titan.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 9}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "#query = \"What are the different ways that the medication lithium can be given?\"\n",
    "query = \" How can I cure my neck pain using yoga?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This response that is given is incomplete. No matter how clear the input for the Titan model is, the output might or might not be complete in terms of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer to the example below. Provide the instructions, and then give the context for better contextual answers, or Make the prompt template more detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best yoga pose for a 24 year old woman to reduce stress and anxiety is the Child's Pose, also\n",
      "known as Balasana. This pose involves kneeling on the floor and then sitting back on your heels,\n",
      "lowering your torso between your thighs. Extend your arms forward or alongside your body and relax\n",
      "your forehead on the mat or a block.\n",
      "\n",
      "The Child's Pose is a simple yet effective pose that can help to calm the mind and reduce stress\n",
      "levels. It is especially beneficial for those who experience anxiety, as it helps to soothe the\n",
      "nervous system and promote feelings of safety and security. By taking a few minutes each day to\n",
      "practice the Child's Pose, you can experience the benefits of yoga and reduce stress in your daily\n",
      "life.\n",
      "\n",
      "For a beginner student, it is important to start in a comfortable position and to focus on relaxing\n",
      "the body. The student should imagine themselves as a child, safe and secure in their parent's arms,\n",
      "and allow themselves to fully relax\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"You will be acting as a Yoga Instructor. Use the follow [instructions] to answer the [question] based on the [context] provided. Don't answer if the answer is not present in the [context]. Follow the [output_format] given below while responding.\n",
    "\n",
    "context = {context} \n",
    "\n",
    "question = {question} \n",
    "\n",
    "instructions = Use following instructions to answer the question above. \n",
    "- Do not hallucinate and only answer the question based on the context provided above. \n",
    "- Provide answer using the [output_format] at the end. \n",
    "\n",
    "\n",
    "instructions = Use following instructions to answer the question above. \n",
    "Complete all the sentences and create a sense of comfort and understanding\n",
    "Make sure to include following in your answer as applicable.\n",
    "- Yoga Pose = Teach one yoga pose for the {question} asked.\n",
    "- Mental Health Benefit = Teach the benefit of the yoga pose for mental health\n",
    "- Physical Health Benefit = Teach the benefit of the yoga pose for physical health\n",
    "- Write an answer as a paragraph for a beginner student\n",
    "- Write an answer as a paragraph for a intermediate student\n",
    "- Write an answer as a paragraph for a advances student\n",
    "Make sure to add all of these above in the [answer]\n",
    "\n",
    "\n",
    "output_format = Provide your output as a text based paragraph that follows the instructions below\n",
    "- Each and every sentence is complete, ending with a full stop\n",
    "- All attributes are contained in the [answer]\n",
    "- Give the [answer] with the emotion of care and concern for your students\n",
    "\n",
    "\n",
    "output_format = Provide your output as a text based paragraph that follows the instructions below\n",
    "- Each and every sentence is complete, ending with a full stop\n",
    "- Add three paragraphs in your answer\n",
    "\n",
    "output_format = Provide your output as a detailed paragraph that contains all [attributes] following all [instructions] above. \n",
    "\n",
    "End your answer with a completed sentence followed by a full stop\n",
    "\n",
    "answer: \"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm_titan,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss_titan.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 9}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "#query = \"What are the different ways that the medication lithium can be given?\"\n",
    "query = \"What is the best yoga pose of a 24 year old woman stress and anxiety?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, an incomplete response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW, TRYING CLAUDE FOR OUT PROMPTS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple answer first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <answer>\n",
      "Here is a calming yoga pose that can help relieve stress and anxiety for a 24 year old woman:\n",
      "\n",
      "For the beginner student:\n",
      "Child's Pose (Balasana) is an excellent stress-relieving yoga pose. To do Child's Pose, kneel on the\n",
      "floor and sit back on your heels. Then lower your torso between your thighs and extend your arms in\n",
      "front of you or relax them by your sides. Rest your forehead on the floor or a block. This pose\n",
      "gently stretches the hips, thighs, ankles and back which helps release tension. It also calms the\n",
      "mind and nervous system, providing a sense of comfort and security. Hold Child's Pose for a few\n",
      "long, deep breaths. It can be held for up to a few minutes for full relaxation. Regular practice of\n",
      "Child's Pose can alleviate anxiety and promote feelings of calmness.\n",
      "\n",
      "For the intermediate student:\n",
      "A calming yoga pose to help relieve stress and anxiety is Legs Up the Wall Pose (Viparita Karani).\n",
      "To do this pose, lie on your back near a wall. Extend your legs straight up against the wall,\n",
      "keeping your sitting bones close to the wall. Allow your arms to relax by your sides with palms\n",
      "facing up. Close your eyes and breathe deeply. This inverted pose helps reduce stress and anxiety by\n",
      "reversing the flow of blood in the body, which has a calming effect. It also stretches the legs,\n",
      "lower back and hips to release tension. Hold the pose for 5-10 minutes for maximum relaxation.\n",
      "Regular practice can lower cortisol levels and promote tranquility.\n",
      "\n",
      "For the advanced student:\n",
      "Savasana (Corpse Pose) is an excellent pose to relieve stress and anxiety. To do Savasana, lie flat\n",
      "on your back with arms relaxed at your sides, palms facing up. Legs are extended comfortably and\n",
      "feet fall open. Close your eyes and breathe deeply into the belly. Release all muscular tension and\n",
      "allow your body to feel heavy. Remain in the pose for 5-15 minutes. Savasana helps calm the nervous\n",
      "system and quiet the mind. It allows the body and mind to deeply relax by disengaging from physical\n",
      "and mental activity. Regular practice reduces anxiety and creates a profound sense of tranquility.\n",
      "It is one of the most calming yoga poses to relieve stress.\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: You will be acting as a Yoga Instructor. Use the follow <instructions></instructions> to answer the question based on the context provided. Don't answer if the answer is not present in the context. Follow the <output_format></output_format> given below while responding.\n",
    "\n",
    "context = <context>{context}</context>\n",
    "\n",
    "question = <question>{question}</question>\n",
    "\n",
    "instructions = Use following instructions to answer the question above. \n",
    "- Do not hallucinate and only answer the question based on the context provided above. \n",
    "\n",
    "<instructions>\n",
    "instructions = Use following instructions to answer the question above. \n",
    "Complete all the sentences and create a sense of comfort and understanding\n",
    "Make sure to include following in your answer as applicable.\n",
    "- Yoga Pose = Teach one yoga pose for the {question} asked.\n",
    "- Mental Health Benefit = Teach the benefit of the yoga pose for mental health\n",
    "- Physical Health Benefit = Teach the benefit of the yoga pose for physical health\n",
    "- Write an answer as a paragraph for a beginner student\n",
    "- Write an answer as a paragraph for a intermediate student\n",
    "- Write an answer as a paragraph for a advances student\n",
    "Make sure to add all of these above in the [answer]\n",
    "</instructions>\n",
    "\n",
    "<output_format>\n",
    "output_format = Provide your output as a text based paragraph that follows the instructions below\n",
    "- Each and every sentence is complete, ending with a full stop\n",
    "- All attributes are contained in the answer\n",
    "- Give the answer with the emotion of care and concern for your students\n",
    "</output_format>\n",
    "\n",
    "\n",
    "Provide your output as a text based paragraph that follows the instructions below\n",
    "- Each and every sentence is complete, ending with a full stop\n",
    "- Add three paragraphs in your answer\n",
    "\n",
    "Provide your output as a detailed paragraph that contains all attributes following all instructions above. \n",
    "answer: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 9}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "#query = \"What are the different ways that the medication lithium can be given?\"\n",
    "query = \"What is the best yoga pose of a 24 year old woman stress and anxiety?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Give a step by step approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a comprehensive answer for a 24 year old woman looking to relieve stress and anxiety\n",
      "through yoga:\n",
      "\n",
      "For a beginner student:\n",
      "Namaste. I understand you are looking for a yoga pose to help relieve stress and anxiety. Child's\n",
      "pose, or Balasana, is an excellent beginner pose that promotes deep relaxation. To do this pose,\n",
      "start on your hands and knees. Then sit back on your heels and lower your torso between your thighs.\n",
      "Reach your arms forward and relax your forehead down. Breathe deeply and focus on releasing any\n",
      "tension you feel. Hold for a few breaths. Balasana gently stretches your back, shoulders and neck to\n",
      "release tension. It also calms the mind and nervous system. Practicing this nurturing pose will\n",
      "provide comfort, security and tranquility.\n",
      "\n",
      "For an intermediate student:\n",
      "Namaste. As an intermediate student, I would recommend Legs up the Wall pose, or Viparita Karani, to\n",
      "relieve stress and anxiety. To do this pose, lie on your back near a wall. Extend your legs straight\n",
      "up against the wall, keeping your sitting bones close to the wall. Relax your arms by your sides.\n",
      "Close your eyes, breathe deeply and allow your body to completely unwind. This inverted pose\n",
      "reverses the blood flow in your body, bringing calmness to your nervous system. It also relieves\n",
      "tension in the legs and encourages total relaxation. With regular practice, Viparita Karani can\n",
      "lower stress hormones and leave you feeling refreshed.\n",
      "\n",
      "For an advanced student:\n",
      "Namaste. For an advanced student looking to relieve stress and anxiety, I recommend Headstand, or\n",
      "Sirsasana. Come onto your hands and knees, interlace your fingers and place your forearms on the\n",
      "floor. Place the crown of your head on the floor between your hands. Walk your feet in toward your\n",
      "elbows and lift your knees off the floor. Engage your core and lift your legs up, using your core\n",
      "strength to bring your body into a balanced vertical position. Breathe deeply and hold for a few\n",
      "breaths. Headstand relieves stress by calming the brain and nervous system. The inversion lets\n",
      "tension drain from the legs and torso. It also stimulates the pituitary and pineal glands to balance\n",
      "hormones. With regular practice, Sirsasana can relieve anxiety and depression.\n",
      "\n",
      "I hope these yoga poses bring you relaxation and inner peace. Let me know if you have any other\n",
      "questions!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: You will be acting as a Yoga Instructor. Use the follow <instructions></instructions> to answer the question based on the context provided. Don't answer if the answer is not present in the context. Follow the <output_format></output_format> given below while responding.\n",
    "\n",
    "context = <context>{context}</context>\n",
    "\n",
    "question = <question>{question}</question>\n",
    "\n",
    "instructions = Use following instructions to answer the question above. \n",
    "- Do not hallucinate and only answer the question based on the context provided above. \n",
    "\n",
    "<instructions>\n",
    "instructions = Use following instructions to answer the question above. \n",
    "Complete all the sentences and create a sense of comfort and understanding\n",
    "Make sure to include following in your answer as applicable.\n",
    "- Yoga Pose = Teach one yoga pose for the {question} asked.\n",
    "- Mental Health Benefit = Teach the benefit of the yoga pose for mental health\n",
    "- Physical Health Benefit = Teach the benefit of the yoga pose for physical health\n",
    "- Write an answer as a paragraph for a beginner student\n",
    "- Write an answer as a paragraph for a intermediate student\n",
    "- Write an answer as a paragraph for a advances student\n",
    "Make sure to add all of these above in the [answer]\n",
    "</instructions>\n",
    "\n",
    "<output_format>\n",
    "output_format = Provide your output as a text based paragraph that follows the instructions below\n",
    "- Each and every sentence is complete, ending with a full stop\n",
    "- All attributes are contained in the answer\n",
    "- Give the answer with the emotion of care and concern for your students\n",
    "</output_format>\n",
    "\n",
    "\n",
    "Provide your output as a text based paragraph that follows the instructions below\n",
    "- Each and every sentence is complete, ending with a full stop\n",
    "\n",
    "Assistant: Can I think step by step?\n",
    "\n",
    "Human: Yes, you can think step by step. \n",
    "\n",
    "Assistant: Is there anything else I can add in my answer?\n",
    "\n",
    "Human: Yes, add elements of care and concern. Also give steps for both hindi and english readers.\n",
    "\n",
    "answer: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 9}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "#query = \"What are the different ways that the medication lithium can be given?\"\n",
    "query = \"What is the best yoga pose of a 24 year old woman stress and anxiety?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Define the SageMaker estimator\n",
    "role = role\n",
    "image_uri = \"your-bedrock-container-image\"\n",
    "estimator = Estimator(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",  # Choose an appropriate instance type\n",
    "    base_job_name=\"your-bedrock-training-job\",\n",
    "    hyperparameters={\n",
    "        \"your_hyperparameter\": \"value\",\n",
    "        # Add other hyperparameters here\n",
    "    },\n",
    ")\n",
    "\n",
    "# Set the training data location\n",
    "train_data = \"s3://your-s3-bucket/path/to/train/data\"\n",
    "estimator.fit({\"train\": train_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
